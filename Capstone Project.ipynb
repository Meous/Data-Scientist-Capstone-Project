{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as py#visualization\n",
    "py.init_notebook_mode(connected=True)#visualization\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "df = pd.read_csv('C:/Users/kwame.adu/Desktop/Kwame/Learning/Data Science/Capstone Project/bank_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>duration</th>\n",
       "      <th>products</th>\n",
       "      <th>churn</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>2343</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Achimota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Legon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kisseman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>2476</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Kwabenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Abokobi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006</td>\n",
       "      <td>42</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>562</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjen Kotoku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007</td>\n",
       "      <td>56</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1201</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Accra New Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>60</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adabraka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009</td>\n",
       "      <td>37</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>608</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Abossey Okai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>28</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>0</td>\n",
       "      <td>5090</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1297</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Kaneshie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  age         job   marital  education  default  balance  \\\n",
       "0         1001   59      admin.   married  secondary        0     2343   \n",
       "1         1002   56      admin.   married  secondary        0       45   \n",
       "2         1003   41  technician   married  secondary        0     1270   \n",
       "3         1004   55    services   married  secondary        0     2476   \n",
       "4         1005   54      admin.   married   tertiary        0      184   \n",
       "5         1006   42  management    single   tertiary        0        0   \n",
       "6         1007   56  management   married   tertiary        0      830   \n",
       "7         1008   60     retired  divorced  secondary        0      545   \n",
       "8         1009   37  technician   married  secondary        0        1   \n",
       "9         1010   28    services    single  secondary        0     5090   \n",
       "\n",
       "   housing  loan  duration  products  churn        location  \n",
       "0        1     0      1042         1      1        Achimota  \n",
       "1        0     0      1467         1      1           Legon  \n",
       "2        1     0      1389         1      1        Kisseman  \n",
       "3        1     0       579         1      1        Kwabenya  \n",
       "4        0     0       673         2      1         Abokobi  \n",
       "5        1     1       562         2      1    Adjen Kotoku  \n",
       "6        1     1      1201         1      1  Accra New Town  \n",
       "7        1     0      1030         1      1        Adabraka  \n",
       "8        1     0       608         1      1    Abossey Okai  \n",
       "9        1     0      1297         3      1        Kaneshie  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view data - first ten rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11162 entries, 0 to 11161\n",
      "Data columns (total 13 columns):\n",
      "customer_id    11162 non-null int64\n",
      "age            11162 non-null int64\n",
      "job            11162 non-null object\n",
      "marital        11162 non-null object\n",
      "education      11162 non-null object\n",
      "default        11162 non-null int64\n",
      "balance        11162 non-null int64\n",
      "housing        11162 non-null int64\n",
      "loan           11162 non-null int64\n",
      "duration       11162 non-null int64\n",
      "products       11162 non-null int64\n",
      "churn          11162 non-null int64\n",
      "location       11162 non-null object\n",
      "dtypes: int64(9), object(4)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#list of columns and data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11162, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reviewing data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>duration</th>\n",
       "      <th>products</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "      <td>11162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6581.500000</td>\n",
       "      <td>41.231948</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>1528.538524</td>\n",
       "      <td>0.473123</td>\n",
       "      <td>0.130801</td>\n",
       "      <td>371.993818</td>\n",
       "      <td>2.508421</td>\n",
       "      <td>0.793585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3222.336187</td>\n",
       "      <td>11.913369</td>\n",
       "      <td>0.121761</td>\n",
       "      <td>3225.413326</td>\n",
       "      <td>0.499299</td>\n",
       "      <td>0.337198</td>\n",
       "      <td>347.128386</td>\n",
       "      <td>2.722077</td>\n",
       "      <td>0.404750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6847.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3791.250000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6581.500000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9371.750000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1708.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12162.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>81204.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3881.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        customer_id           age       default       balance       housing  \\\n",
       "count  11162.000000  11162.000000  11162.000000  11162.000000  11162.000000   \n",
       "mean    6581.500000     41.231948      0.015051   1528.538524      0.473123   \n",
       "std     3222.336187     11.913369      0.121761   3225.413326      0.499299   \n",
       "min     1001.000000     18.000000      0.000000  -6847.000000      0.000000   \n",
       "25%     3791.250000     32.000000      0.000000    122.000000      0.000000   \n",
       "50%     6581.500000     39.000000      0.000000    550.000000      0.000000   \n",
       "75%     9371.750000     49.000000      0.000000   1708.000000      1.000000   \n",
       "max    12162.000000     95.000000      1.000000  81204.000000      1.000000   \n",
       "\n",
       "               loan      duration      products         churn  \n",
       "count  11162.000000  11162.000000  11162.000000  11162.000000  \n",
       "mean       0.130801    371.993818      2.508421      0.793585  \n",
       "std        0.337198    347.128386      2.722077      0.404750  \n",
       "min        0.000000      2.000000      1.000000      0.000000  \n",
       "25%        0.000000    138.000000      1.000000      1.000000  \n",
       "50%        0.000000    255.000000      2.000000      1.000000  \n",
       "75%        0.000000    496.000000      3.000000      1.000000  \n",
       "max        1.000000   3881.000000     63.000000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id    0\n",
       "age            0\n",
       "job            0\n",
       "marital        0\n",
       "education      0\n",
       "default        0\n",
       "balance        0\n",
       "housing        0\n",
       "loan           0\n",
       "duration       0\n",
       "products       0\n",
       "churn          0\n",
       "location       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking cells without data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11162"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Descriptive statistics\n",
    "#Distribution of jobs types\n",
    "df_jobtype = df[\"job\"].count()\n",
    "df_jobtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecba53bcc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualization of job types\n",
    "jobtype = df['job'].str.split(';', expand=True).stack().value_counts().head(10)\n",
    "\n",
    "#plotting bar graph\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Job')\n",
    "jobtype.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.style.use('ggplot')\n",
    "churn = df[df[\"churn\"] == 1]\n",
    "active = df[df[\"churn\"] == 0]\n",
    "\n",
    "#function for EDA visualizations\n",
    "def pie_chart(column):\n",
    "    \n",
    "    chart1 = go.Pie(values = churn[column].value_counts().values.tolist(),\n",
    "                    labels = churn[column].value_counts().keys().tolist(),\n",
    "                    name = \"Churn Customers\",\n",
    "                    marker = dict(line = dict(width = 3,\n",
    "                                             color = \"rgb(243,243,243)\")\n",
    "                                 ),\n",
    "                    hole = .4\n",
    "                   )\n",
    "    \n",
    "    chart2 = go.Pie(values = active[column].value_counts().values.tolist(),\n",
    "                    labels = active[column].value_counts().keys().tolist(),\n",
    "                    name = \"Active Customers\",\n",
    "                    marker = dict(line = dict(width = 3,\n",
    "                                             color = \"rgb(243,243,243)\")\n",
    "                                 ),\n",
    "                    hole = .4\n",
    "                   )\n",
    "    \n",
    "    data = [chart1, chart2]\n",
    "    fig = go.Figure(data = data)\n",
    "    #py.iplot(fig)\n",
    "    \n",
    "cat_cols = [\"marital\", \"location\", \"job\", \"education\" ]\n",
    "num_cols = [\"age\", \"default\", \"balance\", \"housing\", \"loan\"]\n",
    "for i in cat_cols:\n",
    "    pie_chart(i)\n",
    "    \n",
    "for i in num_cols:\n",
    "    pie_chart(i)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "domain": {
          "x": [
           0.55,
           1
          ]
         },
         "hole": 0.4,
         "labels": [
          "married",
          "single",
          "divorced"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Active",
         "type": "pie",
         "values": [
          1449,
          613,
          242
         ]
        },
        {
         "domain": {
          "x": [
           0,
           0.48
          ]
         },
         "hole": 0.4,
         "labels": [
          "married",
          "single",
          "divorced"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Churn",
         "type": "pie",
         "values": [
          4902,
          2905,
          1051
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Churn",
          "x": 0.18,
          "y": 0.5
         },
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Active",
          "x": 0.8,
          "y": 0.5
         }
        ],
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "marital distribution in customer attrition "
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"5cd39ee5-0c97-450a-9faa-005f72f9badf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5cd39ee5-0c97-450a-9faa-005f72f9badf\")) {                    Plotly.newPlot(                        \"5cd39ee5-0c97-450a-9faa-005f72f9badf\",                        [{\"domain\": {\"x\": [0.55, 1]}, \"hole\": 0.4, \"labels\": [\"married\", \"single\", \"divorced\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Active\", \"type\": \"pie\", \"values\": [1449, 613, 242]}, {\"domain\": {\"x\": [0, 0.48]}, \"hole\": 0.4, \"labels\": [\"married\", \"single\", \"divorced\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Churn\", \"type\": \"pie\", \"values\": [4902, 2905, 1051]}],                        {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Churn\", \"x\": 0.18, \"y\": 0.5}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Active\", \"x\": 0.8, \"y\": 0.5}], \"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"marital distribution in customer attrition \"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5cd39ee5-0c97-450a-9faa-005f72f9badf');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "domain": {
          "x": [
           0.55,
           1
          ]
         },
         "hole": 0.4,
         "labels": [
          "blue-collar",
          "management",
          "technician",
          "admin.",
          "services",
          "retired",
          "self-employed",
          "entrepreneur",
          "unemployed",
          "housemaid",
          "student",
          "unknown"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Active",
         "type": "pie",
         "values": [
          520,
          491,
          392,
          275,
          209,
          110,
          81,
          75,
          55,
          50,
          32,
          14
         ]
        },
        {
         "domain": {
          "x": [
           0,
           0.48
          ]
         },
         "hole": 0.4,
         "labels": [
          "management",
          "technician",
          "blue-collar",
          "admin.",
          "services",
          "retired",
          "student",
          "self-employed",
          "unemployed",
          "entrepreneur",
          "housemaid",
          "unknown"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Churn",
         "type": "pie",
         "values": [
          2075,
          1431,
          1424,
          1059,
          714,
          668,
          328,
          324,
          302,
          253,
          224,
          56
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Churn",
          "x": 0.18,
          "y": 0.5
         },
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Active",
          "x": 0.8,
          "y": 0.5
         }
        ],
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "job distribution in customer attrition "
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"6bc533b9-aceb-4851-81d0-b769721bef67\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6bc533b9-aceb-4851-81d0-b769721bef67\")) {                    Plotly.newPlot(                        \"6bc533b9-aceb-4851-81d0-b769721bef67\",                        [{\"domain\": {\"x\": [0.55, 1]}, \"hole\": 0.4, \"labels\": [\"blue-collar\", \"management\", \"technician\", \"admin.\", \"services\", \"retired\", \"self-employed\", \"entrepreneur\", \"unemployed\", \"housemaid\", \"student\", \"unknown\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Active\", \"type\": \"pie\", \"values\": [520, 491, 392, 275, 209, 110, 81, 75, 55, 50, 32, 14]}, {\"domain\": {\"x\": [0, 0.48]}, \"hole\": 0.4, \"labels\": [\"management\", \"technician\", \"blue-collar\", \"admin.\", \"services\", \"retired\", \"student\", \"self-employed\", \"unemployed\", \"entrepreneur\", \"housemaid\", \"unknown\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Churn\", \"type\": \"pie\", \"values\": [2075, 1431, 1424, 1059, 714, 668, 328, 324, 302, 253, 224, 56]}],                        {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Churn\", \"x\": 0.18, \"y\": 0.5}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Active\", \"x\": 0.8, \"y\": 0.5}], \"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"job distribution in customer attrition \"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6bc533b9-aceb-4851-81d0-b769721bef67');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "domain": {
          "x": [
           0.55,
           1
          ]
         },
         "hole": 0.4,
         "labels": [
          "secondary",
          "tertiary",
          "primary",
          "unknown"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Active",
         "type": "pie",
         "values": [
          1195,
          645,
          359,
          105
         ]
        },
        {
         "domain": {
          "x": [
           0,
           0.48
          ]
         },
         "hole": 0.4,
         "labels": [
          "secondary",
          "tertiary",
          "primary",
          "unknown"
         ],
         "marker": {
          "line": {
           "color": "rgb(243,243,243)",
           "width": 1
          }
         },
         "name": "Churn",
         "type": "pie",
         "values": [
          4281,
          3044,
          1141,
          392
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Churn",
          "x": 0.18,
          "y": 0.5
         },
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Active",
          "x": 0.8,
          "y": 0.5
         }
        ],
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "education distribution in customer attrition "
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"724f48c1-2a2e-4b69-89eb-69261b0487a1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"724f48c1-2a2e-4b69-89eb-69261b0487a1\")) {                    Plotly.newPlot(                        \"724f48c1-2a2e-4b69-89eb-69261b0487a1\",                        [{\"domain\": {\"x\": [0.55, 1]}, \"hole\": 0.4, \"labels\": [\"secondary\", \"tertiary\", \"primary\", \"unknown\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Active\", \"type\": \"pie\", \"values\": [1195, 645, 359, 105]}, {\"domain\": {\"x\": [0, 0.48]}, \"hole\": 0.4, \"labels\": [\"secondary\", \"tertiary\", \"primary\", \"unknown\"], \"marker\": {\"line\": {\"color\": \"rgb(243,243,243)\", \"width\": 1}}, \"name\": \"Churn\", \"type\": \"pie\", \"values\": [4281, 3044, 1141, 392]}],                        {\"annotations\": [{\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Churn\", \"x\": 0.18, \"y\": 0.5}, {\"font\": {\"size\": 14}, \"showarrow\": false, \"text\": \"Active\", \"x\": 0.8, \"y\": 0.5}], \"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"education distribution in customer attrition \"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('724f48c1-2a2e-4b69-89eb-69261b0487a1');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Active",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          24,
          31,
          41,
          34,
          31,
          31,
          52,
          38,
          31,
          34,
          22,
          60,
          28,
          31,
          32,
          77,
          70,
          29,
          34,
          38,
          45,
          40,
          34,
          30,
          56,
          50,
          53,
          48,
          53,
          27,
          46,
          54,
          36,
          31,
          58,
          48,
          29,
          57,
          37,
          53,
          58,
          33,
          38,
          28,
          28,
          37,
          27,
          36,
          53,
          26,
          59,
          31,
          59,
          40,
          27,
          32,
          42,
          43,
          53,
          30,
          53,
          53,
          57,
          33,
          38,
          33,
          42,
          32,
          42,
          41,
          35,
          48,
          24,
          59,
          45,
          39,
          46,
          38,
          30,
          52,
          42,
          46,
          33,
          52,
          49,
          59,
          58,
          26,
          41,
          41,
          39,
          27,
          47,
          52,
          37,
          38,
          42,
          41,
          46,
          40,
          41,
          60,
          59,
          59,
          28,
          45,
          31,
          31,
          46,
          51,
          37,
          40,
          31,
          38,
          39,
          29,
          65,
          26,
          44,
          27,
          35,
          36,
          36,
          24,
          33,
          53,
          45,
          37,
          60,
          30,
          37,
          34,
          28,
          44,
          34,
          70,
          38,
          32,
          39,
          38,
          36,
          32,
          34,
          49,
          45,
          24,
          47,
          36,
          25,
          30,
          29,
          27,
          57,
          38,
          43,
          35,
          38,
          71,
          36,
          40,
          50,
          31,
          33,
          57,
          32,
          52,
          30,
          40,
          50,
          32,
          36,
          60,
          48,
          72,
          46,
          45,
          50,
          29,
          40,
          49,
          42,
          43,
          36,
          37,
          59,
          75,
          53,
          53,
          57,
          30,
          44,
          51,
          30,
          42,
          48,
          40,
          36,
          40,
          47,
          39,
          41,
          42,
          50,
          58,
          70,
          29,
          56,
          32,
          39,
          33,
          47,
          39,
          34,
          36,
          40,
          34,
          54,
          36,
          34,
          50,
          48,
          35,
          33,
          42,
          31,
          54,
          55,
          30,
          30,
          59,
          50,
          59,
          27,
          25,
          37,
          36,
          56,
          52,
          36,
          30,
          31,
          55,
          28,
          44,
          28,
          25,
          39,
          32,
          42,
          31,
          38,
          30,
          50,
          50,
          47,
          29,
          34,
          54,
          43,
          47,
          55,
          35,
          32,
          49,
          39,
          32,
          49,
          32,
          47,
          42,
          52,
          31,
          52,
          38,
          53,
          40,
          31,
          53,
          46,
          49,
          34,
          63,
          41,
          55,
          29,
          47,
          37,
          38,
          29,
          41,
          53,
          40,
          56,
          34,
          28,
          41,
          32,
          51,
          33,
          34,
          26,
          31,
          39,
          50,
          33,
          44,
          38,
          33,
          32,
          33,
          55,
          27,
          39,
          21,
          57,
          29,
          45,
          46,
          47,
          44,
          43,
          44,
          52,
          33,
          27,
          43,
          29,
          43,
          34,
          30,
          56,
          46,
          42,
          51,
          30,
          41,
          57,
          36,
          47,
          37,
          33,
          49,
          34,
          54,
          39,
          58,
          36,
          46,
          31,
          31,
          50,
          40,
          47,
          26,
          29,
          38,
          57,
          47,
          56,
          37,
          66,
          49,
          31,
          30,
          42,
          55,
          33,
          33,
          43,
          32,
          42,
          53,
          42,
          37,
          31,
          57,
          33,
          57,
          30,
          31,
          33,
          33,
          53,
          38,
          60,
          36,
          27,
          32,
          45,
          28,
          57,
          37,
          47,
          35,
          32,
          41,
          35,
          26,
          47,
          36,
          46,
          30,
          44,
          47,
          49,
          55,
          56,
          28,
          34,
          55,
          27,
          38,
          43,
          55,
          30,
          48,
          30,
          58,
          39,
          35,
          41,
          32,
          48,
          76,
          43,
          46,
          59,
          29,
          31,
          36,
          46,
          37,
          48,
          31,
          39,
          39,
          39,
          55,
          54,
          32,
          37,
          36,
          60,
          25,
          50,
          59,
          58,
          49,
          47,
          38,
          32,
          40,
          50,
          35,
          47,
          27,
          48,
          40,
          35,
          39,
          34,
          36,
          56,
          30,
          51,
          41,
          35,
          53,
          25,
          34,
          29,
          33,
          43,
          47,
          35,
          49,
          41,
          49,
          44,
          32,
          27,
          53,
          55,
          60,
          27,
          58,
          42,
          31,
          30,
          32,
          56,
          57,
          57,
          39,
          37,
          32,
          41,
          49,
          36,
          58,
          58,
          33,
          31,
          46,
          42,
          46,
          47,
          42,
          53,
          31,
          39,
          57,
          29,
          32,
          38,
          34,
          35,
          55,
          42,
          32,
          50,
          56,
          36,
          49,
          48,
          32,
          35,
          48,
          40,
          55,
          50,
          55,
          26,
          34,
          45,
          44,
          34,
          39,
          31,
          27,
          41,
          45,
          28,
          39,
          31,
          31,
          31,
          35,
          32,
          49,
          42,
          31,
          35,
          31,
          45,
          24,
          49,
          36,
          28,
          54,
          27,
          34,
          40,
          32,
          41,
          38,
          35,
          31,
          47,
          41,
          29,
          33,
          89,
          54,
          72,
          29,
          41,
          39,
          41,
          31,
          32,
          52,
          41,
          45,
          29,
          32,
          35,
          54,
          41,
          42,
          59,
          58,
          32,
          56,
          50,
          45,
          59,
          47,
          54,
          40,
          37,
          29,
          36,
          33,
          32,
          53,
          33,
          36,
          43,
          45,
          52,
          48,
          40,
          31,
          34,
          54,
          40,
          55,
          33,
          50,
          59,
          40,
          45,
          32,
          50,
          58,
          38,
          57,
          31,
          69,
          26,
          30,
          49,
          36,
          37,
          55,
          36,
          31,
          30,
          36,
          50,
          53,
          39,
          31,
          49,
          35,
          45,
          30,
          47,
          37,
          35,
          41,
          35,
          60,
          58,
          28,
          57,
          35,
          54,
          31,
          32,
          56,
          35,
          33,
          50,
          46,
          35,
          49,
          56,
          33,
          37,
          30,
          34,
          29,
          32,
          29,
          50,
          58,
          30,
          52,
          39,
          43,
          51,
          38,
          30,
          52,
          40,
          37,
          30,
          51,
          27,
          35,
          47,
          29,
          52,
          43,
          31,
          54,
          59,
          51,
          32,
          42,
          46,
          30,
          42,
          39,
          31,
          35,
          34,
          42,
          57,
          38,
          29,
          46,
          48,
          33,
          59,
          23,
          37,
          40,
          44,
          37,
          28,
          39,
          57,
          38,
          60,
          38,
          26,
          32,
          35,
          30,
          37,
          36,
          34,
          28,
          37,
          31,
          34,
          28,
          38,
          80,
          59,
          54,
          29,
          39,
          30,
          41,
          48,
          26,
          55,
          36,
          31,
          35,
          30,
          26,
          32,
          43,
          48,
          49,
          34,
          27,
          31,
          35,
          40,
          39,
          19,
          40,
          25,
          38,
          62,
          27,
          40,
          56,
          33,
          52,
          33,
          38,
          27,
          54,
          35,
          41,
          28,
          55,
          56,
          33,
          42,
          34,
          35,
          46,
          31,
          48,
          38,
          47,
          48,
          57,
          30,
          39,
          31,
          44,
          38,
          58,
          42,
          33,
          55,
          34,
          30,
          35,
          35,
          56,
          27,
          39,
          36,
          35,
          34,
          28,
          37,
          57,
          34,
          38,
          48,
          35,
          40,
          48,
          54,
          49,
          38,
          44,
          44,
          60,
          55,
          27,
          33,
          32,
          51,
          44,
          49,
          42,
          35,
          32,
          39,
          54,
          35,
          41,
          35,
          41,
          36,
          51,
          59,
          44,
          49,
          37,
          60,
          40,
          27,
          34,
          41,
          39,
          34,
          38,
          56,
          42,
          47,
          32,
          37,
          25,
          29,
          48,
          39,
          31,
          28,
          32,
          35,
          51,
          32,
          30,
          54,
          41,
          35,
          44,
          58,
          43,
          52,
          60,
          32,
          41,
          54,
          30,
          36,
          27,
          32,
          34,
          43,
          43,
          31,
          37,
          51,
          52,
          57,
          32,
          47,
          31,
          27,
          31,
          36,
          44,
          34,
          32,
          52,
          60,
          30,
          48,
          39,
          59,
          42,
          77,
          29,
          49,
          57,
          37,
          35,
          63,
          37,
          37,
          52,
          60,
          36,
          48,
          47,
          56,
          45,
          53,
          56,
          59,
          34,
          45,
          32,
          60,
          54,
          41,
          44,
          40,
          40,
          29,
          28,
          35,
          33,
          33,
          50,
          56,
          37,
          53,
          27,
          46,
          36,
          37,
          28,
          40,
          48,
          46,
          39,
          37,
          26,
          35,
          37,
          32,
          31,
          36,
          36,
          38,
          45,
          58,
          34,
          51,
          38,
          73,
          57,
          45,
          34,
          48,
          39,
          35,
          58,
          36,
          43,
          34,
          52,
          36,
          32,
          23,
          57,
          43,
          33,
          45,
          31,
          41,
          47,
          39,
          47,
          57,
          52,
          45,
          42,
          32,
          31,
          25,
          30,
          33,
          29,
          55,
          51,
          68,
          29,
          35,
          38,
          39,
          47,
          22,
          44,
          38,
          56,
          56,
          54,
          39,
          36,
          42,
          51,
          32,
          31,
          23,
          21,
          45,
          51,
          36,
          56,
          41,
          57,
          55,
          34,
          32,
          41,
          32,
          53,
          43,
          27,
          48,
          33,
          41,
          33,
          25,
          38,
          43,
          56,
          28,
          52,
          32,
          33,
          31,
          54,
          36,
          40,
          32,
          27,
          42,
          30,
          43,
          44,
          46,
          36,
          44,
          30,
          34,
          20,
          31,
          34,
          27,
          47,
          56,
          45,
          35,
          32,
          44,
          38,
          32,
          58,
          35,
          40,
          36,
          52,
          35,
          38,
          43,
          43,
          35,
          40,
          49,
          35,
          42,
          29,
          26,
          44,
          29,
          36,
          30,
          52,
          55,
          30,
          29,
          27,
          58,
          33,
          48,
          39,
          55,
          40,
          33,
          56,
          42,
          43,
          41,
          58,
          29,
          38,
          49,
          55,
          53,
          56,
          31,
          29,
          48,
          43,
          29,
          39,
          53,
          48,
          34,
          52,
          30,
          45,
          54,
          44,
          35,
          34,
          22,
          65,
          49,
          38,
          28,
          33,
          34,
          26,
          37,
          40,
          28,
          31,
          42,
          34,
          34,
          53,
          55,
          51,
          58,
          35,
          35,
          31,
          32,
          59,
          50,
          37,
          30,
          41,
          43,
          34,
          42,
          33,
          26,
          32,
          41,
          49,
          51,
          43,
          45,
          42,
          37,
          59,
          31,
          22,
          30,
          52,
          50,
          34,
          30,
          56,
          31,
          51,
          33,
          47,
          46,
          44,
          51,
          39,
          41,
          33,
          31,
          60,
          34,
          58,
          36,
          42,
          44,
          31,
          51,
          59,
          33,
          47,
          43,
          44,
          29,
          30,
          43,
          53,
          46,
          41,
          44,
          38,
          43,
          41,
          44,
          31,
          59,
          30,
          35,
          57,
          53,
          35,
          49,
          41,
          40,
          57,
          38,
          49,
          38,
          26,
          34,
          58,
          30,
          52,
          30,
          52,
          34,
          33,
          34,
          23,
          37,
          59,
          42,
          50,
          41,
          34,
          57,
          52,
          61,
          31,
          27,
          48,
          27,
          37,
          28,
          34,
          42,
          39,
          30,
          35,
          43,
          33,
          24,
          39,
          27,
          50,
          30,
          52,
          30,
          35,
          36,
          39,
          43,
          43,
          46,
          33,
          56,
          60,
          48,
          31,
          36,
          24,
          27,
          36,
          29,
          42,
          44,
          42,
          39,
          45,
          36,
          34,
          43,
          32,
          38,
          29,
          28,
          33,
          43,
          42,
          33,
          32,
          42,
          42,
          29,
          27,
          38,
          45,
          32,
          33,
          48,
          53,
          23,
          41,
          41,
          50,
          28,
          54,
          38,
          27,
          54,
          28,
          32,
          39,
          29,
          46,
          37,
          30,
          41,
          39,
          38,
          55,
          29,
          36,
          60,
          43,
          45,
          36,
          53,
          41,
          51,
          34,
          29,
          35,
          40,
          26,
          32,
          31,
          39,
          30,
          40,
          32,
          46,
          41,
          35,
          40,
          58,
          48,
          37,
          59,
          33,
          36,
          40,
          53,
          32,
          32,
          43,
          30,
          31,
          35,
          39,
          57,
          33,
          35,
          53,
          50,
          46,
          35,
          56,
          54,
          56,
          43,
          40,
          38,
          43,
          46,
          29,
          33,
          49,
          53,
          54,
          33,
          39,
          38,
          49,
          33,
          59,
          35,
          40,
          31,
          35,
          42,
          55,
          33,
          31,
          30,
          30,
          37,
          30,
          47,
          41,
          44,
          59,
          29,
          51,
          32,
          36,
          30,
          34,
          42,
          37,
          42,
          31,
          34,
          32,
          46,
          32,
          37,
          39,
          37,
          32,
          41,
          50,
          57,
          37,
          56,
          37,
          32,
          30,
          27,
          36,
          34,
          53,
          58,
          31,
          38,
          32,
          31,
          59,
          41,
          33,
          52,
          36,
          37,
          39,
          36,
          30,
          32,
          47,
          38,
          51,
          27,
          58,
          56,
          40,
          47,
          23,
          46,
          57,
          40,
          30,
          58,
          36,
          37,
          51,
          30,
          39,
          33,
          60,
          29,
          31,
          52,
          30,
          47,
          46,
          31,
          29,
          34,
          44,
          60,
          51,
          31,
          33,
          53,
          45,
          51,
          30,
          58,
          26,
          37,
          47,
          28,
          28,
          43,
          43,
          57,
          34,
          36,
          32,
          46,
          36,
          33,
          48,
          58,
          56,
          53,
          41,
          35,
          31,
          51,
          40,
          40,
          44,
          37,
          49,
          56,
          25,
          29,
          35,
          41,
          38,
          61,
          36,
          52,
          31,
          33,
          49,
          45,
          71,
          34,
          33,
          28,
          52,
          54,
          51,
          59,
          25,
          35,
          49,
          33,
          30,
          46,
          43,
          28,
          41,
          60,
          49,
          30,
          45,
          47,
          29,
          59,
          27,
          37,
          25,
          39,
          35,
          77,
          58,
          38,
          50,
          27,
          46,
          51,
          58,
          42,
          27,
          34,
          31,
          39,
          42,
          44,
          51,
          50,
          32,
          45,
          29,
          53,
          29,
          57,
          41,
          58,
          52,
          44,
          34,
          44,
          56,
          36,
          45,
          57,
          49,
          37,
          44,
          40,
          28,
          39,
          40,
          33,
          37,
          36,
          39,
          26,
          48,
          51,
          35,
          40,
          40,
          37,
          43,
          48,
          46,
          34,
          52,
          57,
          36,
          33,
          30,
          34,
          35,
          34,
          32,
          30,
          33,
          31,
          33,
          48,
          38,
          53,
          36,
          37,
          36,
          39,
          50,
          49,
          41,
          32,
          30,
          46,
          25,
          27,
          28,
          47,
          39,
          28,
          45,
          30,
          48,
          60,
          48,
          38,
          31,
          52,
          28,
          51,
          32,
          52,
          23,
          51,
          49,
          37,
          32,
          32,
          56,
          49,
          49,
          53,
          32,
          60,
          40,
          41,
          55,
          33,
          49,
          26,
          56,
          27,
          55,
          55,
          26,
          34,
          38,
          88,
          38,
          44,
          43,
          48,
          35,
          49,
          33,
          77,
          56,
          33,
          44,
          40,
          55,
          31,
          42,
          45,
          38,
          32,
          35,
          70,
          41,
          27,
          56,
          59,
          31,
          59,
          32,
          37,
          59,
          51,
          42,
          38,
          35,
          42,
          43,
          31,
          56,
          43,
          39,
          52,
          32,
          37,
          35,
          39,
          44,
          41,
          26,
          50,
          33,
          26,
          31,
          56,
          40,
          27,
          41,
          78,
          60,
          46,
          25,
          30,
          37,
          45,
          31,
          35,
          47,
          55,
          28,
          37,
          34,
          45,
          35,
          32,
          26,
          47,
          33,
          42,
          31,
          30,
          40,
          52,
          48,
          36,
          36,
          47,
          31,
          60,
          49,
          41,
          67,
          35,
          34,
          49,
          33,
          37,
          29,
          53,
          36,
          54,
          55,
          38,
          50,
          50,
          35,
          35,
          51,
          34,
          28,
          57,
          38,
          41,
          38,
          54,
          52,
          35,
          40,
          33,
          35,
          41,
          33,
          25,
          34,
          28,
          41,
          29,
          38,
          39,
          58,
          36,
          46,
          36,
          59,
          36,
          34,
          29,
          39,
          32,
          29,
          53,
          29,
          44,
          28,
          37,
          45,
          39,
          34,
          32,
          40,
          30,
          55,
          50,
          45,
          54,
          37,
          40,
          39,
          59,
          40,
          48,
          73,
          28,
          50,
          35,
          45,
          51,
          47,
          54,
          54,
          35,
          74,
          32,
          56,
          57,
          33,
          44,
          30,
          48,
          38,
          26,
          24,
          28,
          55,
          40,
          39,
          43,
          32,
          35,
          53,
          44,
          42,
          59,
          32,
          47,
          30,
          38,
          40,
          34,
          33,
          27,
          39,
          38,
          55,
          59,
          37,
          38,
          20,
          51,
          48,
          36,
          30,
          45,
          31,
          40,
          58,
          35,
          42,
          48,
          52,
          30,
          43,
          43,
          30,
          46,
          43,
          36,
          42,
          41,
          50,
          25,
          36,
          59,
          50,
          45,
          49,
          54,
          33,
          67,
          31,
          59,
          28,
          58,
          30,
          53,
          51,
          47,
          30,
          38,
          53,
          59,
          57,
          36,
          42,
          56,
          38,
          47,
          30,
          42,
          34,
          29,
          49,
          59,
          30,
          37,
          27,
          24,
          55,
          60,
          30,
          34,
          36,
          44,
          38,
          60,
          58,
          30,
          39,
          44,
          32,
          32,
          47,
          37,
          86,
          38,
          31,
          49,
          53,
          55,
          30,
          41,
          31,
          39,
          43,
          45,
          42,
          35,
          43,
          54,
          43,
          33,
          31,
          42,
          34,
          36,
          51,
          50,
          32,
          37,
          50,
          52,
          30,
          38,
          50,
          32,
          24,
          36,
          42,
          50,
          41,
          34,
          29,
          47,
          44,
          49,
          39,
          31,
          36,
          44,
          29,
          33,
          40,
          33,
          54,
          32,
          57,
          46,
          30,
          35,
          36,
          37,
          41,
          38,
          34,
          45,
          53,
          42,
          55,
          57,
          58,
          48,
          33,
          43,
          29,
          26,
          56,
          54,
          33,
          36,
          46,
          40,
          50,
          55,
          41,
          26,
          35,
          53,
          31,
          59,
          42,
          45,
          58,
          33,
          47,
          31,
          40,
          28,
          42,
          53,
          33,
          28,
          50,
          30,
          37,
          39,
          29,
          32,
          38,
          55,
          38,
          47,
          57,
          32,
          37,
          30,
          39,
          45,
          25,
          53,
          39,
          35,
          58,
          27,
          39,
          41,
          56,
          54,
          46,
          35,
          41,
          36,
          37,
          28,
          49,
          41,
          60,
          30,
          48,
          35,
          28,
          42,
          32,
          40,
          53,
          55,
          41,
          33,
          58,
          32,
          36,
          31,
          33,
          45,
          40,
          29,
          53,
          53,
          35,
          33,
          38,
          31,
          48,
          25,
          35,
          55,
          31,
          43,
          31,
          56,
          34,
          55,
          25,
          57,
          47,
          46,
          41,
          26,
          42,
          37,
          34,
          49,
          32,
          38,
          37,
          32,
          43,
          39,
          34,
          38,
          40,
          34,
          34,
          46,
          31,
          42,
          45,
          54,
          45,
          36,
          28,
          50,
          39,
          33,
          30,
          32,
          27,
          34,
          47,
          39,
          30,
          31,
          38,
          54,
          34,
          26,
          41,
          31,
          31,
          33,
          35,
          58,
          38,
          35,
          38,
          55,
          31,
          28,
          40,
          33,
          33,
          31,
          25,
          41,
          34,
          37,
          41,
          52,
          54,
          33,
          60,
          25,
          33,
          37,
          50,
          40,
          60,
          42,
          40,
          27,
          44,
          34,
          34,
          38,
          31,
          57,
          31,
          31,
          56,
          46,
          31,
          40,
          54,
          26,
          51,
          30,
          54,
          32,
          54,
          45,
          31,
          35,
          42,
          37,
          43,
          30,
          49,
          37,
          54,
          45,
          39,
          50,
          30,
          49,
          31,
          42,
          40,
          35,
          30,
          40,
          39,
          46,
          55,
          32,
          37,
          58,
          36,
          31,
          49,
          32,
          30,
          55,
          32,
          52,
          38,
          60,
          60,
          30,
          44,
          32,
          46,
          34,
          40,
          34,
          43,
          52,
          35,
          34,
          33,
          39,
          32,
          43,
          34
         ]
        },
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Churn",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          59,
          56,
          41,
          55,
          54,
          42,
          56,
          60,
          37,
          28,
          38,
          30,
          29,
          46,
          31,
          35,
          32,
          49,
          41,
          49,
          28,
          43,
          43,
          43,
          37,
          35,
          31,
          43,
          31,
          28,
          32,
          60,
          26,
          40,
          33,
          32,
          35,
          33,
          38,
          23,
          60,
          48,
          45,
          36,
          52,
          35,
          43,
          52,
          53,
          48,
          41,
          39,
          59,
          41,
          48,
          40,
          48,
          60,
          40,
          57,
          51,
          41,
          41,
          52,
          59,
          44,
          49,
          40,
          41,
          44,
          60,
          29,
          41,
          41,
          42,
          36,
          39,
          31,
          26,
          31,
          29,
          37,
          33,
          24,
          50,
          27,
          54,
          30,
          34,
          37,
          41,
          47,
          31,
          31,
          43,
          37,
          30,
          41,
          36,
          35,
          29,
          35,
          31,
          35,
          34,
          31,
          44,
          36,
          35,
          41,
          31,
          34,
          32,
          36,
          30,
          37,
          36,
          31,
          27,
          34,
          30,
          32,
          37,
          32,
          33,
          35,
          35,
          30,
          45,
          45,
          33,
          44,
          40,
          31,
          30,
          44,
          34,
          37,
          28,
          32,
          37,
          34,
          38,
          28,
          31,
          59,
          24,
          26,
          59,
          30,
          37,
          30,
          25,
          44,
          29,
          26,
          26,
          27,
          25,
          28,
          23,
          34,
          28,
          51,
          35,
          35,
          56,
          45,
          60,
          56,
          27,
          30,
          45,
          29,
          38,
          30,
          44,
          36,
          38,
          36,
          37,
          35,
          38,
          40,
          42,
          46,
          35,
          34,
          32,
          50,
          35,
          36,
          53,
          46,
          26,
          45,
          31,
          29,
          53,
          27,
          39,
          36,
          41,
          36,
          32,
          35,
          36,
          35,
          30,
          42,
          42,
          32,
          29,
          44,
          32,
          31,
          40,
          43,
          27,
          37,
          23,
          25,
          26,
          47,
          35,
          32,
          45,
          30,
          29,
          38,
          23,
          27,
          27,
          29,
          30,
          36,
          35,
          36,
          59,
          47,
          58,
          37,
          46,
          43,
          27,
          38,
          53,
          47,
          56,
          41,
          29,
          50,
          45,
          35,
          26,
          26,
          37,
          30,
          38,
          37,
          53,
          29,
          35,
          28,
          55,
          49,
          50,
          43,
          58,
          38,
          48,
          59,
          58,
          36,
          42,
          41,
          42,
          39,
          57,
          52,
          59,
          56,
          49,
          35,
          36,
          40,
          55,
          58,
          39,
          47,
          40,
          35,
          48,
          48,
          24,
          49,
          36,
          52,
          42,
          36,
          36,
          48,
          51,
          51,
          46,
          37,
          36,
          47,
          40,
          38,
          40,
          39,
          60,
          39,
          51,
          27,
          41,
          42,
          41,
          33,
          46,
          31,
          41,
          41,
          55,
          53,
          55,
          39,
          40,
          59,
          42,
          41,
          27,
          25,
          43,
          44,
          26,
          52,
          54,
          56,
          57,
          29,
          47,
          57,
          40,
          46,
          43,
          45,
          24,
          40,
          38,
          36,
          27,
          26,
          43,
          25,
          27,
          30,
          28,
          27,
          32,
          41,
          37,
          27,
          29,
          33,
          36,
          28,
          55,
          51,
          39,
          42,
          33,
          36,
          45,
          37,
          46,
          32,
          28,
          45,
          41,
          59,
          47,
          31,
          31,
          33,
          52,
          26,
          25,
          25,
          28,
          33,
          45,
          36,
          26,
          38,
          56,
          26,
          26,
          25,
          26,
          40,
          37,
          43,
          41,
          49,
          29,
          39,
          38,
          26,
          32,
          33,
          31,
          49,
          27,
          30,
          35,
          26,
          45,
          34,
          28,
          25,
          30,
          30,
          47,
          57,
          38,
          54,
          25,
          30,
          26,
          31,
          29,
          49,
          60,
          60,
          29,
          55,
          44,
          47,
          35,
          35,
          38,
          34,
          50,
          43,
          44,
          51,
          31,
          39,
          59,
          25,
          58,
          30,
          34,
          51,
          60,
          51,
          37,
          30,
          46,
          29,
          27,
          28,
          34,
          29,
          27,
          26,
          36,
          44,
          39,
          30,
          31,
          56,
          29,
          43,
          32,
          30,
          40,
          42,
          27,
          35,
          35,
          32,
          39,
          31,
          26,
          42,
          27,
          33,
          51,
          33,
          30,
          52,
          42,
          31,
          27,
          44,
          40,
          46,
          32,
          56,
          33,
          33,
          32,
          41,
          31,
          30,
          34,
          43,
          28,
          32,
          31,
          35,
          31,
          25,
          56,
          47,
          39,
          51,
          54,
          58,
          42,
          56,
          55,
          26,
          40,
          26,
          35,
          34,
          41,
          44,
          33,
          36,
          38,
          35,
          41,
          41,
          28,
          26,
          39,
          25,
          37,
          25,
          46,
          25,
          28,
          33,
          32,
          32,
          47,
          50,
          27,
          45,
          51,
          36,
          51,
          33,
          59,
          38,
          40,
          51,
          41,
          38,
          31,
          28,
          41,
          33,
          25,
          45,
          31,
          28,
          49,
          25,
          28,
          54,
          28,
          27,
          49,
          53,
          26,
          37,
          34,
          51,
          39,
          35,
          51,
          30,
          42,
          54,
          51,
          57,
          30,
          37,
          39,
          35,
          48,
          31,
          35,
          48,
          53,
          27,
          55,
          40,
          40,
          38,
          54,
          54,
          50,
          48,
          41,
          51,
          42,
          58,
          47,
          58,
          41,
          50,
          46,
          44,
          36,
          58,
          44,
          49,
          55,
          52,
          60,
          41,
          36,
          41,
          50,
          44,
          54,
          35,
          59,
          42,
          27,
          59,
          43,
          39,
          33,
          36,
          35,
          50,
          55,
          51,
          48,
          55,
          30,
          53,
          40,
          44,
          49,
          40,
          58,
          49,
          38,
          51,
          36,
          55,
          43,
          60,
          57,
          41,
          50,
          53,
          59,
          59,
          52,
          58,
          59,
          49,
          33,
          55,
          42,
          52,
          38,
          55,
          49,
          56,
          46,
          57,
          61,
          45,
          39,
          46,
          43,
          34,
          35,
          36,
          45,
          47,
          42,
          40,
          40,
          32,
          33,
          32,
          46,
          50,
          51,
          30,
          39,
          47,
          40,
          50,
          60,
          32,
          57,
          41,
          50,
          46,
          31,
          36,
          45,
          33,
          47,
          35,
          38,
          33,
          44,
          31,
          31,
          49,
          52,
          31,
          35,
          31,
          36,
          39,
          46,
          48,
          36,
          30,
          33,
          57,
          33,
          35,
          53,
          41,
          32,
          41,
          35,
          48,
          34,
          47,
          46,
          60,
          39,
          57,
          36,
          30,
          33,
          30,
          36,
          49,
          49,
          40,
          49,
          38,
          31,
          34,
          39,
          40,
          47,
          37,
          44,
          55,
          40,
          54,
          35,
          32,
          36,
          37,
          37,
          50,
          58,
          52,
          31,
          40,
          34,
          53,
          43,
          30,
          34,
          48,
          33,
          37,
          57,
          55,
          34,
          44,
          35,
          34,
          49,
          30,
          32,
          56,
          50,
          32,
          45,
          52,
          60,
          47,
          32,
          55,
          36,
          46,
          55,
          30,
          54,
          31,
          46,
          61,
          58,
          38,
          49,
          43,
          52,
          33,
          33,
          53,
          40,
          44,
          42,
          33,
          53,
          60,
          33,
          37,
          45,
          34,
          56,
          57,
          32,
          50,
          30,
          56,
          55,
          34,
          52,
          32,
          31,
          44,
          33,
          53,
          46,
          46,
          59,
          51,
          57,
          43,
          48,
          33,
          59,
          58,
          32,
          44,
          47,
          34,
          51,
          60,
          46,
          41,
          34,
          31,
          37,
          59,
          55,
          33,
          30,
          36,
          44,
          36,
          33,
          36,
          53,
          47,
          39,
          35,
          32,
          46,
          30,
          36,
          47,
          52,
          43,
          44,
          30,
          32,
          42,
          33,
          45,
          45,
          28,
          32,
          28,
          44,
          51,
          56,
          40,
          49,
          50,
          58,
          37,
          27,
          47,
          39,
          43,
          34,
          35,
          58,
          35,
          42,
          43,
          34,
          35,
          38,
          39,
          58,
          60,
          56,
          31,
          37,
          59,
          49,
          30,
          31,
          40,
          43,
          48,
          34,
          28,
          34,
          47,
          59,
          32,
          31,
          27,
          35,
          32,
          57,
          58,
          30,
          37,
          44,
          57,
          34,
          54,
          55,
          59,
          53,
          37,
          45,
          49,
          31,
          31,
          35,
          37,
          48,
          39,
          34,
          49,
          41,
          46,
          54,
          38,
          39,
          46,
          58,
          32,
          30,
          51,
          30,
          35,
          30,
          30,
          43,
          47,
          51,
          32,
          53,
          33,
          40,
          42,
          46,
          32,
          38,
          32,
          30,
          35,
          36,
          43,
          42,
          40,
          32,
          32,
          41,
          34,
          40,
          32,
          32,
          36,
          45,
          47,
          35,
          36,
          33,
          30,
          32,
          45,
          47,
          46,
          39,
          30,
          45,
          43,
          53,
          43,
          54,
          30,
          30,
          36,
          33,
          38,
          37,
          32,
          59,
          59,
          46,
          32,
          52,
          45,
          52,
          31,
          31,
          39,
          35,
          33,
          50,
          36,
          34,
          46,
          47,
          39,
          31,
          41,
          59,
          39,
          48,
          50,
          31,
          30,
          31,
          30,
          34,
          53,
          34,
          57,
          41,
          40,
          37,
          31,
          58,
          39,
          47,
          44,
          45,
          41,
          33,
          30,
          30,
          42,
          49,
          59,
          31,
          43,
          38,
          30,
          35,
          43,
          34,
          52,
          46,
          46,
          37,
          42,
          23,
          53,
          26,
          49,
          34,
          28,
          46,
          26,
          31,
          51,
          47,
          53,
          29,
          32,
          26,
          40,
          33,
          49,
          60,
          41,
          36,
          29,
          42,
          33,
          31,
          31,
          26,
          40,
          45,
          55,
          29,
          47,
          50,
          44,
          56,
          39,
          59,
          43,
          36,
          29,
          38,
          41,
          58,
          61,
          36,
          30,
          43,
          39,
          48,
          26,
          51,
          37,
          29,
          57,
          25,
          68,
          34,
          38,
          54,
          32,
          41,
          48,
          59,
          39,
          31,
          30,
          40,
          28,
          52,
          32,
          34,
          31,
          57,
          31,
          29,
          75,
          32,
          31,
          41,
          22,
          31,
          69,
          30,
          49,
          29,
          43,
          28,
          56,
          39,
          33,
          38,
          30,
          29,
          69,
          44,
          35,
          31,
          26,
          26,
          34,
          42,
          54,
          53,
          54,
          48,
          40,
          28,
          34,
          34,
          32,
          31,
          31,
          66,
          37,
          33,
          50,
          33,
          56,
          26,
          59,
          32,
          22,
          34,
          39,
          28,
          31,
          33,
          66,
          26,
          30,
          28,
          29,
          49,
          34,
          47,
          42,
          43,
          61,
          35,
          40,
          31,
          33,
          69,
          30,
          41,
          31,
          22,
          37,
          48,
          85,
          72,
          46,
          26,
          37,
          53,
          33,
          90,
          54,
          67,
          42,
          71,
          44,
          47,
          44,
          37,
          37,
          28,
          21,
          32,
          51,
          26,
          74,
          65,
          47,
          38,
          71,
          29,
          61,
          57,
          37,
          68,
          29,
          51,
          41,
          24,
          24,
          59,
          85,
          51,
          39,
          44,
          26,
          48,
          37,
          71,
          62,
          26,
          43,
          30,
          50,
          25,
          25,
          31,
          28,
          66,
          41,
          46,
          42,
          28,
          39,
          42,
          35,
          55,
          37,
          29,
          49,
          44,
          54,
          28,
          30,
          47,
          34,
          37,
          25,
          29,
          62,
          58,
          46,
          56,
          33,
          27,
          60,
          32,
          83,
          35,
          31,
          30,
          29,
          36,
          57,
          21,
          44,
          23,
          34,
          32,
          37,
          35,
          47,
          61,
          30,
          70,
          52,
          28,
          57,
          24,
          32,
          65,
          44,
          32,
          33,
          53,
          31,
          58,
          30,
          47,
          60,
          36,
          37,
          43,
          37,
          31,
          31,
          30,
          30,
          32,
          35,
          59,
          33,
          37,
          31,
          26,
          53,
          34,
          46,
          76,
          46,
          83,
          23,
          34,
          24,
          41,
          28,
          34,
          77,
          36,
          58,
          46,
          59,
          19,
          23,
          27,
          73,
          41,
          30,
          26,
          38,
          46,
          27,
          63,
          34,
          32,
          33,
          47,
          48,
          58,
          50,
          26,
          39,
          55,
          41,
          54,
          33,
          42,
          50,
          40,
          37,
          44,
          30,
          26,
          47,
          46,
          25,
          56,
          36,
          35,
          54,
          48,
          32,
          60,
          67,
          32,
          26,
          33,
          21,
          30,
          27,
          30,
          36,
          27,
          29,
          38,
          23,
          38,
          30,
          41,
          33,
          35,
          29,
          68,
          20,
          30,
          32,
          28,
          45,
          29,
          31,
          39,
          42,
          35,
          34,
          26,
          29,
          44,
          34,
          33,
          34,
          33,
          22,
          32,
          36,
          44,
          65,
          61,
          41,
          24,
          77,
          62,
          30,
          29,
          29,
          65,
          43,
          70,
          32,
          46,
          33,
          57,
          62,
          32,
          39,
          76,
          36,
          27,
          38,
          56,
          51,
          39,
          35,
          32,
          51,
          37,
          52,
          35,
          47,
          31,
          49,
          30,
          35,
          44,
          42,
          29,
          42,
          43,
          36,
          36,
          32,
          54,
          50,
          32,
          28,
          40,
          34,
          52,
          35,
          32,
          60,
          34,
          30,
          39,
          40,
          38,
          35,
          37,
          29,
          36,
          34,
          33,
          29,
          33,
          41,
          74,
          53,
          38,
          41,
          52,
          40,
          32,
          28,
          28,
          35,
          46,
          50,
          44,
          29,
          61,
          70,
          44,
          59,
          45,
          39,
          36,
          78,
          28,
          50,
          73,
          34,
          28,
          42,
          54,
          27,
          73,
          33,
          51,
          32,
          33,
          26,
          32,
          41,
          39,
          24,
          37,
          35,
          26,
          29,
          63,
          95,
          65,
          59,
          35,
          74,
          56,
          26,
          30,
          29,
          78,
          45,
          29,
          39,
          45,
          45,
          36,
          41,
          38,
          33,
          30,
          65,
          75,
          65,
          53,
          40,
          31,
          25,
          37,
          32,
          45,
          35,
          74,
          37,
          59,
          76,
          25,
          27,
          39,
          24,
          28,
          51,
          66,
          28,
          30,
          58,
          32,
          31,
          71,
          37,
          56,
          65,
          72,
          51,
          35,
          34,
          29,
          30,
          39,
          45,
          28,
          52,
          59,
          38,
          47,
          52,
          38,
          29,
          36,
          58,
          31,
          59,
          47,
          36,
          21,
          23,
          47,
          29,
          25,
          37,
          33,
          59,
          58,
          44,
          48,
          34,
          42,
          54,
          42,
          42,
          44,
          37,
          29,
          47,
          24,
          34,
          30,
          46,
          34,
          59,
          35,
          33,
          35,
          37,
          35,
          38,
          33,
          42,
          33,
          53,
          53,
          31,
          26,
          30,
          24,
          28,
          26,
          35,
          28,
          30,
          32,
          52,
          38,
          34,
          29,
          30,
          31,
          24,
          25,
          47,
          33,
          34,
          36,
          50,
          34,
          54,
          47,
          31,
          36,
          35,
          25,
          46,
          49,
          47,
          35,
          30,
          31,
          55,
          25,
          26,
          85,
          29,
          39,
          33,
          33,
          58,
          61,
          50,
          54,
          33,
          36,
          31,
          24,
          29,
          40,
          29,
          30,
          45,
          37,
          28,
          33,
          33,
          45,
          25,
          41,
          38,
          47,
          49,
          42,
          33,
          36,
          60,
          33,
          33,
          33,
          32,
          36,
          45,
          35,
          38,
          30,
          31,
          32,
          44,
          39,
          45,
          32,
          39,
          30,
          63,
          56,
          26,
          37,
          29,
          24,
          35,
          43,
          39,
          59,
          50,
          44,
          35,
          30,
          58,
          37,
          39,
          31,
          33,
          41,
          44,
          46,
          30,
          35,
          27,
          39,
          32,
          37,
          31,
          30,
          34,
          31,
          27,
          36,
          32,
          49,
          29,
          29,
          28,
          60,
          33,
          35,
          31,
          40,
          29,
          35,
          32,
          26,
          42,
          33,
          36,
          35,
          30,
          59,
          44,
          50,
          51,
          51,
          60,
          29,
          43,
          31,
          26,
          23,
          49,
          42,
          39,
          45,
          45,
          39,
          45,
          39,
          23,
          57,
          32,
          29,
          42,
          47,
          36,
          57,
          36,
          53,
          56,
          33,
          59,
          47,
          42,
          38,
          40,
          41,
          43,
          24,
          40,
          40,
          34,
          36,
          38,
          39,
          25,
          39,
          41,
          36,
          34,
          34,
          38,
          40,
          31,
          37,
          58,
          48,
          35,
          34,
          51,
          39,
          39,
          36,
          47,
          35,
          33,
          47,
          38,
          46,
          42,
          47,
          37,
          31,
          46,
          36,
          40,
          39,
          45,
          59,
          34,
          40,
          50,
          36,
          35,
          36,
          35,
          47,
          47,
          38,
          33,
          38,
          39,
          43,
          33,
          34,
          28,
          34,
          42,
          33,
          39,
          32,
          34,
          32,
          48,
          31,
          33,
          48,
          33,
          30,
          35,
          51,
          38,
          39,
          27,
          50,
          34,
          38,
          29,
          40,
          32,
          38,
          36,
          28,
          49,
          26,
          44,
          49,
          31,
          55,
          32,
          35,
          35,
          31,
          28,
          39,
          48,
          32,
          41,
          34,
          32,
          34,
          36,
          29,
          54,
          29,
          35,
          36,
          37,
          37,
          31,
          33,
          59,
          30,
          53,
          26,
          30,
          35,
          34,
          33,
          59,
          36,
          34,
          39,
          42,
          60,
          56,
          30,
          26,
          35,
          22,
          26,
          28,
          35,
          35,
          55,
          32,
          28,
          31,
          23,
          26,
          46,
          34,
          60,
          28,
          29,
          39,
          54,
          34,
          27,
          28,
          39,
          60,
          26,
          32,
          40,
          25,
          48,
          37,
          39,
          40,
          31,
          42,
          51,
          28,
          32,
          36,
          36,
          38,
          36,
          40,
          37,
          51,
          34,
          56,
          35,
          34,
          39,
          29,
          33,
          40,
          22,
          32,
          33,
          29,
          29,
          24,
          31,
          27,
          32,
          30,
          27,
          55,
          28,
          29,
          55,
          25,
          36,
          27,
          25,
          32,
          33,
          38,
          45,
          30,
          32,
          26,
          25,
          33,
          32,
          34,
          29,
          28,
          38,
          35,
          28,
          29,
          20,
          45,
          37,
          32,
          59,
          37,
          59,
          27,
          39,
          43,
          39,
          26,
          39,
          43,
          27,
          32,
          29,
          42,
          53,
          36,
          27,
          43,
          60,
          29,
          26,
          26,
          28,
          28,
          28,
          40,
          24,
          56,
          34,
          28,
          37,
          32,
          43,
          36,
          35,
          29,
          27,
          32,
          33,
          24,
          58,
          43,
          26,
          31,
          25,
          58,
          48,
          60,
          25,
          26,
          25,
          26,
          32,
          25,
          21,
          41,
          30,
          47,
          37,
          34,
          28,
          24,
          26,
          35,
          54,
          44,
          56,
          25,
          25,
          50,
          55,
          57,
          48,
          25,
          49,
          25,
          38,
          45,
          33,
          45,
          26,
          49,
          25,
          53,
          43,
          49,
          54,
          47,
          34,
          54,
          43,
          23,
          48,
          30,
          38,
          34,
          52,
          27,
          40,
          47,
          26,
          37,
          34,
          26,
          44,
          29,
          40,
          43,
          52,
          38,
          33,
          50,
          53,
          42,
          49,
          27,
          32,
          31,
          29,
          49,
          46,
          57,
          28,
          37,
          38,
          49,
          35,
          29,
          52,
          34,
          60,
          32,
          27,
          55,
          29,
          42,
          33,
          29,
          39,
          31,
          60,
          25,
          45,
          29,
          55,
          26,
          30,
          54,
          39,
          26,
          57,
          39,
          25,
          43,
          32,
          43,
          50,
          35,
          37,
          42,
          27,
          27,
          29,
          29,
          26,
          39,
          41,
          29,
          33,
          32,
          30,
          38,
          55,
          25,
          38,
          35,
          31,
          25,
          31,
          27,
          34,
          40,
          27,
          28,
          28,
          35,
          58,
          44,
          57,
          25,
          21,
          49,
          54,
          59,
          28,
          37,
          30,
          35,
          31,
          41,
          30,
          28,
          31,
          53,
          22,
          36,
          33,
          31,
          49,
          51,
          34,
          35,
          32,
          36,
          31,
          25,
          25,
          60,
          40,
          50,
          32,
          60,
          32,
          52,
          59,
          56,
          47,
          43,
          25,
          31,
          27,
          26,
          37,
          27,
          49,
          26,
          28,
          33,
          31,
          32,
          30,
          34,
          58,
          34,
          26,
          49,
          57,
          83,
          61,
          64,
          37,
          68,
          71,
          54,
          37,
          22,
          44,
          37,
          72,
          69,
          41,
          44,
          32,
          79,
          33,
          45,
          40,
          42,
          30,
          28,
          58,
          39,
          31,
          45,
          56,
          46,
          56,
          27,
          30,
          26,
          35,
          52,
          54,
          41,
          24,
          28,
          62,
          52,
          23,
          82,
          63,
          32,
          31,
          37,
          40,
          66,
          37,
          38,
          27,
          58,
          49,
          31,
          31,
          38,
          79,
          71,
          52,
          55,
          46,
          28,
          22,
          49,
          20,
          19,
          34,
          73,
          27,
          59,
          25,
          44,
          28,
          25,
          28,
          52,
          57,
          49,
          33,
          47,
          68,
          40,
          46,
          29,
          55,
          30,
          49,
          48,
          21,
          45,
          43,
          47,
          27,
          20,
          37,
          61,
          57,
          40,
          53,
          65,
          67,
          66,
          67,
          57,
          38,
          56,
          46,
          69,
          32,
          72,
          41,
          72,
          33,
          37,
          34,
          44,
          42,
          58,
          34,
          55,
          71,
          23,
          60,
          18,
          54,
          59,
          32,
          61,
          31,
          42,
          42,
          42,
          70,
          57,
          60,
          53,
          48,
          30,
          39,
          42,
          26,
          46,
          40,
          39,
          58,
          42,
          31,
          21,
          35,
          50,
          36,
          75,
          20,
          57,
          41,
          59,
          36,
          18,
          34,
          55,
          33,
          43,
          34,
          35,
          43,
          74,
          45,
          38,
          32,
          29,
          31,
          50,
          44,
          23,
          56,
          76,
          28,
          34,
          56,
          45,
          36,
          57,
          49,
          49,
          37,
          46,
          26,
          47,
          36,
          37,
          62,
          26,
          45,
          63,
          25,
          60,
          44,
          33,
          39,
          46,
          42,
          56,
          82,
          77,
          36,
          35,
          50,
          62,
          26,
          55,
          22,
          30,
          34,
          31,
          55,
          49,
          67,
          43,
          25,
          59,
          44,
          42,
          29,
          32,
          30,
          38,
          22,
          25,
          34,
          27,
          35,
          45,
          42,
          20,
          60,
          47,
          25,
          65,
          50,
          22,
          43,
          53,
          71,
          28,
          59,
          29,
          51,
          48,
          37,
          37,
          64,
          69,
          36,
          43,
          63,
          70,
          60,
          86,
          72,
          60,
          43,
          57,
          28,
          36,
          66,
          33,
          36,
          29,
          62,
          34,
          29,
          30,
          32,
          39,
          55,
          36,
          27,
          45,
          55,
          52,
          49,
          30,
          39,
          48,
          36,
          27,
          51,
          48,
          70,
          63,
          34,
          46,
          31,
          32,
          40,
          30,
          55,
          57,
          59,
          26,
          73,
          30,
          64,
          36,
          55,
          63,
          52,
          34,
          36,
          31,
          47,
          50,
          70,
          18,
          33,
          24,
          25,
          26,
          32,
          25,
          58,
          42,
          30,
          76,
          79,
          70,
          30,
          60,
          51,
          71,
          31,
          32,
          34,
          28,
          45,
          53,
          38,
          56,
          42,
          47,
          73,
          46,
          73,
          28,
          46,
          74,
          71,
          41,
          73,
          47,
          49,
          40,
          57,
          50,
          34,
          56,
          32,
          50,
          54,
          26,
          40,
          29,
          52,
          38,
          46,
          26,
          25,
          70,
          83,
          43,
          45,
          37,
          71,
          27,
          47,
          42,
          77,
          61,
          57,
          34,
          40,
          84,
          26,
          37,
          47,
          37,
          28,
          54,
          54,
          61,
          30,
          60,
          44,
          33,
          25,
          54,
          24,
          36,
          58,
          33,
          27,
          60,
          56,
          25,
          21,
          73,
          67,
          51,
          48,
          35,
          61,
          35,
          36,
          83,
          50,
          66,
          58,
          30,
          54,
          26,
          43,
          40,
          57,
          75,
          77,
          31,
          22,
          50,
          27,
          73,
          37,
          44,
          40,
          61,
          61,
          47,
          47,
          76,
          30,
          39,
          72,
          77,
          36,
          65,
          54,
          46,
          28,
          61,
          53,
          27,
          55,
          33,
          87,
          57,
          33,
          51,
          76,
          19,
          33,
          55,
          31,
          92,
          30,
          68,
          41,
          29,
          27,
          78,
          82,
          29,
          76,
          38,
          60,
          33,
          43,
          61,
          43,
          57,
          33,
          77,
          35,
          59,
          76,
          73,
          81,
          41,
          57,
          28,
          38,
          52,
          22,
          53,
          60,
          56,
          38,
          44,
          69,
          36,
          75,
          65,
          64,
          30,
          57,
          31,
          36,
          64,
          54,
          26,
          51,
          72,
          30,
          60,
          30,
          66,
          50,
          47,
          80,
          61,
          47,
          30,
          21,
          62,
          26,
          36,
          87,
          61,
          70,
          60,
          41,
          22,
          60,
          54,
          35,
          59,
          29,
          48,
          37,
          36,
          52,
          67,
          36,
          64,
          29,
          27,
          76,
          34,
          54,
          67,
          24,
          53,
          33,
          24,
          32,
          60,
          64,
          35,
          24,
          42,
          40,
          33,
          71,
          52,
          47,
          49,
          26,
          40,
          32,
          33,
          35,
          53,
          20,
          44,
          21,
          36,
          31,
          42,
          33,
          29,
          30,
          32,
          54,
          82,
          47,
          48,
          77,
          38,
          27,
          43,
          33,
          27,
          76,
          81,
          71,
          39,
          18,
          58,
          44,
          67,
          31,
          61,
          36,
          56,
          24,
          33,
          34,
          58,
          64,
          27,
          63,
          23,
          37,
          43,
          35,
          33,
          75,
          40,
          52,
          55,
          71,
          78,
          35,
          52,
          31,
          31,
          38,
          28,
          72,
          29,
          46,
          53,
          58,
          22,
          28,
          31,
          26,
          60,
          29,
          36,
          68,
          48,
          53,
          39,
          28,
          72,
          29,
          26,
          49,
          43,
          28,
          44,
          73,
          39,
          29,
          31,
          30,
          50,
          42,
          37,
          49,
          59,
          80,
          86,
          69,
          79,
          36,
          49,
          69,
          66,
          32,
          47,
          58,
          64,
          72,
          79,
          30,
          36,
          53,
          34,
          29,
          29,
          81,
          57,
          72,
          34,
          57,
          66,
          27,
          50,
          75,
          50,
          32,
          59,
          68,
          31,
          64,
          30,
          60,
          35,
          72,
          30,
          52,
          33,
          84,
          43,
          39,
          25,
          38,
          44,
          20,
          32,
          23,
          35,
          61,
          46,
          70,
          25,
          57,
          55,
          33,
          73,
          34,
          31,
          25,
          30,
          28,
          21,
          87,
          22,
          28,
          31,
          60,
          41,
          24,
          54,
          35,
          70,
          62,
          59,
          48,
          45,
          36,
          33,
          32,
          60,
          39,
          37,
          85,
          79,
          64,
          42,
          47,
          66,
          61,
          27,
          63,
          36,
          38,
          92,
          77,
          30,
          81,
          42,
          48,
          52,
          38,
          61,
          20,
          39,
          44,
          73,
          36,
          48,
          52,
          30,
          36,
          80,
          33,
          56,
          54,
          34,
          66,
          52,
          60,
          28,
          56,
          36,
          45,
          60,
          33,
          77,
          66,
          45,
          30,
          56,
          64,
          30,
          33,
          72,
          44,
          37,
          52,
          37,
          34,
          78,
          45,
          68,
          53,
          39,
          55,
          32,
          76,
          35,
          55,
          41,
          35,
          80,
          38,
          48,
          25,
          36,
          29,
          58,
          46,
          23,
          67,
          30,
          22,
          45,
          40,
          35,
          33,
          60,
          24,
          18,
          65,
          27,
          35,
          39,
          53,
          23,
          34,
          36,
          52,
          36,
          61,
          58,
          48,
          34,
          58,
          39,
          44,
          54,
          27,
          77,
          30,
          61,
          76,
          28,
          35,
          59,
          42,
          35,
          71,
          60,
          34,
          57,
          75,
          64,
          36,
          29,
          34,
          26,
          41,
          47,
          52,
          32,
          21,
          54,
          29,
          32,
          38,
          48,
          46,
          32,
          49,
          37,
          37,
          23,
          58,
          45,
          30,
          23,
          34,
          27,
          28,
          61,
          27,
          34,
          52,
          28,
          35,
          66,
          52,
          26,
          38,
          59,
          47,
          22,
          82,
          60,
          53,
          39,
          27,
          35,
          80,
          47,
          72,
          45,
          37,
          29,
          40,
          32,
          20,
          48,
          60,
          70,
          78,
          52,
          61,
          27,
          63,
          59,
          22,
          66,
          74,
          52,
          71,
          33,
          56,
          30,
          73,
          28,
          27,
          38,
          28,
          37,
          47,
          60,
          42,
          27,
          25,
          90,
          29,
          29,
          63,
          32,
          32,
          42,
          26,
          73,
          26,
          60,
          25,
          64,
          68,
          39,
          40,
          37,
          36,
          33,
          83,
          31,
          64,
          59,
          33,
          35,
          69,
          31,
          73,
          35,
          28,
          34,
          33,
          36,
          44,
          19,
          28,
          51,
          52,
          61,
          39,
          37,
          80,
          61,
          72,
          36,
          74,
          35,
          82,
          71,
          38,
          38,
          53,
          29,
          64,
          35,
          35,
          46,
          25,
          46,
          59,
          35,
          22,
          31,
          43,
          35,
          60,
          54,
          29,
          30,
          28,
          43,
          37,
          62,
          57,
          33,
          47,
          39,
          30,
          28,
          57,
          23,
          56,
          44,
          80,
          47,
          27,
          26,
          50,
          84,
          86,
          46,
          45,
          60,
          47,
          53,
          54,
          19,
          24,
          39,
          53,
          29,
          30,
          32,
          33,
          33,
          57,
          27,
          55,
          36,
          26,
          31,
          25,
          49,
          30,
          32,
          36,
          38,
          45,
          36,
          23,
          38,
          44,
          35,
          75,
          26,
          59,
          30,
          36,
          39,
          24,
          22,
          36,
          22,
          34,
          22,
          57,
          30,
          55,
          38,
          39,
          38,
          24,
          30,
          29,
          25,
          52,
          62,
          40,
          76,
          31,
          42,
          43,
          73,
          41,
          77,
          46,
          35,
          26,
          45,
          29,
          30,
          26,
          63,
          37,
          26,
          54,
          30,
          61,
          69,
          76,
          30,
          64,
          55,
          55,
          29,
          41,
          63,
          45,
          32,
          41,
          37,
          19,
          61,
          40,
          30,
          54,
          49,
          45,
          48,
          60,
          34,
          29,
          77,
          64,
          80,
          26,
          33,
          41,
          28,
          74,
          29,
          72,
          18,
          46,
          37,
          37,
          32,
          51,
          77,
          35,
          53,
          46,
          78,
          34,
          54,
          33,
          23,
          34,
          32,
          54,
          40,
          63,
          37,
          48,
          28,
          35,
          29,
          68,
          53,
          37,
          58,
          37,
          37,
          34,
          50,
          56,
          57,
          29,
          75,
          24,
          46,
          27,
          52,
          38,
          33,
          53,
          64,
          84,
          62,
          41,
          29,
          53,
          30,
          37,
          78,
          47,
          61,
          46,
          31,
          21,
          71,
          52,
          25,
          60,
          30,
          29,
          52,
          59,
          38,
          33,
          31,
          40,
          28,
          66,
          32,
          67,
          39,
          46,
          54,
          47,
          45,
          63,
          60,
          29,
          33,
          48,
          26,
          55,
          36,
          33,
          28,
          28,
          31,
          28,
          49,
          27,
          28,
          65,
          30,
          38,
          36,
          63,
          63,
          28,
          80,
          39,
          31,
          39,
          30,
          67,
          29,
          36,
          24,
          30,
          21,
          27,
          56,
          66,
          71,
          26,
          62,
          32,
          68,
          37,
          27,
          45,
          64,
          38,
          66,
          30,
          39,
          24,
          49,
          34,
          46,
          32,
          52,
          25,
          57,
          64,
          33,
          36,
          59,
          25,
          43,
          37,
          49,
          42,
          27,
          29,
          37,
          47,
          38,
          48,
          55,
          39,
          49,
          68,
          45,
          67,
          60,
          31,
          58,
          41,
          23,
          35,
          33,
          32,
          48,
          43,
          32,
          74,
          32,
          63,
          28,
          31,
          26,
          31,
          39,
          28,
          30,
          24,
          32,
          46,
          60,
          37,
          48,
          52,
          29,
          71,
          47,
          70,
          28,
          31,
          80,
          29,
          23,
          34,
          69,
          19,
          35,
          38,
          32,
          50,
          27,
          53,
          35,
          43,
          30,
          57,
          31,
          65,
          25,
          33,
          79,
          24,
          27,
          36,
          26,
          51,
          70,
          23,
          41,
          53,
          86,
          27,
          34,
          68,
          35,
          28,
          66,
          30,
          60,
          46,
          68,
          63,
          29,
          35,
          64,
          22,
          19,
          31,
          48,
          33,
          34,
          46,
          60,
          36,
          32,
          45,
          44,
          57,
          29,
          31,
          54,
          49,
          35,
          24,
          43,
          34,
          27,
          54,
          38,
          34,
          93,
          36,
          33,
          35,
          41,
          36,
          30,
          41,
          29,
          82,
          37,
          46,
          44,
          61,
          33,
          23,
          45,
          61,
          40,
          82,
          27,
          26,
          27,
          53,
          54,
          64,
          57,
          31,
          61,
          35,
          77,
          36,
          32,
          61,
          69,
          69,
          34,
          45,
          64,
          35,
          22,
          34,
          34,
          55,
          53,
          63,
          37,
          29,
          29,
          38,
          62,
          34,
          56,
          48,
          42,
          68,
          43,
          62,
          68,
          38,
          63,
          48,
          45,
          59,
          35,
          32,
          93,
          33,
          48,
          38,
          29,
          38,
          41,
          57,
          53,
          56,
          33,
          41,
          34,
          29,
          34,
          21,
          37,
          33,
          45,
          61,
          22,
          37,
          45,
          35,
          28,
          31,
          62,
          47,
          57,
          60,
          25,
          40,
          33,
          53,
          30,
          36,
          56,
          38,
          29,
          61,
          62,
          24,
          65,
          70,
          60,
          30,
          51,
          36,
          42,
          58,
          43,
          77,
          34,
          35,
          39,
          35,
          41,
          25,
          51,
          48,
          26,
          55,
          49,
          29,
          67,
          42,
          35,
          37,
          35,
          61,
          60,
          33,
          27,
          24,
          57,
          63,
          61,
          52,
          34,
          28,
          60,
          32,
          34,
          31,
          28,
          55,
          65,
          37,
          35,
          33,
          75,
          48,
          33,
          38,
          25,
          41,
          25,
          27,
          49,
          43,
          52,
          48,
          29,
          43,
          65,
          72,
          26,
          37,
          37,
          28,
          61,
          32,
          51,
          38,
          34,
          60,
          24,
          36,
          46,
          80,
          37,
          39,
          62,
          84,
          24,
          24,
          28,
          31,
          33,
          33,
          64,
          48,
          28,
          28,
          69,
          28,
          24,
          52,
          37,
          25,
          58,
          62,
          41,
          28,
          75,
          21,
          40,
          35,
          49,
          39,
          63,
          38,
          72,
          31,
          49,
          29,
          39,
          78,
          61,
          60,
          58,
          67,
          20,
          28,
          41,
          31,
          37,
          81,
          46,
          46,
          44,
          23,
          40,
          27,
          60,
          55,
          43,
          26,
          25,
          31,
          54,
          29,
          36,
          59,
          37,
          28,
          35,
          32,
          49,
          50,
          55,
          81,
          24,
          38,
          61,
          67,
          28,
          29,
          31,
          71,
          33,
          47,
          39,
          26,
          22,
          34,
          32,
          48,
          32,
          58,
          48,
          28,
          36,
          67,
          30,
          27,
          31,
          27,
          33,
          36,
          62,
          29,
          26,
          59,
          51,
          27,
          60,
          42,
          47,
          43,
          22,
          34,
          39,
          27,
          49,
          42,
          64,
          44,
          64,
          29,
          52,
          52,
          54,
          37,
          48,
          28,
          77,
          63,
          34,
          29,
          38,
          47,
          63,
          30,
          74,
          31,
          33,
          25,
          78,
          26,
          47,
          56,
          54,
          46,
          47,
          24,
          70,
          63,
          22,
          38,
          44,
          52,
          35,
          29,
          41,
          62,
          62,
          38,
          36,
          34,
          60,
          32,
          29,
          32,
          75,
          29,
          68,
          25,
          36,
          38,
          53,
          34,
          23,
          73,
          25,
          51,
          72,
          42,
          59,
          32,
          36,
          34,
          28,
          54,
          27,
          42,
          31,
          41,
          37,
          31,
          33,
          27,
          44,
          22,
          25,
          30,
          46,
          32,
          57,
          41,
          62,
          28,
          77,
          47,
          40,
          41,
          28,
          51,
          47,
          37,
          29,
          24,
          27,
          23,
          32,
          40,
          32,
          57,
          28,
          35,
          35,
          24,
          36,
          66,
          45,
          28,
          45,
          31,
          45,
          34,
          39,
          24,
          33,
          54,
          26,
          30,
          37,
          32,
          39,
          27,
          54,
          54,
          71,
          41,
          29,
          60,
          60,
          36,
          34,
          27,
          34,
          36,
          32,
          55,
          34,
          25,
          51,
          33,
          35,
          55,
          61,
          34,
          55,
          23,
          69,
          37,
          49,
          41,
          39,
          30,
          32,
          61,
          33,
          32,
          45,
          59,
          48,
          18,
          27,
          32,
          50,
          30,
          23,
          40,
          27,
          35,
          53,
          59,
          37,
          24,
          42,
          27,
          29,
          30,
          22,
          39,
          35,
          61,
          33,
          42,
          30,
          51,
          25,
          44,
          34,
          28,
          61,
          21,
          53,
          46,
          52,
          56,
          36,
          30,
          41,
          33,
          28,
          54,
          50,
          58,
          33,
          36,
          51,
          35,
          35,
          34,
          45,
          24,
          45,
          42,
          30,
          60,
          40,
          25,
          48,
          28,
          55,
          47,
          32,
          55,
          27,
          42,
          47,
          36,
          64,
          34,
          72,
          27,
          54,
          19,
          33,
          36,
          26,
          42,
          24,
          28,
          58,
          59,
          46,
          49,
          45,
          32,
          46,
          29,
          30,
          31,
          31,
          33,
          49,
          30,
          29,
          44,
          34,
          62,
          58,
          60,
          62,
          53,
          32,
          39,
          58,
          38,
          56,
          52,
          45,
          28,
          29,
          29,
          60,
          31,
          31,
          57,
          25,
          27,
          24,
          39,
          27,
          24,
          56,
          33,
          38,
          34,
          45,
          35,
          52,
          38,
          23,
          38,
          36,
          33,
          43,
          41,
          46,
          60,
          40,
          33,
          35,
          36,
          28,
          60,
          47,
          35,
          53,
          39,
          46,
          34,
          58,
          34,
          33,
          42,
          32,
          47,
          35,
          77,
          62,
          62,
          38,
          37,
          38,
          31,
          38,
          31,
          34,
          33,
          38,
          28,
          67,
          31,
          49,
          53,
          31,
          60,
          30,
          38,
          49,
          40,
          34,
          35,
          33,
          34,
          28,
          27,
          22,
          27,
          34,
          62,
          36,
          47,
          32,
          33,
          27,
          35,
          47,
          57,
          26,
          46,
          58,
          43,
          49,
          34,
          28,
          33,
          25,
          26,
          32,
          29,
          30,
          23,
          41,
          66,
          62,
          28,
          33,
          77,
          54,
          50,
          31,
          45,
          30,
          40,
          67,
          39,
          20,
          31,
          31,
          33,
          45,
          45,
          31,
          33,
          41,
          31,
          50,
          56,
          28,
          35,
          30,
          64,
          32,
          29,
          29,
          51,
          40,
          45,
          36,
          31,
          43,
          42,
          46,
          34,
          52,
          47,
          40,
          26,
          45,
          55,
          42,
          40,
          43,
          26,
          31,
          31,
          29,
          44,
          35,
          36,
          30,
          48,
          33,
          36,
          46,
          35,
          46,
          32,
          46,
          54,
          36,
          29,
          58,
          35,
          30,
          31,
          35,
          36,
          34,
          28,
          23,
          46,
          33,
          25,
          43,
          38,
          62,
          33,
          53,
          38,
          32,
          24,
          26,
          55,
          47,
          58,
          30,
          39,
          30,
          34,
          34,
          56,
          27,
          46,
          32,
          27,
          36,
          46,
          46,
          49,
          39,
          33,
          48,
          38,
          60,
          33,
          64,
          22,
          34,
          36,
          72,
          37,
          78,
          31,
          20,
          60,
          36,
          51,
          26,
          48,
          49,
          20,
          77,
          42,
          31,
          56,
          32,
          33,
          33,
          54,
          28,
          41,
          51,
          43,
          36,
          49,
          47,
          30,
          31,
          39,
          40,
          48,
          30,
          41,
          33,
          25,
          49,
          32,
          64,
          28,
          37,
          25,
          53,
          62,
          34,
          26,
          39,
          45,
          19,
          33,
          43,
          35,
          52,
          28,
          35,
          50,
          37,
          36,
          61,
          24,
          35,
          32,
          45,
          43,
          39,
          35,
          23,
          34,
          47,
          21,
          37,
          53,
          46,
          26,
          51,
          58,
          51,
          33,
          46,
          34,
          40,
          40,
          46,
          59,
          34,
          49,
          28,
          56,
          28,
          33,
          37,
          38,
          25,
          74,
          42,
          29,
          60,
          41,
          51,
          49,
          41,
          31,
          25,
          32,
          73,
          31,
          32,
          27,
          60,
          34,
          43,
          31,
          42,
          27,
          57,
          30,
          45,
          49,
          31,
          41,
          57,
          57,
          58,
          54,
          36,
          53,
          34,
          27,
          41,
          62,
          27,
          37,
          22,
          27,
          30,
          28,
          27,
          42,
          36,
          27,
          58,
          55,
          52,
          42,
          46,
          32,
          52,
          57,
          40,
          30,
          30,
          39,
          28,
          34,
          30,
          24,
          59,
          38,
          47,
          52,
          35,
          61,
          46,
          55,
          42,
          26,
          58,
          46,
          39,
          29,
          21,
          57,
          31,
          49,
          56,
          30,
          34,
          35,
          55,
          44,
          36,
          63,
          37,
          58,
          37,
          26,
          44,
          37,
          53,
          37,
          40,
          40,
          31,
          36,
          36,
          52,
          39,
          47,
          49,
          32,
          36,
          27,
          71,
          32,
          29,
          64,
          24,
          21,
          42,
          47,
          53,
          33,
          62,
          33,
          58,
          52,
          37,
          37,
          34,
          79,
          54,
          28,
          33,
          21,
          46,
          38,
          32,
          34,
          49,
          31,
          32,
          38,
          27,
          78,
          60,
          36,
          75,
          30,
          30,
          50,
          37,
          60,
          26,
          49,
          40,
          46,
          38,
          74,
          35,
          50,
          54,
          35,
          36,
          30,
          36,
          41,
          30,
          39,
          34,
          32,
          35,
          50,
          28,
          44,
          32,
          30,
          59,
          35,
          30,
          42,
          63,
          35,
          43,
          27,
          29,
          54,
          80,
          57,
          33,
          65,
          59,
          30,
          28,
          29,
          31,
          31,
          49,
          43,
          40,
          39,
          71,
          38,
          30,
          45,
          67,
          58,
          45,
          34,
          27,
          51,
          38,
          28,
          25,
          37,
          38,
          37,
          34,
          37,
          76,
          48,
          49,
          62,
          33,
          19,
          25,
          31,
          38,
          31,
          24,
          24,
          26,
          73,
          29,
          49,
          42,
          68,
          60,
          53,
          62,
          33,
          36,
          46,
          37,
          53,
          34,
          24,
          36,
          34,
          48,
          43,
          32,
          46,
          32,
          45,
          29,
          25,
          20,
          45,
          42,
          29,
          23,
          37,
          35,
          61,
          31,
          41,
          34,
          32,
          32,
          34,
          35,
          44,
          32,
          34,
          50,
          32,
          23,
          32,
          60,
          44,
          30,
          37,
          35,
          36,
          39,
          25,
          33,
          28,
          31,
          56,
          41,
          68,
          61,
          37,
          28,
          46,
          53,
          32,
          33,
          57,
          41,
          24,
          45,
          49,
          30,
          26,
          57,
          39,
          40,
          52,
          35,
          46,
          49,
          22,
          36,
          38,
          40,
          24,
          78,
          25,
          63,
          31,
          41,
          50,
          27,
          27,
          50,
          44,
          26,
          46,
          36,
          35,
          68,
          50,
          36,
          34,
          30,
          22,
          59,
          60,
          32,
          46,
          23,
          22,
          59,
          26,
          35,
          53,
          39,
          32,
          41,
          37,
          35,
          42,
          27,
          57,
          41,
          37,
          44,
          32,
          34,
          42,
          54,
          48,
          73,
          33,
          42,
          57,
          29,
          47,
          51,
          34,
          25,
          50,
          47,
          40,
          79,
          30,
          54,
          53,
          33,
          60,
          35,
          46,
          52,
          50,
          47,
          73,
          35,
          26,
          46,
          42,
          29,
          30,
          37,
          28,
          32,
          53,
          41,
          40,
          53,
          64,
          32,
          48,
          35,
          43,
          25,
          43,
          29,
          61,
          64,
          27,
          38,
          36,
          61,
          49,
          33,
          25,
          33,
          51,
          32,
          32,
          72,
          34,
          38,
          35,
          38,
          43,
          31,
          30,
          22,
          46,
          52,
          35,
          71,
          30,
          25,
          34,
          51,
          25,
          48,
          42,
          55,
          38,
          26,
          27,
          75,
          32,
          39,
          46,
          40,
          36,
          54,
          42,
          42,
          37,
          25,
          29,
          32,
          64,
          55,
          56,
          28,
          52,
          36,
          25,
          66,
          31,
          43,
          59,
          35,
          48,
          38,
          27,
          25,
          56,
          54,
          36,
          31,
          62,
          31,
          66,
          32,
          40,
          43,
          51,
          34,
          47,
          36,
          27,
          52,
          44,
          26,
          32,
          34,
          35,
          32,
          43,
          50,
          40,
          29,
          32,
          50,
          65,
          59,
          44,
          33,
          33,
          57,
          38,
          49,
          45,
          25,
          22,
          32,
          36,
          32,
          45,
          35,
          23,
          50,
          31,
          61,
          42,
          39,
          37,
          30,
          55,
          30,
          57,
          31,
          37,
          34,
          31,
          52,
          48,
          73,
          54,
          38,
          39,
          44,
          29,
          51,
          31,
          33,
          39,
          30,
          39,
          25,
          57,
          35,
          27,
          27,
          28,
          41,
          46,
          38,
          35,
          24,
          24,
          28,
          28,
          53,
          45,
          33,
          51,
          32,
          65,
          32,
          67,
          53,
          41,
          30,
          37,
          27,
          77,
          32,
          25,
          32,
          43,
          30,
          59,
          61,
          32,
          37,
          51,
          55,
          38,
          47,
          59,
          51,
          25,
          67,
          58,
          52,
          54,
          27,
          41,
          72,
          24,
          36,
          39,
          55,
          35,
          32,
          29,
          59,
          33,
          30,
          30,
          37,
          39,
          40,
          29,
          24,
          31,
          62,
          33,
          67,
          44,
          79,
          30,
          38,
          31,
          38,
          36,
          30,
          34,
          27,
          32,
          37,
          56,
          28,
          49,
          23,
          56,
          54,
          59,
          53,
          51,
          31,
          62,
          38,
          25,
          43,
          45,
          78,
          30,
          69,
          48,
          31,
          57,
          45,
          48,
          34,
          37,
          32,
          35,
          31,
          54,
          52,
          29,
          34,
          37,
          66,
          33,
          60,
          55,
          34,
          52,
          29,
          43,
          57,
          32,
          67,
          45,
          47,
          31,
          52,
          41,
          35,
          48,
          53,
          59,
          26,
          30,
          56,
          41,
          52,
          35,
          41,
          39,
          51,
          46,
          43,
          31,
          56,
          35,
          58,
          33,
          36,
          49,
          47,
          61,
          45,
          53,
          31,
          35,
          55,
          50,
          53,
          45,
          45,
          32,
          33,
          51,
          27,
          37,
          45,
          40,
          41,
          35,
          62,
          42,
          38,
          36,
          59,
          46,
          42,
          27,
          44,
          32,
          32,
          43,
          38,
          40,
          44,
          49,
          37,
          40,
          54,
          38,
          45,
          49,
          46,
          39,
          56,
          37,
          39,
          53,
          38,
          25,
          35,
          47,
          40,
          31,
          50,
          40,
          55,
          31,
          59,
          52,
          55,
          49,
          31,
          55,
          38,
          52,
          57,
          65,
          53,
          34,
          41,
          44,
          49,
          39,
          54,
          22,
          31,
          53,
          59,
          59,
          56,
          52,
          57,
          41,
          39,
          35,
          53,
          35,
          25,
          34,
          38,
          42,
          28,
          35,
          34,
          32,
          38,
          32,
          41,
          41,
          26,
          60,
          51,
          24,
          37,
          44,
          74,
          50,
          31,
          31,
          25,
          39,
          30,
          60,
          48,
          30,
          43,
          44,
          29,
          46,
          28,
          34,
          42,
          59,
          27,
          36,
          34,
          30,
          50,
          50,
          45,
          40,
          49,
          54,
          31,
          35,
          34,
          48,
          51,
          47,
          45,
          33,
          33,
          27,
          47,
          54,
          37,
          43,
          49,
          43,
          27,
          44,
          37,
          50,
          50,
          52,
          26,
          35,
          47,
          55,
          26,
          37,
          56,
          33,
          36,
          32,
          30,
          39,
          28,
          36,
          39,
          48,
          34,
          50,
          26,
          35,
          44,
          47,
          62,
          55,
          26,
          88,
          28,
          41,
          25,
          40,
          35,
          65,
          40,
          43,
          29,
          48,
          35,
          30,
          32,
          30,
          30,
          24,
          29,
          44,
          42,
          45,
          57,
          36,
          49,
          30,
          48,
          32,
          35,
          40,
          32,
          37,
          37,
          43,
          48,
          36,
          33,
          33,
          36,
          65,
          50,
          34,
          35,
          32,
          34,
          34,
          33,
          45,
          35,
          53,
          39,
          38,
          35,
          52,
          30,
          37,
          33,
          52,
          29,
          28,
          37,
          33,
          32,
          25,
          37,
          34,
          40,
          40,
          38,
          39,
          37,
          35,
          33,
          38,
          37,
          29,
          42,
          45,
          42,
          33,
          35,
          30,
          31,
          45,
          44,
          31,
          30,
          54,
          35,
          36,
          55,
          45,
          30,
          57,
          59,
          47,
          34,
          46,
          56,
          24,
          40,
          50,
          31,
          37,
          45,
          45,
          36,
          57,
          39,
          47,
          44,
          39,
          40,
          48,
          56,
          50,
          29,
          33,
          57,
          54,
          64,
          61,
          36,
          50,
          30,
          40,
          40,
          30,
          55,
          59,
          37,
          53,
          56,
          33,
          49,
          47,
          34,
          33,
          31,
          44,
          50,
          39,
          41,
          33,
          40,
          42,
          37,
          56,
          49,
          35,
          41,
          52,
          33,
          40,
          44,
          39,
          39,
          28,
          45,
          63,
          32,
          28,
          36,
          34,
          45,
          30,
          59,
          54,
          47,
          41,
          35,
          38,
          36,
          39,
          48,
          42,
          24,
          57,
          35,
          32,
          26,
          27,
          47,
          32,
          48,
          43,
          30,
          28,
          32,
          26,
          38,
          30,
          33,
          37,
          47,
          36,
          48,
          47,
          34,
          28,
          54,
          33,
          41,
          41,
          42,
          32,
          47,
          36,
          34,
          36,
          50,
          53,
          49,
          39,
          47,
          57,
          30,
          54,
          35,
          47,
          38,
          40,
          51,
          51,
          57,
          41,
          31,
          59,
          26,
          41,
          39,
          25,
          33,
          35,
          49,
          31,
          40,
          32,
          42,
          33,
          48,
          28,
          43,
          30,
          41,
          55,
          55,
          54,
          55,
          57,
          27,
          28,
          51,
          27,
          40,
          56,
          35,
          47,
          49,
          35,
          50,
          55,
          56,
          35,
          42,
          34,
          40,
          33,
          33,
          59,
          51,
          47,
          48,
          31,
          39,
          52,
          28,
          56,
          47,
          77,
          39,
          31,
          39,
          32,
          59,
          42,
          50,
          44,
          47,
          51,
          34,
          36,
          32,
          28,
          34,
          40,
          49,
          30,
          56,
          28,
          31,
          28,
          53,
          38,
          54,
          36,
          60,
          40,
          50,
          28,
          33,
          53,
          38,
          34,
          41,
          64,
          45,
          38,
          57,
          34,
          36,
          30,
          32,
          42,
          35,
          42,
          50,
          38,
          38,
          55,
          60,
          33,
          30,
          30,
          51,
          37,
          37,
          45,
          37,
          48,
          42,
          49,
          36,
          54,
          52,
          30,
          37,
          31,
          26,
          51,
          34,
          26,
          32,
          38,
          52,
          44,
          31,
          38,
          32,
          41,
          48,
          29,
          41,
          49,
          40,
          27,
          45,
          48,
          31,
          45,
          43,
          31,
          37,
          38,
          29,
          28,
          55,
          27,
          43,
          29,
          52,
          33,
          27,
          27,
          31,
          31,
          45,
          34,
          48,
          47,
          49,
          55,
          51,
          37,
          33,
          33,
          29,
          57,
          60,
          51,
          45,
          45,
          39,
          25,
          47,
          34,
          40,
          48,
          39,
          38,
          32,
          27,
          47,
          40,
          41,
          40,
          39,
          45,
          31,
          27,
          56,
          39,
          52,
          58,
          48,
          33,
          38,
          42,
          36,
          35,
          45,
          51,
          43,
          45,
          30,
          40,
          56,
          48,
          32,
          47,
          30,
          37,
          52,
          49,
          53,
          53,
          34,
          40,
          39,
          33,
          41,
          31,
          45,
          23,
          51,
          34,
          36,
          42,
          33,
          46,
          37,
          33,
          30,
          53,
          46,
          35,
          36,
          46,
          31,
          52,
          48,
          45,
          36,
          53,
          35,
          44,
          38,
          44,
          51,
          32,
          59,
          31,
          52,
          26,
          40,
          35,
          54,
          36,
          45,
          29,
          49,
          52,
          52,
          47,
          33,
          30,
          52,
          54,
          59,
          35,
          29,
          33,
          32,
          50,
          54,
          49,
          39,
          34,
          31,
          46,
          59,
          32,
          39,
          54,
          33,
          59,
          28,
          48,
          39,
          41,
          33,
          37,
          30,
          36,
          53,
          26,
          59,
          44,
          55,
          26,
          41,
          28,
          40,
          47,
          39,
          38,
          37,
          36,
          53,
          47,
          28,
          32,
          25,
          39,
          60,
          39,
          31,
          45,
          36,
          44,
          33,
          31,
          38,
          50,
          40,
          33,
          42,
          42,
          33,
          40,
          52,
          49,
          30,
          26,
          40,
          30,
          55,
          48,
          28,
          54,
          36,
          34,
          39,
          66,
          42,
          55,
          85,
          63,
          37,
          24,
          34,
          28,
          58,
          49,
          31,
          56,
          43,
          32,
          55,
          55,
          31,
          49,
          41,
          56,
          50,
          37,
          35,
          52,
          33,
          42,
          33,
          35,
          28,
          41,
          44,
          26,
          40,
          37,
          33,
          36,
          28,
          37,
          32,
          49,
          36,
          39,
          34,
          32,
          26,
          46,
          78,
          47,
          80,
          25,
          50,
          36,
          42,
          54,
          43,
          29,
          36,
          59,
          47,
          36,
          56,
          55,
          38,
          39,
          35,
          54,
          32,
          30,
          51,
          39,
          46,
          33,
          34,
          33,
          55,
          49,
          37,
          29,
          28,
          21,
          44,
          30,
          46,
          33,
          40,
          42,
          44,
          32,
          34,
          55,
          35,
          47,
          29,
          47,
          39,
          40,
          42,
          31,
          31,
          82,
          44,
          32,
          40,
          26,
          30,
          45,
          32,
          56,
          58,
          28,
          43,
          34,
          40,
          31,
          28,
          45,
          33,
          42,
          32,
          41,
          32,
          37,
          57,
          56,
          37,
          34,
          46,
          40,
          32,
          28,
          46,
          44,
          55,
          75,
          41,
          30,
          33,
          43,
          49,
          32,
          26,
          55,
          46,
          31,
          34,
          37,
          29,
          24,
          46,
          41,
          52,
          27,
          44,
          39,
          52,
          77,
          35,
          49,
          37,
          46,
          43,
          35,
          29,
          52,
          55,
          33,
          40,
          39,
          61,
          63,
          44,
          52,
          58,
          38,
          51,
          29,
          42,
          56,
          55,
          43,
          27,
          54,
          25,
          33,
          46,
          26,
          30,
          39,
          58,
          49,
          49,
          43,
          60,
          57,
          38,
          56,
          45,
          45,
          44,
          40,
          38,
          55,
          40,
          35,
          32,
          31,
          49,
          55,
          51,
          30,
          37,
          36,
          51,
          29,
          32,
          36,
          32,
          56,
          46,
          39,
          57,
          53,
          33,
          43,
          40,
          44,
          59,
          59,
          28,
          26,
          59,
          78,
          33,
          38,
          35,
          30,
          31,
          35,
          45,
          40,
          50,
          29,
          41,
          44,
          29,
          49,
          30,
          37,
          52,
          30,
          40,
          33,
          34,
          33,
          35,
          54,
          32,
          32,
          44,
          53,
          53,
          40,
          29,
          36,
          40,
          58,
          34,
          60,
          23,
          59,
          42,
          50,
          31,
          31,
          28,
          56,
          23,
          30,
          54,
          35,
          29,
          48,
          43,
          40,
          38,
          30,
          60,
          31,
          28,
          31,
          48,
          36,
          41,
          35,
          51,
          35,
          51,
          56,
          59,
          27,
          48,
          24,
          56,
          34,
          35,
          57,
          30,
          40,
          48,
          35,
          46,
          30,
          37,
          30,
          46,
          43,
          40,
          34,
          43,
          31,
          37,
          49,
          38,
          27,
          35,
          47,
          56,
          26,
          29,
          53,
          38,
          33,
          30,
          36,
          37,
          60,
          48,
          32,
          31,
          54,
          31,
          31,
          29,
          23,
          51,
          32,
          48,
          32,
          44,
          35,
          46,
          31,
          31,
          36,
          35,
          35,
          36,
          52,
          27,
          47,
          43,
          41,
          18,
          45,
          41,
          34,
          46,
          38,
          23,
          73,
          41,
          43,
          51,
          45,
          47,
          35,
          38,
          31,
          49,
          29,
          39,
          41,
          34,
          39,
          78,
          33,
          59,
          52,
          35,
          43,
          43,
          36,
          36,
          45,
          47,
          38,
          45,
          33,
          38,
          36,
          38,
          34,
          32,
          49,
          37,
          47,
          28,
          29,
          30,
          34,
          39,
          26,
          38,
          47,
          61,
          35,
          34,
          34,
          56,
          47,
          36,
          43,
          51,
          40,
          38,
          34,
          47,
          57,
          35,
          45,
          55,
          54,
          39,
          58,
          31,
          35,
          45,
          44,
          57,
          41,
          42,
          49,
          52,
          57,
          29,
          34,
          36,
          39,
          53,
          40,
          38,
          34,
          40,
          39,
          51,
          33,
          59,
          30,
          61,
          24,
          36,
          34,
          47,
          40,
          46,
          32,
          39,
          35,
          37,
          45,
          38,
          52,
          28,
          35,
          45,
          33,
          42,
          23,
          49,
          52,
          27,
          34,
          31,
          45,
          32,
          51,
          56,
          44,
          41,
          54,
          31,
          25,
          50,
          41,
          33,
          49,
          34,
          60,
          45,
          40,
          50,
          27,
          53,
          31,
          24,
          68,
          27,
          46,
          23,
          76,
          48,
          60,
          46,
          36,
          30,
          33,
          36,
          44,
          36,
          44,
          70,
          48,
          52,
          34,
          35,
          54,
          47,
          38,
          41,
          28,
          58,
          59,
          32,
          55,
          38,
          40,
          45,
          42,
          53,
          32,
          55,
          54,
          46,
          39,
          36,
          44,
          28,
          36,
          43,
          25,
          31,
          60,
          29,
          46,
          31,
          31,
          45,
          60,
          50,
          34,
          33,
          57,
          38,
          42,
          54,
          46,
          59,
          38,
          49,
          49,
          30,
          52,
          38,
          44,
          39,
          38,
          36,
          32,
          57,
          45,
          43,
          42,
          58,
          53,
          30,
          40,
          35,
          34,
          35,
          28,
          39,
          51,
          29,
          44,
          32,
          28,
          53,
          81,
          33,
          37,
          37,
          35,
          30,
          43,
          45,
          34,
          40,
          60,
          54,
          39,
          44,
          32,
          34,
          44,
          30,
          47,
          74,
          27,
          34,
          52,
          32,
          42,
          31,
          31,
          31,
          57,
          40,
          23,
          43,
          45,
          58,
          38,
          48,
          47,
          49,
          28,
          31,
          38,
          43,
          52,
          43,
          29,
          45,
          59,
          48,
          30,
          23,
          38,
          52,
          44,
          34,
          32,
          41,
          31,
          40,
          52,
          32,
          26,
          29,
          31,
          48,
          27,
          30,
          28,
          35,
          47,
          35,
          31,
          53,
          26,
          35,
          29,
          39,
          31,
          48,
          36,
          27,
          49,
          34,
          28,
          49,
          35,
          53,
          58,
          37,
          27,
          30,
          35,
          32,
          53,
          34,
          38,
          34,
          40,
          48,
          30,
          31,
          37,
          32,
          40,
          41,
          52,
          29,
          49,
          43,
          42,
          37,
          57,
          25,
          30,
          31,
          49,
          47,
          57,
          26,
          40,
          34,
          41,
          53,
          26,
          28,
          54,
          42,
          29,
          49,
          32,
          29,
          60,
          51,
          40,
          44,
          47,
          32,
          43,
          59,
          24,
          26,
          37,
          27,
          32,
          44,
          34,
          39,
          30,
          44,
          22,
          28,
          41,
          30,
          36,
          39,
          38,
          31,
          29,
          42,
          27,
          55,
          28,
          47,
          49,
          38,
          53,
          31,
          57,
          36,
          38,
          30,
          36,
          29,
          32,
          48,
          53,
          35,
          48,
          31,
          30,
          28,
          42,
          31,
          36,
          43,
          28,
          34,
          36,
          28,
          34,
          53,
          52,
          44,
          58,
          47,
          40,
          39,
          33,
          23,
          31,
          39,
          42,
          46,
          22,
          50,
          50,
          34,
          40,
          39,
          51,
          45,
          34,
          30,
          57,
          42,
          51,
          38,
          60,
          38,
          37,
          34,
          39,
          55,
          48,
          42,
          29,
          33,
          35,
          28,
          26,
          40,
          42,
          49,
          32,
          33,
          30,
          20,
          51,
          38,
          53,
          34,
          30,
          36,
          36,
          39,
          33,
          42,
          30,
          23,
          37,
          77,
          33,
          33,
          40,
          58,
          29,
          43,
          34,
          38,
          38,
          40,
          59,
          44,
          60,
          37,
          50,
          50,
          35,
          39,
          29,
          34,
          31,
          51,
          46,
          27,
          61,
          34,
          30,
          58,
          30,
          50,
          30,
          43,
          35,
          36,
          46,
          46,
          29,
          48,
          50,
          53,
          38,
          27,
          36,
          48,
          59,
          21,
          56,
          57,
          41,
          36,
          45,
          40,
          30,
          75,
          35,
          58,
          29,
          54,
          28,
          33,
          56,
          50,
          37,
          49,
          54,
          40,
          33,
          48,
          43,
          35,
          55,
          33,
          41,
          57,
          50,
          63,
          33,
          47,
          36,
          23,
          33,
          36,
          47,
          39,
          44,
          60,
          42,
          32,
          33,
          37,
          33,
          59,
          42,
          59,
          31,
          50,
          29,
          36,
          33,
          38,
          33,
          36,
          36,
          44,
          29,
          44,
          55,
          38,
          39,
          45,
          36,
          36,
          59,
          41,
          29,
          50,
          51,
          29,
          40,
          36,
          49,
          45,
          40,
          31,
          32,
          44,
          27,
          25,
          56,
          59,
          31,
          49,
          50,
          42,
          37,
          36,
          53,
          51,
          42,
          47,
          40,
          25,
          36,
          80,
          60,
          51,
          59,
          80,
          47,
          36,
          31,
          33,
          45,
          33,
          51,
          51,
          48,
          34,
          28,
          33,
          36,
          40,
          34,
          31,
          49,
          35,
          38,
          32,
          33,
          39,
          30,
          53,
          24,
          52,
          27,
          36,
          29,
          47,
          43,
          48,
          57,
          25,
          43,
          50,
          54,
          38,
          41,
          35,
          33,
          51,
          29,
          34,
          51,
          58,
          50,
          38,
          57,
          49,
          42,
          33,
          34,
          36,
          60,
          37,
          41,
          36,
          56,
          34,
          33,
          29,
          43,
          29,
          36,
          33,
          39,
          26,
          41,
          36,
          31,
          34,
          28,
          47,
          41,
          39,
          53,
          45,
          30,
          39,
          30,
          32,
          66,
          26,
          30,
          42,
          44,
          38,
          31,
          34,
          42,
          34,
          31,
          30,
          31,
          47,
          52,
          31,
          51,
          48,
          47,
          55,
          58,
          74,
          30,
          40,
          50,
          36,
          26,
          30,
          34,
          53,
          42,
          33,
          35,
          33,
          41,
          40,
          32,
          54,
          31,
          35,
          47,
          48,
          50,
          33,
          40,
          60,
          68,
          54,
          30,
          42,
          57,
          28,
          55,
          42,
          49,
          41,
          31,
          33,
          55,
          34,
          38,
          46,
          49,
          44,
          57,
          44,
          42,
          37,
          42,
          33,
          31,
          34,
          26,
          42,
          41,
          35,
          26,
          42,
          36,
          60,
          51,
          32,
          46,
          31,
          42,
          45,
          31,
          41,
          28,
          41,
          36,
          41,
          40,
          40,
          58,
          32,
          30,
          43,
          33,
          30,
          37,
          40,
          39,
          39,
          26,
          35,
          59,
          31,
          36,
          26,
          36,
          59,
          42,
          35,
          59,
          44,
          34,
          36,
          43,
          45,
          29,
          29,
          40,
          31,
          29,
          37,
          35,
          35,
          39,
          28,
          31,
          34,
          31,
          51,
          53,
          34,
          42,
          33,
          29,
          25,
          42,
          46,
          52,
          60,
          25,
          55,
          37,
          49,
          42,
          32,
          43,
          26,
          41,
          30,
          42,
          34,
          20,
          45,
          58,
          51,
          55,
          31,
          42,
          48,
          62,
          40,
          37,
          39,
          34,
          52,
          35,
          56,
          37,
          35,
          31,
          42,
          27,
          33,
          31,
          53,
          56,
          80,
          63,
          37,
          43,
          62,
          25,
          51,
          39,
          47,
          39,
          32,
          30,
          34,
          41,
          44,
          43,
          30,
          46,
          36,
          56,
          44,
          37,
          36,
          24,
          49,
          69,
          47,
          29,
          31,
          42,
          30,
          26,
          37,
          28,
          36,
          46,
          58,
          46,
          47,
          50,
          47,
          26,
          42,
          34,
          46,
          51,
          52,
          38,
          44,
          52,
          32,
          35,
          36,
          37,
          26,
          32,
          46,
          50,
          43,
          55,
          35,
          45,
          48,
          31,
          46,
          50,
          53,
          36,
          50,
          40,
          39,
          38,
          32,
          31,
          41,
          41,
          58,
          36,
          54,
          32,
          54,
          46,
          31,
          34,
          46,
          45,
          30,
          38,
          31,
          30,
          43,
          26,
          32,
          33,
          40,
          47,
          34,
          39,
          49,
          41,
          45,
          45,
          54,
          34,
          44,
          56,
          60,
          56,
          60,
          57,
          29,
          39,
          33,
          61,
          58,
          27,
          37,
          34,
          35,
          35,
          49,
          34,
          34,
          48,
          55,
          47,
          52,
          30,
          54,
          40,
          43,
          53,
          25,
          51,
          45,
          42,
          35,
          37,
          43,
          42,
          58,
          43,
          29,
          39,
          59,
          36,
          44,
          34,
          49,
          26,
          51,
          37,
          33,
          36,
          34,
          45,
          38,
          40,
          54,
          34,
          51,
          46,
          52,
          38,
          26,
          39,
          54,
          44,
          52,
          43,
          45,
          38,
          59,
          40,
          34,
          41,
          36,
          40,
          39,
          34,
          31,
          32,
          53,
          28,
          38,
          50,
          36,
          23,
          51,
          51,
          35,
          32,
          60,
          34,
          58,
          58,
          34,
          38,
          34,
          31,
          46,
          36,
          35,
          42,
          28,
          39,
          39,
          39,
          43,
          69,
          43,
          36,
          38,
          38,
          37,
          37,
          31,
          31,
          32,
          42,
          46,
          35,
          31,
          60,
          55,
          45,
          33,
          48,
          36,
          37,
          37,
          54,
          40,
          44,
          46,
          37,
          57,
          43,
          33,
          38,
          31,
          44,
          28,
          31,
          42,
          41,
          20,
          29,
          33,
          36,
          42,
          57,
          35,
          33,
          37,
          55,
          28,
          38,
          38,
          60,
          55,
          48,
          30,
          31,
          25,
          37,
          26,
          45,
          76,
          32,
          52,
          35,
          54,
          33,
          44,
          34,
          38,
          33,
          32,
          36,
          77,
          26,
          33,
          45,
          55,
          55,
          35,
          28,
          55,
          37,
          35,
          33,
          35,
          30,
          35,
          49,
          26,
          26,
          31,
          38,
          54,
          50,
          37,
          29,
          35,
          36,
          57,
          72,
          27,
          48,
          30,
          27,
          57,
          38,
          47,
          57,
          43,
          43,
          32,
          37,
          49,
          32,
          40,
          43,
          46,
          49,
          37,
          40,
          56,
          30,
          40,
          29,
          37,
          38,
          28,
          40,
          58,
          41,
          55,
          40,
          40,
          27,
          52,
          31,
          30,
          59,
          47,
          57,
          43,
          38,
          27,
          46,
          42,
          51,
          42,
          42,
          38,
          43,
          34,
          28,
          43,
          55,
          32,
          49,
          48,
          30,
          43,
          42,
          30,
          31,
          33,
          46,
          38,
          53,
          45,
          56,
          33,
          28,
          39,
          38,
          32,
          52,
          29,
          29,
          33,
          46,
          36,
          35,
          39,
          56,
          42,
          44,
          50,
          46,
          50,
          48,
          39,
          26,
          32,
          36,
          56,
          30,
          39,
          33,
          43,
          32,
          27,
          46,
          42,
          43,
          59,
          44,
          31,
          26,
          51,
          24,
          26,
          54,
          31,
          33,
          42,
          33,
          40,
          46,
          33,
          56,
          38,
          35,
          32,
          51,
          33,
          31,
          31,
          49,
          43,
          40,
          27,
          33,
          47,
          59,
          52,
          35,
          32,
          49,
          54,
          30,
          33,
          50,
          40,
          38,
          34,
          33,
          41,
          30,
          45,
          28,
          35,
          31,
          41,
          32,
          34,
          26,
          29,
          49,
          30,
          44,
          42,
          35,
          39,
          34,
          58,
          35,
          32,
          31,
          41,
          39,
          39,
          38,
          46,
          54,
          34,
          40,
          40,
          30,
          37,
          34,
          66,
          30,
          59,
          56,
          25,
          36,
          33,
          30,
          29,
          52,
          54,
          28,
          58,
          23,
          49,
          47,
          73,
          44,
          29,
          48,
          35,
          41,
          40,
          57,
          43,
          31,
          51,
          52,
          31,
          26,
          36,
          43,
          32,
          26,
          25,
          34,
          30,
          35,
          42,
          53,
          52,
          35,
          29,
          43,
          81,
          35,
          36,
          76,
          44,
          38,
          37,
          41,
          49,
          53,
          34,
          30,
          43,
          36,
          60,
          45,
          49,
          39,
          57,
          49,
          39,
          37,
          45,
          36,
          58,
          48,
          52,
          32,
          51,
          31,
          30,
          34,
          53,
          32,
          52,
          50,
          30,
          34,
          51,
          34,
          26,
          51,
          31,
          29,
          29,
          46,
          35,
          44,
          34,
          27,
          28,
          59,
          38,
          34,
          44,
          35,
          45,
          34,
          57,
          76,
          30,
          48,
          43,
          34,
          30,
          33,
          38,
          52,
          34,
          40,
          36,
          56,
          29,
          34,
          31,
          36,
          30,
          36,
          25,
          32,
          31,
          49,
          37,
          26,
          41,
          47,
          57,
          50,
          36,
          42,
          30,
          43,
          49,
          35,
          32,
          36,
          42,
          34,
          33,
          55,
          46,
          34,
          50,
          30,
          53,
          48,
          55,
          35,
          48,
          45,
          31,
          55,
          41,
          33,
          40,
          51,
          31,
          26,
          39,
          30,
          28,
          48,
          56,
          45,
          38,
          28,
          51,
          32,
          31,
          34,
          46,
          36,
          39,
          42,
          32,
          35,
          50,
          55,
          51,
          43,
          46,
          31,
          38,
          40,
          53,
          41,
          46,
          48,
          31,
          29,
          37,
          37,
          56,
          60,
          46,
          31,
          51,
          38,
          39,
          30,
          35,
          31,
          27,
          42,
          67,
          32,
          49,
          56,
          34,
          50,
          27,
          56,
          44,
          45,
          45,
          37,
          26,
          32,
          25,
          48,
          29,
          46,
          34,
          46,
          31,
          39,
          29,
          38,
          40,
          39,
          49,
          34,
          30,
          57,
          34,
          21,
          35,
          39,
          42,
          25,
          27,
          33,
          51,
          32,
          53,
          29,
          33,
          33,
          32,
          31,
          45,
          57,
          47,
          52,
          56,
          51,
          40,
          30,
          39,
          42,
          25,
          60,
          37,
          48,
          32,
          33,
          51,
          43,
          31,
          36,
          58,
          33,
          31,
          28,
          29,
          34,
          31,
          31,
          37,
          32,
          42,
          34,
          74,
          60,
          32,
          47,
          45,
          50,
          39,
          60,
          47,
          40,
          26,
          43,
          30,
          34,
          35,
          21,
          52,
          31,
          42,
          31,
          58,
          51,
          38,
          32,
          31,
          36,
          33,
          47,
          52,
          42,
          39,
          31,
          52,
          40,
          29,
          49,
          57,
          35,
          26,
          35,
          31,
          57,
          32,
          87,
          44,
          41,
          36,
          56,
          35,
          46,
          32,
          41,
          34,
          48,
          31,
          41,
          36,
          31,
          37,
          48,
          31,
          42,
          50,
          47,
          43,
          59,
          35,
          24,
          49,
          47,
          57,
          29,
          50,
          32,
          23,
          41,
          48,
          29,
          42,
          38,
          31,
          38,
          50,
          36,
          52,
          50,
          38,
          30,
          46,
          54,
          46,
          49,
          36,
          58,
          29,
          41,
          32,
          33,
          31,
          44,
          51,
          38,
          38,
          27,
          41,
          32,
          45,
          54,
          36,
          33,
          40,
          37,
          48,
          36,
          46,
          29,
          41,
          44,
          40,
          33,
          60,
          48,
          37,
          39,
          28,
          44,
          37,
          58,
          50,
          38,
          31,
          33,
          35,
          53,
          58,
          26,
          34,
          50,
          42,
          46,
          45,
          46,
          31,
          44,
          36,
          50,
          33,
          33,
          55,
          40,
          30,
          36,
          34,
          46,
          55,
          56,
          34,
          42,
          48,
          51,
          45,
          35,
          63,
          35,
          25,
          26,
          29,
          26,
          26,
          23,
          51,
          51,
          43,
          81,
          42,
          41,
          35,
          32,
          51,
          33,
          38,
          48,
          50,
          39,
          30,
          39,
          35,
          41,
          41,
          35,
          43,
          27,
          32,
          39,
          37,
          40,
          33,
          29,
          50,
          39,
          32,
          49,
          37,
          57,
          44,
          36,
          55,
          40,
          40,
          44,
          41,
          65,
          31,
          48,
          55,
          34,
          36,
          33,
          46,
          52,
          33,
          29,
          36,
          37,
          53,
          28,
          19,
          41,
          33,
          28,
          40,
          78,
          29,
          36,
          59,
          34,
          82,
          36,
          48,
          36,
          25,
          40,
          33,
          30,
          27,
          34,
          28,
          40,
          47,
          59,
          30,
          39,
          44,
          46,
          32,
          35,
          35,
          38,
          27,
          32,
          37,
          59,
          32,
          31,
          55,
          26,
          30,
          34,
          83,
          46,
          48,
          47,
          31,
          32,
          32,
          39,
          54,
          55,
          28,
          48,
          44,
          47,
          43,
          53,
          42,
          39,
          37,
          58,
          31,
          40,
          38,
          46,
          31,
          32,
          46,
          25,
          28,
          40,
          31,
          39,
          36,
          37,
          48,
          34,
          24,
          33,
          54,
          56,
          41,
          46,
          27,
          32,
          40,
          38,
          36,
          33,
          31,
          34,
          34,
          45,
          52,
          37,
          30,
          35,
          31,
          47,
          32,
          32,
          39,
          31,
          31,
          44,
          40,
          77,
          34,
          48,
          31,
          31,
          32,
          42,
          35,
          57,
          56,
          34,
          52,
          31,
          42,
          46,
          32,
          47,
          46,
          41,
          42,
          31,
          47,
          60,
          35,
          46,
          40,
          48,
          52,
          50,
          34,
          34,
          44,
          44,
          28,
          28,
          29,
          29,
          55,
          31,
          31,
          32,
          31,
          47,
          34,
          53,
          29,
          50,
          49,
          32,
          42,
          37,
          36,
          33,
          36,
          30,
          57,
          37,
          25,
          70,
          41,
          26,
          53,
          55,
          23,
          45,
          34,
          35,
          30,
          36,
          48,
          67,
          35,
          38,
          44,
          54,
          42,
          36,
          26,
          35,
          33,
          58,
          21,
          34,
          38,
          41,
          36,
          38,
          25,
          34,
          34,
          45,
          30,
          42,
          50,
          33,
          35,
          59,
          38,
          33,
          45,
          37,
          35,
          41,
          33,
          30,
          49,
          39,
          44,
          38,
          27,
          45,
          40,
          53,
          32,
          34,
          29,
          34,
          40,
          44,
          36,
          39,
          58,
          37,
          31,
          46,
          26,
          37,
          39,
          36,
          29,
          47,
          47,
          44,
          24,
          30,
          55,
          41,
          57,
          34,
          31,
          56,
          53,
          33,
          67,
          53,
          41,
          46,
          58,
          39,
          30,
          37,
          45,
          50,
          30,
          29,
          32,
          37,
          26,
          51,
          38,
          72,
          42,
          57,
          22,
          46,
          32,
          42,
          37,
          27,
          30,
          34,
          56,
          54,
          57,
          56,
          41,
          32,
          34,
          30,
          42,
          40,
          36,
          42,
          34,
          30,
          35,
          44,
          46,
          38,
          32,
          34,
          53,
          38,
          54,
          32,
          33,
          46,
          49,
          37,
          40,
          35,
          26,
          27,
          39,
          31,
          53,
          50,
          36,
          46,
          36,
          38,
          44,
          56,
          37,
          51,
          41,
          55,
          38,
          21,
          40,
          39,
          30,
          28,
          58,
          50,
          32,
          44,
          47,
          45,
          48,
          60,
          45,
          31,
          30,
          52,
          31,
          53,
          47,
          42,
          57,
          35,
          47,
          37,
          55,
          58,
          33,
          29,
          41,
          53,
          41,
          42,
          52,
          42,
          33,
          44,
          53,
          35,
          47,
          59,
          31,
          40,
          55,
          41,
          46,
          35,
          46,
          31,
          49,
          37,
          41,
          42,
          34,
          55,
          49,
          59,
          26,
          43,
          35,
          43,
          51,
          29,
          42,
          73,
          39,
          39,
          44,
          30,
          32,
          57,
          55,
          24,
          41,
          39,
          52,
          28,
          56,
          33,
          41,
          32,
          50,
          28,
          36,
          36,
          53,
          40,
          34,
          35,
          55,
          49,
          57,
          44,
          57,
          36,
          53,
          25,
          26,
          33,
          33,
          29,
          34,
          46,
          25,
          57,
          50,
          48,
          32,
          45,
          32,
          34,
          56,
          49,
          54,
          44,
          35,
          31,
          34,
          38,
          56,
          49,
          44,
          33,
          33,
          29,
          34,
          57,
          34,
          55,
          47,
          48,
          36,
          34,
          43,
          55,
          58,
          62,
          32,
          35,
          33,
          54,
          41,
          58,
          34,
          58,
          30,
          40,
          35,
          37,
          32,
          33,
          48,
          46,
          35,
          32,
          30,
          46,
          27,
          40,
          60,
          28,
          27,
          31,
          28,
          27,
          36,
          41,
          30,
          52,
          35,
          37,
          37,
          48,
          38,
          44,
          43,
          35,
          40,
          42,
          51,
          46,
          40,
          51
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "age distribution in customer attrition "
        },
        "xaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "age"
         },
         "zerolinewidth": 1
        },
        "yaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "percent"
         },
         "zerolinewidth": 1
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a0a25a5c-495c-4004-b98f-adae4033784c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a0a25a5c-495c-4004-b98f-adae4033784c\")) {                    Plotly.newPlot(                        \"a0a25a5c-495c-4004-b98f-adae4033784c\",                        [{\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Active\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [24, 31, 41, 34, 31, 31, 52, 38, 31, 34, 22, 60, 28, 31, 32, 77, 70, 29, 34, 38, 45, 40, 34, 30, 56, 50, 53, 48, 53, 27, 46, 54, 36, 31, 58, 48, 29, 57, 37, 53, 58, 33, 38, 28, 28, 37, 27, 36, 53, 26, 59, 31, 59, 40, 27, 32, 42, 43, 53, 30, 53, 53, 57, 33, 38, 33, 42, 32, 42, 41, 35, 48, 24, 59, 45, 39, 46, 38, 30, 52, 42, 46, 33, 52, 49, 59, 58, 26, 41, 41, 39, 27, 47, 52, 37, 38, 42, 41, 46, 40, 41, 60, 59, 59, 28, 45, 31, 31, 46, 51, 37, 40, 31, 38, 39, 29, 65, 26, 44, 27, 35, 36, 36, 24, 33, 53, 45, 37, 60, 30, 37, 34, 28, 44, 34, 70, 38, 32, 39, 38, 36, 32, 34, 49, 45, 24, 47, 36, 25, 30, 29, 27, 57, 38, 43, 35, 38, 71, 36, 40, 50, 31, 33, 57, 32, 52, 30, 40, 50, 32, 36, 60, 48, 72, 46, 45, 50, 29, 40, 49, 42, 43, 36, 37, 59, 75, 53, 53, 57, 30, 44, 51, 30, 42, 48, 40, 36, 40, 47, 39, 41, 42, 50, 58, 70, 29, 56, 32, 39, 33, 47, 39, 34, 36, 40, 34, 54, 36, 34, 50, 48, 35, 33, 42, 31, 54, 55, 30, 30, 59, 50, 59, 27, 25, 37, 36, 56, 52, 36, 30, 31, 55, 28, 44, 28, 25, 39, 32, 42, 31, 38, 30, 50, 50, 47, 29, 34, 54, 43, 47, 55, 35, 32, 49, 39, 32, 49, 32, 47, 42, 52, 31, 52, 38, 53, 40, 31, 53, 46, 49, 34, 63, 41, 55, 29, 47, 37, 38, 29, 41, 53, 40, 56, 34, 28, 41, 32, 51, 33, 34, 26, 31, 39, 50, 33, 44, 38, 33, 32, 33, 55, 27, 39, 21, 57, 29, 45, 46, 47, 44, 43, 44, 52, 33, 27, 43, 29, 43, 34, 30, 56, 46, 42, 51, 30, 41, 57, 36, 47, 37, 33, 49, 34, 54, 39, 58, 36, 46, 31, 31, 50, 40, 47, 26, 29, 38, 57, 47, 56, 37, 66, 49, 31, 30, 42, 55, 33, 33, 43, 32, 42, 53, 42, 37, 31, 57, 33, 57, 30, 31, 33, 33, 53, 38, 60, 36, 27, 32, 45, 28, 57, 37, 47, 35, 32, 41, 35, 26, 47, 36, 46, 30, 44, 47, 49, 55, 56, 28, 34, 55, 27, 38, 43, 55, 30, 48, 30, 58, 39, 35, 41, 32, 48, 76, 43, 46, 59, 29, 31, 36, 46, 37, 48, 31, 39, 39, 39, 55, 54, 32, 37, 36, 60, 25, 50, 59, 58, 49, 47, 38, 32, 40, 50, 35, 47, 27, 48, 40, 35, 39, 34, 36, 56, 30, 51, 41, 35, 53, 25, 34, 29, 33, 43, 47, 35, 49, 41, 49, 44, 32, 27, 53, 55, 60, 27, 58, 42, 31, 30, 32, 56, 57, 57, 39, 37, 32, 41, 49, 36, 58, 58, 33, 31, 46, 42, 46, 47, 42, 53, 31, 39, 57, 29, 32, 38, 34, 35, 55, 42, 32, 50, 56, 36, 49, 48, 32, 35, 48, 40, 55, 50, 55, 26, 34, 45, 44, 34, 39, 31, 27, 41, 45, 28, 39, 31, 31, 31, 35, 32, 49, 42, 31, 35, 31, 45, 24, 49, 36, 28, 54, 27, 34, 40, 32, 41, 38, 35, 31, 47, 41, 29, 33, 89, 54, 72, 29, 41, 39, 41, 31, 32, 52, 41, 45, 29, 32, 35, 54, 41, 42, 59, 58, 32, 56, 50, 45, 59, 47, 54, 40, 37, 29, 36, 33, 32, 53, 33, 36, 43, 45, 52, 48, 40, 31, 34, 54, 40, 55, 33, 50, 59, 40, 45, 32, 50, 58, 38, 57, 31, 69, 26, 30, 49, 36, 37, 55, 36, 31, 30, 36, 50, 53, 39, 31, 49, 35, 45, 30, 47, 37, 35, 41, 35, 60, 58, 28, 57, 35, 54, 31, 32, 56, 35, 33, 50, 46, 35, 49, 56, 33, 37, 30, 34, 29, 32, 29, 50, 58, 30, 52, 39, 43, 51, 38, 30, 52, 40, 37, 30, 51, 27, 35, 47, 29, 52, 43, 31, 54, 59, 51, 32, 42, 46, 30, 42, 39, 31, 35, 34, 42, 57, 38, 29, 46, 48, 33, 59, 23, 37, 40, 44, 37, 28, 39, 57, 38, 60, 38, 26, 32, 35, 30, 37, 36, 34, 28, 37, 31, 34, 28, 38, 80, 59, 54, 29, 39, 30, 41, 48, 26, 55, 36, 31, 35, 30, 26, 32, 43, 48, 49, 34, 27, 31, 35, 40, 39, 19, 40, 25, 38, 62, 27, 40, 56, 33, 52, 33, 38, 27, 54, 35, 41, 28, 55, 56, 33, 42, 34, 35, 46, 31, 48, 38, 47, 48, 57, 30, 39, 31, 44, 38, 58, 42, 33, 55, 34, 30, 35, 35, 56, 27, 39, 36, 35, 34, 28, 37, 57, 34, 38, 48, 35, 40, 48, 54, 49, 38, 44, 44, 60, 55, 27, 33, 32, 51, 44, 49, 42, 35, 32, 39, 54, 35, 41, 35, 41, 36, 51, 59, 44, 49, 37, 60, 40, 27, 34, 41, 39, 34, 38, 56, 42, 47, 32, 37, 25, 29, 48, 39, 31, 28, 32, 35, 51, 32, 30, 54, 41, 35, 44, 58, 43, 52, 60, 32, 41, 54, 30, 36, 27, 32, 34, 43, 43, 31, 37, 51, 52, 57, 32, 47, 31, 27, 31, 36, 44, 34, 32, 52, 60, 30, 48, 39, 59, 42, 77, 29, 49, 57, 37, 35, 63, 37, 37, 52, 60, 36, 48, 47, 56, 45, 53, 56, 59, 34, 45, 32, 60, 54, 41, 44, 40, 40, 29, 28, 35, 33, 33, 50, 56, 37, 53, 27, 46, 36, 37, 28, 40, 48, 46, 39, 37, 26, 35, 37, 32, 31, 36, 36, 38, 45, 58, 34, 51, 38, 73, 57, 45, 34, 48, 39, 35, 58, 36, 43, 34, 52, 36, 32, 23, 57, 43, 33, 45, 31, 41, 47, 39, 47, 57, 52, 45, 42, 32, 31, 25, 30, 33, 29, 55, 51, 68, 29, 35, 38, 39, 47, 22, 44, 38, 56, 56, 54, 39, 36, 42, 51, 32, 31, 23, 21, 45, 51, 36, 56, 41, 57, 55, 34, 32, 41, 32, 53, 43, 27, 48, 33, 41, 33, 25, 38, 43, 56, 28, 52, 32, 33, 31, 54, 36, 40, 32, 27, 42, 30, 43, 44, 46, 36, 44, 30, 34, 20, 31, 34, 27, 47, 56, 45, 35, 32, 44, 38, 32, 58, 35, 40, 36, 52, 35, 38, 43, 43, 35, 40, 49, 35, 42, 29, 26, 44, 29, 36, 30, 52, 55, 30, 29, 27, 58, 33, 48, 39, 55, 40, 33, 56, 42, 43, 41, 58, 29, 38, 49, 55, 53, 56, 31, 29, 48, 43, 29, 39, 53, 48, 34, 52, 30, 45, 54, 44, 35, 34, 22, 65, 49, 38, 28, 33, 34, 26, 37, 40, 28, 31, 42, 34, 34, 53, 55, 51, 58, 35, 35, 31, 32, 59, 50, 37, 30, 41, 43, 34, 42, 33, 26, 32, 41, 49, 51, 43, 45, 42, 37, 59, 31, 22, 30, 52, 50, 34, 30, 56, 31, 51, 33, 47, 46, 44, 51, 39, 41, 33, 31, 60, 34, 58, 36, 42, 44, 31, 51, 59, 33, 47, 43, 44, 29, 30, 43, 53, 46, 41, 44, 38, 43, 41, 44, 31, 59, 30, 35, 57, 53, 35, 49, 41, 40, 57, 38, 49, 38, 26, 34, 58, 30, 52, 30, 52, 34, 33, 34, 23, 37, 59, 42, 50, 41, 34, 57, 52, 61, 31, 27, 48, 27, 37, 28, 34, 42, 39, 30, 35, 43, 33, 24, 39, 27, 50, 30, 52, 30, 35, 36, 39, 43, 43, 46, 33, 56, 60, 48, 31, 36, 24, 27, 36, 29, 42, 44, 42, 39, 45, 36, 34, 43, 32, 38, 29, 28, 33, 43, 42, 33, 32, 42, 42, 29, 27, 38, 45, 32, 33, 48, 53, 23, 41, 41, 50, 28, 54, 38, 27, 54, 28, 32, 39, 29, 46, 37, 30, 41, 39, 38, 55, 29, 36, 60, 43, 45, 36, 53, 41, 51, 34, 29, 35, 40, 26, 32, 31, 39, 30, 40, 32, 46, 41, 35, 40, 58, 48, 37, 59, 33, 36, 40, 53, 32, 32, 43, 30, 31, 35, 39, 57, 33, 35, 53, 50, 46, 35, 56, 54, 56, 43, 40, 38, 43, 46, 29, 33, 49, 53, 54, 33, 39, 38, 49, 33, 59, 35, 40, 31, 35, 42, 55, 33, 31, 30, 30, 37, 30, 47, 41, 44, 59, 29, 51, 32, 36, 30, 34, 42, 37, 42, 31, 34, 32, 46, 32, 37, 39, 37, 32, 41, 50, 57, 37, 56, 37, 32, 30, 27, 36, 34, 53, 58, 31, 38, 32, 31, 59, 41, 33, 52, 36, 37, 39, 36, 30, 32, 47, 38, 51, 27, 58, 56, 40, 47, 23, 46, 57, 40, 30, 58, 36, 37, 51, 30, 39, 33, 60, 29, 31, 52, 30, 47, 46, 31, 29, 34, 44, 60, 51, 31, 33, 53, 45, 51, 30, 58, 26, 37, 47, 28, 28, 43, 43, 57, 34, 36, 32, 46, 36, 33, 48, 58, 56, 53, 41, 35, 31, 51, 40, 40, 44, 37, 49, 56, 25, 29, 35, 41, 38, 61, 36, 52, 31, 33, 49, 45, 71, 34, 33, 28, 52, 54, 51, 59, 25, 35, 49, 33, 30, 46, 43, 28, 41, 60, 49, 30, 45, 47, 29, 59, 27, 37, 25, 39, 35, 77, 58, 38, 50, 27, 46, 51, 58, 42, 27, 34, 31, 39, 42, 44, 51, 50, 32, 45, 29, 53, 29, 57, 41, 58, 52, 44, 34, 44, 56, 36, 45, 57, 49, 37, 44, 40, 28, 39, 40, 33, 37, 36, 39, 26, 48, 51, 35, 40, 40, 37, 43, 48, 46, 34, 52, 57, 36, 33, 30, 34, 35, 34, 32, 30, 33, 31, 33, 48, 38, 53, 36, 37, 36, 39, 50, 49, 41, 32, 30, 46, 25, 27, 28, 47, 39, 28, 45, 30, 48, 60, 48, 38, 31, 52, 28, 51, 32, 52, 23, 51, 49, 37, 32, 32, 56, 49, 49, 53, 32, 60, 40, 41, 55, 33, 49, 26, 56, 27, 55, 55, 26, 34, 38, 88, 38, 44, 43, 48, 35, 49, 33, 77, 56, 33, 44, 40, 55, 31, 42, 45, 38, 32, 35, 70, 41, 27, 56, 59, 31, 59, 32, 37, 59, 51, 42, 38, 35, 42, 43, 31, 56, 43, 39, 52, 32, 37, 35, 39, 44, 41, 26, 50, 33, 26, 31, 56, 40, 27, 41, 78, 60, 46, 25, 30, 37, 45, 31, 35, 47, 55, 28, 37, 34, 45, 35, 32, 26, 47, 33, 42, 31, 30, 40, 52, 48, 36, 36, 47, 31, 60, 49, 41, 67, 35, 34, 49, 33, 37, 29, 53, 36, 54, 55, 38, 50, 50, 35, 35, 51, 34, 28, 57, 38, 41, 38, 54, 52, 35, 40, 33, 35, 41, 33, 25, 34, 28, 41, 29, 38, 39, 58, 36, 46, 36, 59, 36, 34, 29, 39, 32, 29, 53, 29, 44, 28, 37, 45, 39, 34, 32, 40, 30, 55, 50, 45, 54, 37, 40, 39, 59, 40, 48, 73, 28, 50, 35, 45, 51, 47, 54, 54, 35, 74, 32, 56, 57, 33, 44, 30, 48, 38, 26, 24, 28, 55, 40, 39, 43, 32, 35, 53, 44, 42, 59, 32, 47, 30, 38, 40, 34, 33, 27, 39, 38, 55, 59, 37, 38, 20, 51, 48, 36, 30, 45, 31, 40, 58, 35, 42, 48, 52, 30, 43, 43, 30, 46, 43, 36, 42, 41, 50, 25, 36, 59, 50, 45, 49, 54, 33, 67, 31, 59, 28, 58, 30, 53, 51, 47, 30, 38, 53, 59, 57, 36, 42, 56, 38, 47, 30, 42, 34, 29, 49, 59, 30, 37, 27, 24, 55, 60, 30, 34, 36, 44, 38, 60, 58, 30, 39, 44, 32, 32, 47, 37, 86, 38, 31, 49, 53, 55, 30, 41, 31, 39, 43, 45, 42, 35, 43, 54, 43, 33, 31, 42, 34, 36, 51, 50, 32, 37, 50, 52, 30, 38, 50, 32, 24, 36, 42, 50, 41, 34, 29, 47, 44, 49, 39, 31, 36, 44, 29, 33, 40, 33, 54, 32, 57, 46, 30, 35, 36, 37, 41, 38, 34, 45, 53, 42, 55, 57, 58, 48, 33, 43, 29, 26, 56, 54, 33, 36, 46, 40, 50, 55, 41, 26, 35, 53, 31, 59, 42, 45, 58, 33, 47, 31, 40, 28, 42, 53, 33, 28, 50, 30, 37, 39, 29, 32, 38, 55, 38, 47, 57, 32, 37, 30, 39, 45, 25, 53, 39, 35, 58, 27, 39, 41, 56, 54, 46, 35, 41, 36, 37, 28, 49, 41, 60, 30, 48, 35, 28, 42, 32, 40, 53, 55, 41, 33, 58, 32, 36, 31, 33, 45, 40, 29, 53, 53, 35, 33, 38, 31, 48, 25, 35, 55, 31, 43, 31, 56, 34, 55, 25, 57, 47, 46, 41, 26, 42, 37, 34, 49, 32, 38, 37, 32, 43, 39, 34, 38, 40, 34, 34, 46, 31, 42, 45, 54, 45, 36, 28, 50, 39, 33, 30, 32, 27, 34, 47, 39, 30, 31, 38, 54, 34, 26, 41, 31, 31, 33, 35, 58, 38, 35, 38, 55, 31, 28, 40, 33, 33, 31, 25, 41, 34, 37, 41, 52, 54, 33, 60, 25, 33, 37, 50, 40, 60, 42, 40, 27, 44, 34, 34, 38, 31, 57, 31, 31, 56, 46, 31, 40, 54, 26, 51, 30, 54, 32, 54, 45, 31, 35, 42, 37, 43, 30, 49, 37, 54, 45, 39, 50, 30, 49, 31, 42, 40, 35, 30, 40, 39, 46, 55, 32, 37, 58, 36, 31, 49, 32, 30, 55, 32, 52, 38, 60, 60, 30, 44, 32, 46, 34, 40, 34, 43, 52, 35, 34, 33, 39, 32, 43, 34]}, {\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Churn\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [59, 56, 41, 55, 54, 42, 56, 60, 37, 28, 38, 30, 29, 46, 31, 35, 32, 49, 41, 49, 28, 43, 43, 43, 37, 35, 31, 43, 31, 28, 32, 60, 26, 40, 33, 32, 35, 33, 38, 23, 60, 48, 45, 36, 52, 35, 43, 52, 53, 48, 41, 39, 59, 41, 48, 40, 48, 60, 40, 57, 51, 41, 41, 52, 59, 44, 49, 40, 41, 44, 60, 29, 41, 41, 42, 36, 39, 31, 26, 31, 29, 37, 33, 24, 50, 27, 54, 30, 34, 37, 41, 47, 31, 31, 43, 37, 30, 41, 36, 35, 29, 35, 31, 35, 34, 31, 44, 36, 35, 41, 31, 34, 32, 36, 30, 37, 36, 31, 27, 34, 30, 32, 37, 32, 33, 35, 35, 30, 45, 45, 33, 44, 40, 31, 30, 44, 34, 37, 28, 32, 37, 34, 38, 28, 31, 59, 24, 26, 59, 30, 37, 30, 25, 44, 29, 26, 26, 27, 25, 28, 23, 34, 28, 51, 35, 35, 56, 45, 60, 56, 27, 30, 45, 29, 38, 30, 44, 36, 38, 36, 37, 35, 38, 40, 42, 46, 35, 34, 32, 50, 35, 36, 53, 46, 26, 45, 31, 29, 53, 27, 39, 36, 41, 36, 32, 35, 36, 35, 30, 42, 42, 32, 29, 44, 32, 31, 40, 43, 27, 37, 23, 25, 26, 47, 35, 32, 45, 30, 29, 38, 23, 27, 27, 29, 30, 36, 35, 36, 59, 47, 58, 37, 46, 43, 27, 38, 53, 47, 56, 41, 29, 50, 45, 35, 26, 26, 37, 30, 38, 37, 53, 29, 35, 28, 55, 49, 50, 43, 58, 38, 48, 59, 58, 36, 42, 41, 42, 39, 57, 52, 59, 56, 49, 35, 36, 40, 55, 58, 39, 47, 40, 35, 48, 48, 24, 49, 36, 52, 42, 36, 36, 48, 51, 51, 46, 37, 36, 47, 40, 38, 40, 39, 60, 39, 51, 27, 41, 42, 41, 33, 46, 31, 41, 41, 55, 53, 55, 39, 40, 59, 42, 41, 27, 25, 43, 44, 26, 52, 54, 56, 57, 29, 47, 57, 40, 46, 43, 45, 24, 40, 38, 36, 27, 26, 43, 25, 27, 30, 28, 27, 32, 41, 37, 27, 29, 33, 36, 28, 55, 51, 39, 42, 33, 36, 45, 37, 46, 32, 28, 45, 41, 59, 47, 31, 31, 33, 52, 26, 25, 25, 28, 33, 45, 36, 26, 38, 56, 26, 26, 25, 26, 40, 37, 43, 41, 49, 29, 39, 38, 26, 32, 33, 31, 49, 27, 30, 35, 26, 45, 34, 28, 25, 30, 30, 47, 57, 38, 54, 25, 30, 26, 31, 29, 49, 60, 60, 29, 55, 44, 47, 35, 35, 38, 34, 50, 43, 44, 51, 31, 39, 59, 25, 58, 30, 34, 51, 60, 51, 37, 30, 46, 29, 27, 28, 34, 29, 27, 26, 36, 44, 39, 30, 31, 56, 29, 43, 32, 30, 40, 42, 27, 35, 35, 32, 39, 31, 26, 42, 27, 33, 51, 33, 30, 52, 42, 31, 27, 44, 40, 46, 32, 56, 33, 33, 32, 41, 31, 30, 34, 43, 28, 32, 31, 35, 31, 25, 56, 47, 39, 51, 54, 58, 42, 56, 55, 26, 40, 26, 35, 34, 41, 44, 33, 36, 38, 35, 41, 41, 28, 26, 39, 25, 37, 25, 46, 25, 28, 33, 32, 32, 47, 50, 27, 45, 51, 36, 51, 33, 59, 38, 40, 51, 41, 38, 31, 28, 41, 33, 25, 45, 31, 28, 49, 25, 28, 54, 28, 27, 49, 53, 26, 37, 34, 51, 39, 35, 51, 30, 42, 54, 51, 57, 30, 37, 39, 35, 48, 31, 35, 48, 53, 27, 55, 40, 40, 38, 54, 54, 50, 48, 41, 51, 42, 58, 47, 58, 41, 50, 46, 44, 36, 58, 44, 49, 55, 52, 60, 41, 36, 41, 50, 44, 54, 35, 59, 42, 27, 59, 43, 39, 33, 36, 35, 50, 55, 51, 48, 55, 30, 53, 40, 44, 49, 40, 58, 49, 38, 51, 36, 55, 43, 60, 57, 41, 50, 53, 59, 59, 52, 58, 59, 49, 33, 55, 42, 52, 38, 55, 49, 56, 46, 57, 61, 45, 39, 46, 43, 34, 35, 36, 45, 47, 42, 40, 40, 32, 33, 32, 46, 50, 51, 30, 39, 47, 40, 50, 60, 32, 57, 41, 50, 46, 31, 36, 45, 33, 47, 35, 38, 33, 44, 31, 31, 49, 52, 31, 35, 31, 36, 39, 46, 48, 36, 30, 33, 57, 33, 35, 53, 41, 32, 41, 35, 48, 34, 47, 46, 60, 39, 57, 36, 30, 33, 30, 36, 49, 49, 40, 49, 38, 31, 34, 39, 40, 47, 37, 44, 55, 40, 54, 35, 32, 36, 37, 37, 50, 58, 52, 31, 40, 34, 53, 43, 30, 34, 48, 33, 37, 57, 55, 34, 44, 35, 34, 49, 30, 32, 56, 50, 32, 45, 52, 60, 47, 32, 55, 36, 46, 55, 30, 54, 31, 46, 61, 58, 38, 49, 43, 52, 33, 33, 53, 40, 44, 42, 33, 53, 60, 33, 37, 45, 34, 56, 57, 32, 50, 30, 56, 55, 34, 52, 32, 31, 44, 33, 53, 46, 46, 59, 51, 57, 43, 48, 33, 59, 58, 32, 44, 47, 34, 51, 60, 46, 41, 34, 31, 37, 59, 55, 33, 30, 36, 44, 36, 33, 36, 53, 47, 39, 35, 32, 46, 30, 36, 47, 52, 43, 44, 30, 32, 42, 33, 45, 45, 28, 32, 28, 44, 51, 56, 40, 49, 50, 58, 37, 27, 47, 39, 43, 34, 35, 58, 35, 42, 43, 34, 35, 38, 39, 58, 60, 56, 31, 37, 59, 49, 30, 31, 40, 43, 48, 34, 28, 34, 47, 59, 32, 31, 27, 35, 32, 57, 58, 30, 37, 44, 57, 34, 54, 55, 59, 53, 37, 45, 49, 31, 31, 35, 37, 48, 39, 34, 49, 41, 46, 54, 38, 39, 46, 58, 32, 30, 51, 30, 35, 30, 30, 43, 47, 51, 32, 53, 33, 40, 42, 46, 32, 38, 32, 30, 35, 36, 43, 42, 40, 32, 32, 41, 34, 40, 32, 32, 36, 45, 47, 35, 36, 33, 30, 32, 45, 47, 46, 39, 30, 45, 43, 53, 43, 54, 30, 30, 36, 33, 38, 37, 32, 59, 59, 46, 32, 52, 45, 52, 31, 31, 39, 35, 33, 50, 36, 34, 46, 47, 39, 31, 41, 59, 39, 48, 50, 31, 30, 31, 30, 34, 53, 34, 57, 41, 40, 37, 31, 58, 39, 47, 44, 45, 41, 33, 30, 30, 42, 49, 59, 31, 43, 38, 30, 35, 43, 34, 52, 46, 46, 37, 42, 23, 53, 26, 49, 34, 28, 46, 26, 31, 51, 47, 53, 29, 32, 26, 40, 33, 49, 60, 41, 36, 29, 42, 33, 31, 31, 26, 40, 45, 55, 29, 47, 50, 44, 56, 39, 59, 43, 36, 29, 38, 41, 58, 61, 36, 30, 43, 39, 48, 26, 51, 37, 29, 57, 25, 68, 34, 38, 54, 32, 41, 48, 59, 39, 31, 30, 40, 28, 52, 32, 34, 31, 57, 31, 29, 75, 32, 31, 41, 22, 31, 69, 30, 49, 29, 43, 28, 56, 39, 33, 38, 30, 29, 69, 44, 35, 31, 26, 26, 34, 42, 54, 53, 54, 48, 40, 28, 34, 34, 32, 31, 31, 66, 37, 33, 50, 33, 56, 26, 59, 32, 22, 34, 39, 28, 31, 33, 66, 26, 30, 28, 29, 49, 34, 47, 42, 43, 61, 35, 40, 31, 33, 69, 30, 41, 31, 22, 37, 48, 85, 72, 46, 26, 37, 53, 33, 90, 54, 67, 42, 71, 44, 47, 44, 37, 37, 28, 21, 32, 51, 26, 74, 65, 47, 38, 71, 29, 61, 57, 37, 68, 29, 51, 41, 24, 24, 59, 85, 51, 39, 44, 26, 48, 37, 71, 62, 26, 43, 30, 50, 25, 25, 31, 28, 66, 41, 46, 42, 28, 39, 42, 35, 55, 37, 29, 49, 44, 54, 28, 30, 47, 34, 37, 25, 29, 62, 58, 46, 56, 33, 27, 60, 32, 83, 35, 31, 30, 29, 36, 57, 21, 44, 23, 34, 32, 37, 35, 47, 61, 30, 70, 52, 28, 57, 24, 32, 65, 44, 32, 33, 53, 31, 58, 30, 47, 60, 36, 37, 43, 37, 31, 31, 30, 30, 32, 35, 59, 33, 37, 31, 26, 53, 34, 46, 76, 46, 83, 23, 34, 24, 41, 28, 34, 77, 36, 58, 46, 59, 19, 23, 27, 73, 41, 30, 26, 38, 46, 27, 63, 34, 32, 33, 47, 48, 58, 50, 26, 39, 55, 41, 54, 33, 42, 50, 40, 37, 44, 30, 26, 47, 46, 25, 56, 36, 35, 54, 48, 32, 60, 67, 32, 26, 33, 21, 30, 27, 30, 36, 27, 29, 38, 23, 38, 30, 41, 33, 35, 29, 68, 20, 30, 32, 28, 45, 29, 31, 39, 42, 35, 34, 26, 29, 44, 34, 33, 34, 33, 22, 32, 36, 44, 65, 61, 41, 24, 77, 62, 30, 29, 29, 65, 43, 70, 32, 46, 33, 57, 62, 32, 39, 76, 36, 27, 38, 56, 51, 39, 35, 32, 51, 37, 52, 35, 47, 31, 49, 30, 35, 44, 42, 29, 42, 43, 36, 36, 32, 54, 50, 32, 28, 40, 34, 52, 35, 32, 60, 34, 30, 39, 40, 38, 35, 37, 29, 36, 34, 33, 29, 33, 41, 74, 53, 38, 41, 52, 40, 32, 28, 28, 35, 46, 50, 44, 29, 61, 70, 44, 59, 45, 39, 36, 78, 28, 50, 73, 34, 28, 42, 54, 27, 73, 33, 51, 32, 33, 26, 32, 41, 39, 24, 37, 35, 26, 29, 63, 95, 65, 59, 35, 74, 56, 26, 30, 29, 78, 45, 29, 39, 45, 45, 36, 41, 38, 33, 30, 65, 75, 65, 53, 40, 31, 25, 37, 32, 45, 35, 74, 37, 59, 76, 25, 27, 39, 24, 28, 51, 66, 28, 30, 58, 32, 31, 71, 37, 56, 65, 72, 51, 35, 34, 29, 30, 39, 45, 28, 52, 59, 38, 47, 52, 38, 29, 36, 58, 31, 59, 47, 36, 21, 23, 47, 29, 25, 37, 33, 59, 58, 44, 48, 34, 42, 54, 42, 42, 44, 37, 29, 47, 24, 34, 30, 46, 34, 59, 35, 33, 35, 37, 35, 38, 33, 42, 33, 53, 53, 31, 26, 30, 24, 28, 26, 35, 28, 30, 32, 52, 38, 34, 29, 30, 31, 24, 25, 47, 33, 34, 36, 50, 34, 54, 47, 31, 36, 35, 25, 46, 49, 47, 35, 30, 31, 55, 25, 26, 85, 29, 39, 33, 33, 58, 61, 50, 54, 33, 36, 31, 24, 29, 40, 29, 30, 45, 37, 28, 33, 33, 45, 25, 41, 38, 47, 49, 42, 33, 36, 60, 33, 33, 33, 32, 36, 45, 35, 38, 30, 31, 32, 44, 39, 45, 32, 39, 30, 63, 56, 26, 37, 29, 24, 35, 43, 39, 59, 50, 44, 35, 30, 58, 37, 39, 31, 33, 41, 44, 46, 30, 35, 27, 39, 32, 37, 31, 30, 34, 31, 27, 36, 32, 49, 29, 29, 28, 60, 33, 35, 31, 40, 29, 35, 32, 26, 42, 33, 36, 35, 30, 59, 44, 50, 51, 51, 60, 29, 43, 31, 26, 23, 49, 42, 39, 45, 45, 39, 45, 39, 23, 57, 32, 29, 42, 47, 36, 57, 36, 53, 56, 33, 59, 47, 42, 38, 40, 41, 43, 24, 40, 40, 34, 36, 38, 39, 25, 39, 41, 36, 34, 34, 38, 40, 31, 37, 58, 48, 35, 34, 51, 39, 39, 36, 47, 35, 33, 47, 38, 46, 42, 47, 37, 31, 46, 36, 40, 39, 45, 59, 34, 40, 50, 36, 35, 36, 35, 47, 47, 38, 33, 38, 39, 43, 33, 34, 28, 34, 42, 33, 39, 32, 34, 32, 48, 31, 33, 48, 33, 30, 35, 51, 38, 39, 27, 50, 34, 38, 29, 40, 32, 38, 36, 28, 49, 26, 44, 49, 31, 55, 32, 35, 35, 31, 28, 39, 48, 32, 41, 34, 32, 34, 36, 29, 54, 29, 35, 36, 37, 37, 31, 33, 59, 30, 53, 26, 30, 35, 34, 33, 59, 36, 34, 39, 42, 60, 56, 30, 26, 35, 22, 26, 28, 35, 35, 55, 32, 28, 31, 23, 26, 46, 34, 60, 28, 29, 39, 54, 34, 27, 28, 39, 60, 26, 32, 40, 25, 48, 37, 39, 40, 31, 42, 51, 28, 32, 36, 36, 38, 36, 40, 37, 51, 34, 56, 35, 34, 39, 29, 33, 40, 22, 32, 33, 29, 29, 24, 31, 27, 32, 30, 27, 55, 28, 29, 55, 25, 36, 27, 25, 32, 33, 38, 45, 30, 32, 26, 25, 33, 32, 34, 29, 28, 38, 35, 28, 29, 20, 45, 37, 32, 59, 37, 59, 27, 39, 43, 39, 26, 39, 43, 27, 32, 29, 42, 53, 36, 27, 43, 60, 29, 26, 26, 28, 28, 28, 40, 24, 56, 34, 28, 37, 32, 43, 36, 35, 29, 27, 32, 33, 24, 58, 43, 26, 31, 25, 58, 48, 60, 25, 26, 25, 26, 32, 25, 21, 41, 30, 47, 37, 34, 28, 24, 26, 35, 54, 44, 56, 25, 25, 50, 55, 57, 48, 25, 49, 25, 38, 45, 33, 45, 26, 49, 25, 53, 43, 49, 54, 47, 34, 54, 43, 23, 48, 30, 38, 34, 52, 27, 40, 47, 26, 37, 34, 26, 44, 29, 40, 43, 52, 38, 33, 50, 53, 42, 49, 27, 32, 31, 29, 49, 46, 57, 28, 37, 38, 49, 35, 29, 52, 34, 60, 32, 27, 55, 29, 42, 33, 29, 39, 31, 60, 25, 45, 29, 55, 26, 30, 54, 39, 26, 57, 39, 25, 43, 32, 43, 50, 35, 37, 42, 27, 27, 29, 29, 26, 39, 41, 29, 33, 32, 30, 38, 55, 25, 38, 35, 31, 25, 31, 27, 34, 40, 27, 28, 28, 35, 58, 44, 57, 25, 21, 49, 54, 59, 28, 37, 30, 35, 31, 41, 30, 28, 31, 53, 22, 36, 33, 31, 49, 51, 34, 35, 32, 36, 31, 25, 25, 60, 40, 50, 32, 60, 32, 52, 59, 56, 47, 43, 25, 31, 27, 26, 37, 27, 49, 26, 28, 33, 31, 32, 30, 34, 58, 34, 26, 49, 57, 83, 61, 64, 37, 68, 71, 54, 37, 22, 44, 37, 72, 69, 41, 44, 32, 79, 33, 45, 40, 42, 30, 28, 58, 39, 31, 45, 56, 46, 56, 27, 30, 26, 35, 52, 54, 41, 24, 28, 62, 52, 23, 82, 63, 32, 31, 37, 40, 66, 37, 38, 27, 58, 49, 31, 31, 38, 79, 71, 52, 55, 46, 28, 22, 49, 20, 19, 34, 73, 27, 59, 25, 44, 28, 25, 28, 52, 57, 49, 33, 47, 68, 40, 46, 29, 55, 30, 49, 48, 21, 45, 43, 47, 27, 20, 37, 61, 57, 40, 53, 65, 67, 66, 67, 57, 38, 56, 46, 69, 32, 72, 41, 72, 33, 37, 34, 44, 42, 58, 34, 55, 71, 23, 60, 18, 54, 59, 32, 61, 31, 42, 42, 42, 70, 57, 60, 53, 48, 30, 39, 42, 26, 46, 40, 39, 58, 42, 31, 21, 35, 50, 36, 75, 20, 57, 41, 59, 36, 18, 34, 55, 33, 43, 34, 35, 43, 74, 45, 38, 32, 29, 31, 50, 44, 23, 56, 76, 28, 34, 56, 45, 36, 57, 49, 49, 37, 46, 26, 47, 36, 37, 62, 26, 45, 63, 25, 60, 44, 33, 39, 46, 42, 56, 82, 77, 36, 35, 50, 62, 26, 55, 22, 30, 34, 31, 55, 49, 67, 43, 25, 59, 44, 42, 29, 32, 30, 38, 22, 25, 34, 27, 35, 45, 42, 20, 60, 47, 25, 65, 50, 22, 43, 53, 71, 28, 59, 29, 51, 48, 37, 37, 64, 69, 36, 43, 63, 70, 60, 86, 72, 60, 43, 57, 28, 36, 66, 33, 36, 29, 62, 34, 29, 30, 32, 39, 55, 36, 27, 45, 55, 52, 49, 30, 39, 48, 36, 27, 51, 48, 70, 63, 34, 46, 31, 32, 40, 30, 55, 57, 59, 26, 73, 30, 64, 36, 55, 63, 52, 34, 36, 31, 47, 50, 70, 18, 33, 24, 25, 26, 32, 25, 58, 42, 30, 76, 79, 70, 30, 60, 51, 71, 31, 32, 34, 28, 45, 53, 38, 56, 42, 47, 73, 46, 73, 28, 46, 74, 71, 41, 73, 47, 49, 40, 57, 50, 34, 56, 32, 50, 54, 26, 40, 29, 52, 38, 46, 26, 25, 70, 83, 43, 45, 37, 71, 27, 47, 42, 77, 61, 57, 34, 40, 84, 26, 37, 47, 37, 28, 54, 54, 61, 30, 60, 44, 33, 25, 54, 24, 36, 58, 33, 27, 60, 56, 25, 21, 73, 67, 51, 48, 35, 61, 35, 36, 83, 50, 66, 58, 30, 54, 26, 43, 40, 57, 75, 77, 31, 22, 50, 27, 73, 37, 44, 40, 61, 61, 47, 47, 76, 30, 39, 72, 77, 36, 65, 54, 46, 28, 61, 53, 27, 55, 33, 87, 57, 33, 51, 76, 19, 33, 55, 31, 92, 30, 68, 41, 29, 27, 78, 82, 29, 76, 38, 60, 33, 43, 61, 43, 57, 33, 77, 35, 59, 76, 73, 81, 41, 57, 28, 38, 52, 22, 53, 60, 56, 38, 44, 69, 36, 75, 65, 64, 30, 57, 31, 36, 64, 54, 26, 51, 72, 30, 60, 30, 66, 50, 47, 80, 61, 47, 30, 21, 62, 26, 36, 87, 61, 70, 60, 41, 22, 60, 54, 35, 59, 29, 48, 37, 36, 52, 67, 36, 64, 29, 27, 76, 34, 54, 67, 24, 53, 33, 24, 32, 60, 64, 35, 24, 42, 40, 33, 71, 52, 47, 49, 26, 40, 32, 33, 35, 53, 20, 44, 21, 36, 31, 42, 33, 29, 30, 32, 54, 82, 47, 48, 77, 38, 27, 43, 33, 27, 76, 81, 71, 39, 18, 58, 44, 67, 31, 61, 36, 56, 24, 33, 34, 58, 64, 27, 63, 23, 37, 43, 35, 33, 75, 40, 52, 55, 71, 78, 35, 52, 31, 31, 38, 28, 72, 29, 46, 53, 58, 22, 28, 31, 26, 60, 29, 36, 68, 48, 53, 39, 28, 72, 29, 26, 49, 43, 28, 44, 73, 39, 29, 31, 30, 50, 42, 37, 49, 59, 80, 86, 69, 79, 36, 49, 69, 66, 32, 47, 58, 64, 72, 79, 30, 36, 53, 34, 29, 29, 81, 57, 72, 34, 57, 66, 27, 50, 75, 50, 32, 59, 68, 31, 64, 30, 60, 35, 72, 30, 52, 33, 84, 43, 39, 25, 38, 44, 20, 32, 23, 35, 61, 46, 70, 25, 57, 55, 33, 73, 34, 31, 25, 30, 28, 21, 87, 22, 28, 31, 60, 41, 24, 54, 35, 70, 62, 59, 48, 45, 36, 33, 32, 60, 39, 37, 85, 79, 64, 42, 47, 66, 61, 27, 63, 36, 38, 92, 77, 30, 81, 42, 48, 52, 38, 61, 20, 39, 44, 73, 36, 48, 52, 30, 36, 80, 33, 56, 54, 34, 66, 52, 60, 28, 56, 36, 45, 60, 33, 77, 66, 45, 30, 56, 64, 30, 33, 72, 44, 37, 52, 37, 34, 78, 45, 68, 53, 39, 55, 32, 76, 35, 55, 41, 35, 80, 38, 48, 25, 36, 29, 58, 46, 23, 67, 30, 22, 45, 40, 35, 33, 60, 24, 18, 65, 27, 35, 39, 53, 23, 34, 36, 52, 36, 61, 58, 48, 34, 58, 39, 44, 54, 27, 77, 30, 61, 76, 28, 35, 59, 42, 35, 71, 60, 34, 57, 75, 64, 36, 29, 34, 26, 41, 47, 52, 32, 21, 54, 29, 32, 38, 48, 46, 32, 49, 37, 37, 23, 58, 45, 30, 23, 34, 27, 28, 61, 27, 34, 52, 28, 35, 66, 52, 26, 38, 59, 47, 22, 82, 60, 53, 39, 27, 35, 80, 47, 72, 45, 37, 29, 40, 32, 20, 48, 60, 70, 78, 52, 61, 27, 63, 59, 22, 66, 74, 52, 71, 33, 56, 30, 73, 28, 27, 38, 28, 37, 47, 60, 42, 27, 25, 90, 29, 29, 63, 32, 32, 42, 26, 73, 26, 60, 25, 64, 68, 39, 40, 37, 36, 33, 83, 31, 64, 59, 33, 35, 69, 31, 73, 35, 28, 34, 33, 36, 44, 19, 28, 51, 52, 61, 39, 37, 80, 61, 72, 36, 74, 35, 82, 71, 38, 38, 53, 29, 64, 35, 35, 46, 25, 46, 59, 35, 22, 31, 43, 35, 60, 54, 29, 30, 28, 43, 37, 62, 57, 33, 47, 39, 30, 28, 57, 23, 56, 44, 80, 47, 27, 26, 50, 84, 86, 46, 45, 60, 47, 53, 54, 19, 24, 39, 53, 29, 30, 32, 33, 33, 57, 27, 55, 36, 26, 31, 25, 49, 30, 32, 36, 38, 45, 36, 23, 38, 44, 35, 75, 26, 59, 30, 36, 39, 24, 22, 36, 22, 34, 22, 57, 30, 55, 38, 39, 38, 24, 30, 29, 25, 52, 62, 40, 76, 31, 42, 43, 73, 41, 77, 46, 35, 26, 45, 29, 30, 26, 63, 37, 26, 54, 30, 61, 69, 76, 30, 64, 55, 55, 29, 41, 63, 45, 32, 41, 37, 19, 61, 40, 30, 54, 49, 45, 48, 60, 34, 29, 77, 64, 80, 26, 33, 41, 28, 74, 29, 72, 18, 46, 37, 37, 32, 51, 77, 35, 53, 46, 78, 34, 54, 33, 23, 34, 32, 54, 40, 63, 37, 48, 28, 35, 29, 68, 53, 37, 58, 37, 37, 34, 50, 56, 57, 29, 75, 24, 46, 27, 52, 38, 33, 53, 64, 84, 62, 41, 29, 53, 30, 37, 78, 47, 61, 46, 31, 21, 71, 52, 25, 60, 30, 29, 52, 59, 38, 33, 31, 40, 28, 66, 32, 67, 39, 46, 54, 47, 45, 63, 60, 29, 33, 48, 26, 55, 36, 33, 28, 28, 31, 28, 49, 27, 28, 65, 30, 38, 36, 63, 63, 28, 80, 39, 31, 39, 30, 67, 29, 36, 24, 30, 21, 27, 56, 66, 71, 26, 62, 32, 68, 37, 27, 45, 64, 38, 66, 30, 39, 24, 49, 34, 46, 32, 52, 25, 57, 64, 33, 36, 59, 25, 43, 37, 49, 42, 27, 29, 37, 47, 38, 48, 55, 39, 49, 68, 45, 67, 60, 31, 58, 41, 23, 35, 33, 32, 48, 43, 32, 74, 32, 63, 28, 31, 26, 31, 39, 28, 30, 24, 32, 46, 60, 37, 48, 52, 29, 71, 47, 70, 28, 31, 80, 29, 23, 34, 69, 19, 35, 38, 32, 50, 27, 53, 35, 43, 30, 57, 31, 65, 25, 33, 79, 24, 27, 36, 26, 51, 70, 23, 41, 53, 86, 27, 34, 68, 35, 28, 66, 30, 60, 46, 68, 63, 29, 35, 64, 22, 19, 31, 48, 33, 34, 46, 60, 36, 32, 45, 44, 57, 29, 31, 54, 49, 35, 24, 43, 34, 27, 54, 38, 34, 93, 36, 33, 35, 41, 36, 30, 41, 29, 82, 37, 46, 44, 61, 33, 23, 45, 61, 40, 82, 27, 26, 27, 53, 54, 64, 57, 31, 61, 35, 77, 36, 32, 61, 69, 69, 34, 45, 64, 35, 22, 34, 34, 55, 53, 63, 37, 29, 29, 38, 62, 34, 56, 48, 42, 68, 43, 62, 68, 38, 63, 48, 45, 59, 35, 32, 93, 33, 48, 38, 29, 38, 41, 57, 53, 56, 33, 41, 34, 29, 34, 21, 37, 33, 45, 61, 22, 37, 45, 35, 28, 31, 62, 47, 57, 60, 25, 40, 33, 53, 30, 36, 56, 38, 29, 61, 62, 24, 65, 70, 60, 30, 51, 36, 42, 58, 43, 77, 34, 35, 39, 35, 41, 25, 51, 48, 26, 55, 49, 29, 67, 42, 35, 37, 35, 61, 60, 33, 27, 24, 57, 63, 61, 52, 34, 28, 60, 32, 34, 31, 28, 55, 65, 37, 35, 33, 75, 48, 33, 38, 25, 41, 25, 27, 49, 43, 52, 48, 29, 43, 65, 72, 26, 37, 37, 28, 61, 32, 51, 38, 34, 60, 24, 36, 46, 80, 37, 39, 62, 84, 24, 24, 28, 31, 33, 33, 64, 48, 28, 28, 69, 28, 24, 52, 37, 25, 58, 62, 41, 28, 75, 21, 40, 35, 49, 39, 63, 38, 72, 31, 49, 29, 39, 78, 61, 60, 58, 67, 20, 28, 41, 31, 37, 81, 46, 46, 44, 23, 40, 27, 60, 55, 43, 26, 25, 31, 54, 29, 36, 59, 37, 28, 35, 32, 49, 50, 55, 81, 24, 38, 61, 67, 28, 29, 31, 71, 33, 47, 39, 26, 22, 34, 32, 48, 32, 58, 48, 28, 36, 67, 30, 27, 31, 27, 33, 36, 62, 29, 26, 59, 51, 27, 60, 42, 47, 43, 22, 34, 39, 27, 49, 42, 64, 44, 64, 29, 52, 52, 54, 37, 48, 28, 77, 63, 34, 29, 38, 47, 63, 30, 74, 31, 33, 25, 78, 26, 47, 56, 54, 46, 47, 24, 70, 63, 22, 38, 44, 52, 35, 29, 41, 62, 62, 38, 36, 34, 60, 32, 29, 32, 75, 29, 68, 25, 36, 38, 53, 34, 23, 73, 25, 51, 72, 42, 59, 32, 36, 34, 28, 54, 27, 42, 31, 41, 37, 31, 33, 27, 44, 22, 25, 30, 46, 32, 57, 41, 62, 28, 77, 47, 40, 41, 28, 51, 47, 37, 29, 24, 27, 23, 32, 40, 32, 57, 28, 35, 35, 24, 36, 66, 45, 28, 45, 31, 45, 34, 39, 24, 33, 54, 26, 30, 37, 32, 39, 27, 54, 54, 71, 41, 29, 60, 60, 36, 34, 27, 34, 36, 32, 55, 34, 25, 51, 33, 35, 55, 61, 34, 55, 23, 69, 37, 49, 41, 39, 30, 32, 61, 33, 32, 45, 59, 48, 18, 27, 32, 50, 30, 23, 40, 27, 35, 53, 59, 37, 24, 42, 27, 29, 30, 22, 39, 35, 61, 33, 42, 30, 51, 25, 44, 34, 28, 61, 21, 53, 46, 52, 56, 36, 30, 41, 33, 28, 54, 50, 58, 33, 36, 51, 35, 35, 34, 45, 24, 45, 42, 30, 60, 40, 25, 48, 28, 55, 47, 32, 55, 27, 42, 47, 36, 64, 34, 72, 27, 54, 19, 33, 36, 26, 42, 24, 28, 58, 59, 46, 49, 45, 32, 46, 29, 30, 31, 31, 33, 49, 30, 29, 44, 34, 62, 58, 60, 62, 53, 32, 39, 58, 38, 56, 52, 45, 28, 29, 29, 60, 31, 31, 57, 25, 27, 24, 39, 27, 24, 56, 33, 38, 34, 45, 35, 52, 38, 23, 38, 36, 33, 43, 41, 46, 60, 40, 33, 35, 36, 28, 60, 47, 35, 53, 39, 46, 34, 58, 34, 33, 42, 32, 47, 35, 77, 62, 62, 38, 37, 38, 31, 38, 31, 34, 33, 38, 28, 67, 31, 49, 53, 31, 60, 30, 38, 49, 40, 34, 35, 33, 34, 28, 27, 22, 27, 34, 62, 36, 47, 32, 33, 27, 35, 47, 57, 26, 46, 58, 43, 49, 34, 28, 33, 25, 26, 32, 29, 30, 23, 41, 66, 62, 28, 33, 77, 54, 50, 31, 45, 30, 40, 67, 39, 20, 31, 31, 33, 45, 45, 31, 33, 41, 31, 50, 56, 28, 35, 30, 64, 32, 29, 29, 51, 40, 45, 36, 31, 43, 42, 46, 34, 52, 47, 40, 26, 45, 55, 42, 40, 43, 26, 31, 31, 29, 44, 35, 36, 30, 48, 33, 36, 46, 35, 46, 32, 46, 54, 36, 29, 58, 35, 30, 31, 35, 36, 34, 28, 23, 46, 33, 25, 43, 38, 62, 33, 53, 38, 32, 24, 26, 55, 47, 58, 30, 39, 30, 34, 34, 56, 27, 46, 32, 27, 36, 46, 46, 49, 39, 33, 48, 38, 60, 33, 64, 22, 34, 36, 72, 37, 78, 31, 20, 60, 36, 51, 26, 48, 49, 20, 77, 42, 31, 56, 32, 33, 33, 54, 28, 41, 51, 43, 36, 49, 47, 30, 31, 39, 40, 48, 30, 41, 33, 25, 49, 32, 64, 28, 37, 25, 53, 62, 34, 26, 39, 45, 19, 33, 43, 35, 52, 28, 35, 50, 37, 36, 61, 24, 35, 32, 45, 43, 39, 35, 23, 34, 47, 21, 37, 53, 46, 26, 51, 58, 51, 33, 46, 34, 40, 40, 46, 59, 34, 49, 28, 56, 28, 33, 37, 38, 25, 74, 42, 29, 60, 41, 51, 49, 41, 31, 25, 32, 73, 31, 32, 27, 60, 34, 43, 31, 42, 27, 57, 30, 45, 49, 31, 41, 57, 57, 58, 54, 36, 53, 34, 27, 41, 62, 27, 37, 22, 27, 30, 28, 27, 42, 36, 27, 58, 55, 52, 42, 46, 32, 52, 57, 40, 30, 30, 39, 28, 34, 30, 24, 59, 38, 47, 52, 35, 61, 46, 55, 42, 26, 58, 46, 39, 29, 21, 57, 31, 49, 56, 30, 34, 35, 55, 44, 36, 63, 37, 58, 37, 26, 44, 37, 53, 37, 40, 40, 31, 36, 36, 52, 39, 47, 49, 32, 36, 27, 71, 32, 29, 64, 24, 21, 42, 47, 53, 33, 62, 33, 58, 52, 37, 37, 34, 79, 54, 28, 33, 21, 46, 38, 32, 34, 49, 31, 32, 38, 27, 78, 60, 36, 75, 30, 30, 50, 37, 60, 26, 49, 40, 46, 38, 74, 35, 50, 54, 35, 36, 30, 36, 41, 30, 39, 34, 32, 35, 50, 28, 44, 32, 30, 59, 35, 30, 42, 63, 35, 43, 27, 29, 54, 80, 57, 33, 65, 59, 30, 28, 29, 31, 31, 49, 43, 40, 39, 71, 38, 30, 45, 67, 58, 45, 34, 27, 51, 38, 28, 25, 37, 38, 37, 34, 37, 76, 48, 49, 62, 33, 19, 25, 31, 38, 31, 24, 24, 26, 73, 29, 49, 42, 68, 60, 53, 62, 33, 36, 46, 37, 53, 34, 24, 36, 34, 48, 43, 32, 46, 32, 45, 29, 25, 20, 45, 42, 29, 23, 37, 35, 61, 31, 41, 34, 32, 32, 34, 35, 44, 32, 34, 50, 32, 23, 32, 60, 44, 30, 37, 35, 36, 39, 25, 33, 28, 31, 56, 41, 68, 61, 37, 28, 46, 53, 32, 33, 57, 41, 24, 45, 49, 30, 26, 57, 39, 40, 52, 35, 46, 49, 22, 36, 38, 40, 24, 78, 25, 63, 31, 41, 50, 27, 27, 50, 44, 26, 46, 36, 35, 68, 50, 36, 34, 30, 22, 59, 60, 32, 46, 23, 22, 59, 26, 35, 53, 39, 32, 41, 37, 35, 42, 27, 57, 41, 37, 44, 32, 34, 42, 54, 48, 73, 33, 42, 57, 29, 47, 51, 34, 25, 50, 47, 40, 79, 30, 54, 53, 33, 60, 35, 46, 52, 50, 47, 73, 35, 26, 46, 42, 29, 30, 37, 28, 32, 53, 41, 40, 53, 64, 32, 48, 35, 43, 25, 43, 29, 61, 64, 27, 38, 36, 61, 49, 33, 25, 33, 51, 32, 32, 72, 34, 38, 35, 38, 43, 31, 30, 22, 46, 52, 35, 71, 30, 25, 34, 51, 25, 48, 42, 55, 38, 26, 27, 75, 32, 39, 46, 40, 36, 54, 42, 42, 37, 25, 29, 32, 64, 55, 56, 28, 52, 36, 25, 66, 31, 43, 59, 35, 48, 38, 27, 25, 56, 54, 36, 31, 62, 31, 66, 32, 40, 43, 51, 34, 47, 36, 27, 52, 44, 26, 32, 34, 35, 32, 43, 50, 40, 29, 32, 50, 65, 59, 44, 33, 33, 57, 38, 49, 45, 25, 22, 32, 36, 32, 45, 35, 23, 50, 31, 61, 42, 39, 37, 30, 55, 30, 57, 31, 37, 34, 31, 52, 48, 73, 54, 38, 39, 44, 29, 51, 31, 33, 39, 30, 39, 25, 57, 35, 27, 27, 28, 41, 46, 38, 35, 24, 24, 28, 28, 53, 45, 33, 51, 32, 65, 32, 67, 53, 41, 30, 37, 27, 77, 32, 25, 32, 43, 30, 59, 61, 32, 37, 51, 55, 38, 47, 59, 51, 25, 67, 58, 52, 54, 27, 41, 72, 24, 36, 39, 55, 35, 32, 29, 59, 33, 30, 30, 37, 39, 40, 29, 24, 31, 62, 33, 67, 44, 79, 30, 38, 31, 38, 36, 30, 34, 27, 32, 37, 56, 28, 49, 23, 56, 54, 59, 53, 51, 31, 62, 38, 25, 43, 45, 78, 30, 69, 48, 31, 57, 45, 48, 34, 37, 32, 35, 31, 54, 52, 29, 34, 37, 66, 33, 60, 55, 34, 52, 29, 43, 57, 32, 67, 45, 47, 31, 52, 41, 35, 48, 53, 59, 26, 30, 56, 41, 52, 35, 41, 39, 51, 46, 43, 31, 56, 35, 58, 33, 36, 49, 47, 61, 45, 53, 31, 35, 55, 50, 53, 45, 45, 32, 33, 51, 27, 37, 45, 40, 41, 35, 62, 42, 38, 36, 59, 46, 42, 27, 44, 32, 32, 43, 38, 40, 44, 49, 37, 40, 54, 38, 45, 49, 46, 39, 56, 37, 39, 53, 38, 25, 35, 47, 40, 31, 50, 40, 55, 31, 59, 52, 55, 49, 31, 55, 38, 52, 57, 65, 53, 34, 41, 44, 49, 39, 54, 22, 31, 53, 59, 59, 56, 52, 57, 41, 39, 35, 53, 35, 25, 34, 38, 42, 28, 35, 34, 32, 38, 32, 41, 41, 26, 60, 51, 24, 37, 44, 74, 50, 31, 31, 25, 39, 30, 60, 48, 30, 43, 44, 29, 46, 28, 34, 42, 59, 27, 36, 34, 30, 50, 50, 45, 40, 49, 54, 31, 35, 34, 48, 51, 47, 45, 33, 33, 27, 47, 54, 37, 43, 49, 43, 27, 44, 37, 50, 50, 52, 26, 35, 47, 55, 26, 37, 56, 33, 36, 32, 30, 39, 28, 36, 39, 48, 34, 50, 26, 35, 44, 47, 62, 55, 26, 88, 28, 41, 25, 40, 35, 65, 40, 43, 29, 48, 35, 30, 32, 30, 30, 24, 29, 44, 42, 45, 57, 36, 49, 30, 48, 32, 35, 40, 32, 37, 37, 43, 48, 36, 33, 33, 36, 65, 50, 34, 35, 32, 34, 34, 33, 45, 35, 53, 39, 38, 35, 52, 30, 37, 33, 52, 29, 28, 37, 33, 32, 25, 37, 34, 40, 40, 38, 39, 37, 35, 33, 38, 37, 29, 42, 45, 42, 33, 35, 30, 31, 45, 44, 31, 30, 54, 35, 36, 55, 45, 30, 57, 59, 47, 34, 46, 56, 24, 40, 50, 31, 37, 45, 45, 36, 57, 39, 47, 44, 39, 40, 48, 56, 50, 29, 33, 57, 54, 64, 61, 36, 50, 30, 40, 40, 30, 55, 59, 37, 53, 56, 33, 49, 47, 34, 33, 31, 44, 50, 39, 41, 33, 40, 42, 37, 56, 49, 35, 41, 52, 33, 40, 44, 39, 39, 28, 45, 63, 32, 28, 36, 34, 45, 30, 59, 54, 47, 41, 35, 38, 36, 39, 48, 42, 24, 57, 35, 32, 26, 27, 47, 32, 48, 43, 30, 28, 32, 26, 38, 30, 33, 37, 47, 36, 48, 47, 34, 28, 54, 33, 41, 41, 42, 32, 47, 36, 34, 36, 50, 53, 49, 39, 47, 57, 30, 54, 35, 47, 38, 40, 51, 51, 57, 41, 31, 59, 26, 41, 39, 25, 33, 35, 49, 31, 40, 32, 42, 33, 48, 28, 43, 30, 41, 55, 55, 54, 55, 57, 27, 28, 51, 27, 40, 56, 35, 47, 49, 35, 50, 55, 56, 35, 42, 34, 40, 33, 33, 59, 51, 47, 48, 31, 39, 52, 28, 56, 47, 77, 39, 31, 39, 32, 59, 42, 50, 44, 47, 51, 34, 36, 32, 28, 34, 40, 49, 30, 56, 28, 31, 28, 53, 38, 54, 36, 60, 40, 50, 28, 33, 53, 38, 34, 41, 64, 45, 38, 57, 34, 36, 30, 32, 42, 35, 42, 50, 38, 38, 55, 60, 33, 30, 30, 51, 37, 37, 45, 37, 48, 42, 49, 36, 54, 52, 30, 37, 31, 26, 51, 34, 26, 32, 38, 52, 44, 31, 38, 32, 41, 48, 29, 41, 49, 40, 27, 45, 48, 31, 45, 43, 31, 37, 38, 29, 28, 55, 27, 43, 29, 52, 33, 27, 27, 31, 31, 45, 34, 48, 47, 49, 55, 51, 37, 33, 33, 29, 57, 60, 51, 45, 45, 39, 25, 47, 34, 40, 48, 39, 38, 32, 27, 47, 40, 41, 40, 39, 45, 31, 27, 56, 39, 52, 58, 48, 33, 38, 42, 36, 35, 45, 51, 43, 45, 30, 40, 56, 48, 32, 47, 30, 37, 52, 49, 53, 53, 34, 40, 39, 33, 41, 31, 45, 23, 51, 34, 36, 42, 33, 46, 37, 33, 30, 53, 46, 35, 36, 46, 31, 52, 48, 45, 36, 53, 35, 44, 38, 44, 51, 32, 59, 31, 52, 26, 40, 35, 54, 36, 45, 29, 49, 52, 52, 47, 33, 30, 52, 54, 59, 35, 29, 33, 32, 50, 54, 49, 39, 34, 31, 46, 59, 32, 39, 54, 33, 59, 28, 48, 39, 41, 33, 37, 30, 36, 53, 26, 59, 44, 55, 26, 41, 28, 40, 47, 39, 38, 37, 36, 53, 47, 28, 32, 25, 39, 60, 39, 31, 45, 36, 44, 33, 31, 38, 50, 40, 33, 42, 42, 33, 40, 52, 49, 30, 26, 40, 30, 55, 48, 28, 54, 36, 34, 39, 66, 42, 55, 85, 63, 37, 24, 34, 28, 58, 49, 31, 56, 43, 32, 55, 55, 31, 49, 41, 56, 50, 37, 35, 52, 33, 42, 33, 35, 28, 41, 44, 26, 40, 37, 33, 36, 28, 37, 32, 49, 36, 39, 34, 32, 26, 46, 78, 47, 80, 25, 50, 36, 42, 54, 43, 29, 36, 59, 47, 36, 56, 55, 38, 39, 35, 54, 32, 30, 51, 39, 46, 33, 34, 33, 55, 49, 37, 29, 28, 21, 44, 30, 46, 33, 40, 42, 44, 32, 34, 55, 35, 47, 29, 47, 39, 40, 42, 31, 31, 82, 44, 32, 40, 26, 30, 45, 32, 56, 58, 28, 43, 34, 40, 31, 28, 45, 33, 42, 32, 41, 32, 37, 57, 56, 37, 34, 46, 40, 32, 28, 46, 44, 55, 75, 41, 30, 33, 43, 49, 32, 26, 55, 46, 31, 34, 37, 29, 24, 46, 41, 52, 27, 44, 39, 52, 77, 35, 49, 37, 46, 43, 35, 29, 52, 55, 33, 40, 39, 61, 63, 44, 52, 58, 38, 51, 29, 42, 56, 55, 43, 27, 54, 25, 33, 46, 26, 30, 39, 58, 49, 49, 43, 60, 57, 38, 56, 45, 45, 44, 40, 38, 55, 40, 35, 32, 31, 49, 55, 51, 30, 37, 36, 51, 29, 32, 36, 32, 56, 46, 39, 57, 53, 33, 43, 40, 44, 59, 59, 28, 26, 59, 78, 33, 38, 35, 30, 31, 35, 45, 40, 50, 29, 41, 44, 29, 49, 30, 37, 52, 30, 40, 33, 34, 33, 35, 54, 32, 32, 44, 53, 53, 40, 29, 36, 40, 58, 34, 60, 23, 59, 42, 50, 31, 31, 28, 56, 23, 30, 54, 35, 29, 48, 43, 40, 38, 30, 60, 31, 28, 31, 48, 36, 41, 35, 51, 35, 51, 56, 59, 27, 48, 24, 56, 34, 35, 57, 30, 40, 48, 35, 46, 30, 37, 30, 46, 43, 40, 34, 43, 31, 37, 49, 38, 27, 35, 47, 56, 26, 29, 53, 38, 33, 30, 36, 37, 60, 48, 32, 31, 54, 31, 31, 29, 23, 51, 32, 48, 32, 44, 35, 46, 31, 31, 36, 35, 35, 36, 52, 27, 47, 43, 41, 18, 45, 41, 34, 46, 38, 23, 73, 41, 43, 51, 45, 47, 35, 38, 31, 49, 29, 39, 41, 34, 39, 78, 33, 59, 52, 35, 43, 43, 36, 36, 45, 47, 38, 45, 33, 38, 36, 38, 34, 32, 49, 37, 47, 28, 29, 30, 34, 39, 26, 38, 47, 61, 35, 34, 34, 56, 47, 36, 43, 51, 40, 38, 34, 47, 57, 35, 45, 55, 54, 39, 58, 31, 35, 45, 44, 57, 41, 42, 49, 52, 57, 29, 34, 36, 39, 53, 40, 38, 34, 40, 39, 51, 33, 59, 30, 61, 24, 36, 34, 47, 40, 46, 32, 39, 35, 37, 45, 38, 52, 28, 35, 45, 33, 42, 23, 49, 52, 27, 34, 31, 45, 32, 51, 56, 44, 41, 54, 31, 25, 50, 41, 33, 49, 34, 60, 45, 40, 50, 27, 53, 31, 24, 68, 27, 46, 23, 76, 48, 60, 46, 36, 30, 33, 36, 44, 36, 44, 70, 48, 52, 34, 35, 54, 47, 38, 41, 28, 58, 59, 32, 55, 38, 40, 45, 42, 53, 32, 55, 54, 46, 39, 36, 44, 28, 36, 43, 25, 31, 60, 29, 46, 31, 31, 45, 60, 50, 34, 33, 57, 38, 42, 54, 46, 59, 38, 49, 49, 30, 52, 38, 44, 39, 38, 36, 32, 57, 45, 43, 42, 58, 53, 30, 40, 35, 34, 35, 28, 39, 51, 29, 44, 32, 28, 53, 81, 33, 37, 37, 35, 30, 43, 45, 34, 40, 60, 54, 39, 44, 32, 34, 44, 30, 47, 74, 27, 34, 52, 32, 42, 31, 31, 31, 57, 40, 23, 43, 45, 58, 38, 48, 47, 49, 28, 31, 38, 43, 52, 43, 29, 45, 59, 48, 30, 23, 38, 52, 44, 34, 32, 41, 31, 40, 52, 32, 26, 29, 31, 48, 27, 30, 28, 35, 47, 35, 31, 53, 26, 35, 29, 39, 31, 48, 36, 27, 49, 34, 28, 49, 35, 53, 58, 37, 27, 30, 35, 32, 53, 34, 38, 34, 40, 48, 30, 31, 37, 32, 40, 41, 52, 29, 49, 43, 42, 37, 57, 25, 30, 31, 49, 47, 57, 26, 40, 34, 41, 53, 26, 28, 54, 42, 29, 49, 32, 29, 60, 51, 40, 44, 47, 32, 43, 59, 24, 26, 37, 27, 32, 44, 34, 39, 30, 44, 22, 28, 41, 30, 36, 39, 38, 31, 29, 42, 27, 55, 28, 47, 49, 38, 53, 31, 57, 36, 38, 30, 36, 29, 32, 48, 53, 35, 48, 31, 30, 28, 42, 31, 36, 43, 28, 34, 36, 28, 34, 53, 52, 44, 58, 47, 40, 39, 33, 23, 31, 39, 42, 46, 22, 50, 50, 34, 40, 39, 51, 45, 34, 30, 57, 42, 51, 38, 60, 38, 37, 34, 39, 55, 48, 42, 29, 33, 35, 28, 26, 40, 42, 49, 32, 33, 30, 20, 51, 38, 53, 34, 30, 36, 36, 39, 33, 42, 30, 23, 37, 77, 33, 33, 40, 58, 29, 43, 34, 38, 38, 40, 59, 44, 60, 37, 50, 50, 35, 39, 29, 34, 31, 51, 46, 27, 61, 34, 30, 58, 30, 50, 30, 43, 35, 36, 46, 46, 29, 48, 50, 53, 38, 27, 36, 48, 59, 21, 56, 57, 41, 36, 45, 40, 30, 75, 35, 58, 29, 54, 28, 33, 56, 50, 37, 49, 54, 40, 33, 48, 43, 35, 55, 33, 41, 57, 50, 63, 33, 47, 36, 23, 33, 36, 47, 39, 44, 60, 42, 32, 33, 37, 33, 59, 42, 59, 31, 50, 29, 36, 33, 38, 33, 36, 36, 44, 29, 44, 55, 38, 39, 45, 36, 36, 59, 41, 29, 50, 51, 29, 40, 36, 49, 45, 40, 31, 32, 44, 27, 25, 56, 59, 31, 49, 50, 42, 37, 36, 53, 51, 42, 47, 40, 25, 36, 80, 60, 51, 59, 80, 47, 36, 31, 33, 45, 33, 51, 51, 48, 34, 28, 33, 36, 40, 34, 31, 49, 35, 38, 32, 33, 39, 30, 53, 24, 52, 27, 36, 29, 47, 43, 48, 57, 25, 43, 50, 54, 38, 41, 35, 33, 51, 29, 34, 51, 58, 50, 38, 57, 49, 42, 33, 34, 36, 60, 37, 41, 36, 56, 34, 33, 29, 43, 29, 36, 33, 39, 26, 41, 36, 31, 34, 28, 47, 41, 39, 53, 45, 30, 39, 30, 32, 66, 26, 30, 42, 44, 38, 31, 34, 42, 34, 31, 30, 31, 47, 52, 31, 51, 48, 47, 55, 58, 74, 30, 40, 50, 36, 26, 30, 34, 53, 42, 33, 35, 33, 41, 40, 32, 54, 31, 35, 47, 48, 50, 33, 40, 60, 68, 54, 30, 42, 57, 28, 55, 42, 49, 41, 31, 33, 55, 34, 38, 46, 49, 44, 57, 44, 42, 37, 42, 33, 31, 34, 26, 42, 41, 35, 26, 42, 36, 60, 51, 32, 46, 31, 42, 45, 31, 41, 28, 41, 36, 41, 40, 40, 58, 32, 30, 43, 33, 30, 37, 40, 39, 39, 26, 35, 59, 31, 36, 26, 36, 59, 42, 35, 59, 44, 34, 36, 43, 45, 29, 29, 40, 31, 29, 37, 35, 35, 39, 28, 31, 34, 31, 51, 53, 34, 42, 33, 29, 25, 42, 46, 52, 60, 25, 55, 37, 49, 42, 32, 43, 26, 41, 30, 42, 34, 20, 45, 58, 51, 55, 31, 42, 48, 62, 40, 37, 39, 34, 52, 35, 56, 37, 35, 31, 42, 27, 33, 31, 53, 56, 80, 63, 37, 43, 62, 25, 51, 39, 47, 39, 32, 30, 34, 41, 44, 43, 30, 46, 36, 56, 44, 37, 36, 24, 49, 69, 47, 29, 31, 42, 30, 26, 37, 28, 36, 46, 58, 46, 47, 50, 47, 26, 42, 34, 46, 51, 52, 38, 44, 52, 32, 35, 36, 37, 26, 32, 46, 50, 43, 55, 35, 45, 48, 31, 46, 50, 53, 36, 50, 40, 39, 38, 32, 31, 41, 41, 58, 36, 54, 32, 54, 46, 31, 34, 46, 45, 30, 38, 31, 30, 43, 26, 32, 33, 40, 47, 34, 39, 49, 41, 45, 45, 54, 34, 44, 56, 60, 56, 60, 57, 29, 39, 33, 61, 58, 27, 37, 34, 35, 35, 49, 34, 34, 48, 55, 47, 52, 30, 54, 40, 43, 53, 25, 51, 45, 42, 35, 37, 43, 42, 58, 43, 29, 39, 59, 36, 44, 34, 49, 26, 51, 37, 33, 36, 34, 45, 38, 40, 54, 34, 51, 46, 52, 38, 26, 39, 54, 44, 52, 43, 45, 38, 59, 40, 34, 41, 36, 40, 39, 34, 31, 32, 53, 28, 38, 50, 36, 23, 51, 51, 35, 32, 60, 34, 58, 58, 34, 38, 34, 31, 46, 36, 35, 42, 28, 39, 39, 39, 43, 69, 43, 36, 38, 38, 37, 37, 31, 31, 32, 42, 46, 35, 31, 60, 55, 45, 33, 48, 36, 37, 37, 54, 40, 44, 46, 37, 57, 43, 33, 38, 31, 44, 28, 31, 42, 41, 20, 29, 33, 36, 42, 57, 35, 33, 37, 55, 28, 38, 38, 60, 55, 48, 30, 31, 25, 37, 26, 45, 76, 32, 52, 35, 54, 33, 44, 34, 38, 33, 32, 36, 77, 26, 33, 45, 55, 55, 35, 28, 55, 37, 35, 33, 35, 30, 35, 49, 26, 26, 31, 38, 54, 50, 37, 29, 35, 36, 57, 72, 27, 48, 30, 27, 57, 38, 47, 57, 43, 43, 32, 37, 49, 32, 40, 43, 46, 49, 37, 40, 56, 30, 40, 29, 37, 38, 28, 40, 58, 41, 55, 40, 40, 27, 52, 31, 30, 59, 47, 57, 43, 38, 27, 46, 42, 51, 42, 42, 38, 43, 34, 28, 43, 55, 32, 49, 48, 30, 43, 42, 30, 31, 33, 46, 38, 53, 45, 56, 33, 28, 39, 38, 32, 52, 29, 29, 33, 46, 36, 35, 39, 56, 42, 44, 50, 46, 50, 48, 39, 26, 32, 36, 56, 30, 39, 33, 43, 32, 27, 46, 42, 43, 59, 44, 31, 26, 51, 24, 26, 54, 31, 33, 42, 33, 40, 46, 33, 56, 38, 35, 32, 51, 33, 31, 31, 49, 43, 40, 27, 33, 47, 59, 52, 35, 32, 49, 54, 30, 33, 50, 40, 38, 34, 33, 41, 30, 45, 28, 35, 31, 41, 32, 34, 26, 29, 49, 30, 44, 42, 35, 39, 34, 58, 35, 32, 31, 41, 39, 39, 38, 46, 54, 34, 40, 40, 30, 37, 34, 66, 30, 59, 56, 25, 36, 33, 30, 29, 52, 54, 28, 58, 23, 49, 47, 73, 44, 29, 48, 35, 41, 40, 57, 43, 31, 51, 52, 31, 26, 36, 43, 32, 26, 25, 34, 30, 35, 42, 53, 52, 35, 29, 43, 81, 35, 36, 76, 44, 38, 37, 41, 49, 53, 34, 30, 43, 36, 60, 45, 49, 39, 57, 49, 39, 37, 45, 36, 58, 48, 52, 32, 51, 31, 30, 34, 53, 32, 52, 50, 30, 34, 51, 34, 26, 51, 31, 29, 29, 46, 35, 44, 34, 27, 28, 59, 38, 34, 44, 35, 45, 34, 57, 76, 30, 48, 43, 34, 30, 33, 38, 52, 34, 40, 36, 56, 29, 34, 31, 36, 30, 36, 25, 32, 31, 49, 37, 26, 41, 47, 57, 50, 36, 42, 30, 43, 49, 35, 32, 36, 42, 34, 33, 55, 46, 34, 50, 30, 53, 48, 55, 35, 48, 45, 31, 55, 41, 33, 40, 51, 31, 26, 39, 30, 28, 48, 56, 45, 38, 28, 51, 32, 31, 34, 46, 36, 39, 42, 32, 35, 50, 55, 51, 43, 46, 31, 38, 40, 53, 41, 46, 48, 31, 29, 37, 37, 56, 60, 46, 31, 51, 38, 39, 30, 35, 31, 27, 42, 67, 32, 49, 56, 34, 50, 27, 56, 44, 45, 45, 37, 26, 32, 25, 48, 29, 46, 34, 46, 31, 39, 29, 38, 40, 39, 49, 34, 30, 57, 34, 21, 35, 39, 42, 25, 27, 33, 51, 32, 53, 29, 33, 33, 32, 31, 45, 57, 47, 52, 56, 51, 40, 30, 39, 42, 25, 60, 37, 48, 32, 33, 51, 43, 31, 36, 58, 33, 31, 28, 29, 34, 31, 31, 37, 32, 42, 34, 74, 60, 32, 47, 45, 50, 39, 60, 47, 40, 26, 43, 30, 34, 35, 21, 52, 31, 42, 31, 58, 51, 38, 32, 31, 36, 33, 47, 52, 42, 39, 31, 52, 40, 29, 49, 57, 35, 26, 35, 31, 57, 32, 87, 44, 41, 36, 56, 35, 46, 32, 41, 34, 48, 31, 41, 36, 31, 37, 48, 31, 42, 50, 47, 43, 59, 35, 24, 49, 47, 57, 29, 50, 32, 23, 41, 48, 29, 42, 38, 31, 38, 50, 36, 52, 50, 38, 30, 46, 54, 46, 49, 36, 58, 29, 41, 32, 33, 31, 44, 51, 38, 38, 27, 41, 32, 45, 54, 36, 33, 40, 37, 48, 36, 46, 29, 41, 44, 40, 33, 60, 48, 37, 39, 28, 44, 37, 58, 50, 38, 31, 33, 35, 53, 58, 26, 34, 50, 42, 46, 45, 46, 31, 44, 36, 50, 33, 33, 55, 40, 30, 36, 34, 46, 55, 56, 34, 42, 48, 51, 45, 35, 63, 35, 25, 26, 29, 26, 26, 23, 51, 51, 43, 81, 42, 41, 35, 32, 51, 33, 38, 48, 50, 39, 30, 39, 35, 41, 41, 35, 43, 27, 32, 39, 37, 40, 33, 29, 50, 39, 32, 49, 37, 57, 44, 36, 55, 40, 40, 44, 41, 65, 31, 48, 55, 34, 36, 33, 46, 52, 33, 29, 36, 37, 53, 28, 19, 41, 33, 28, 40, 78, 29, 36, 59, 34, 82, 36, 48, 36, 25, 40, 33, 30, 27, 34, 28, 40, 47, 59, 30, 39, 44, 46, 32, 35, 35, 38, 27, 32, 37, 59, 32, 31, 55, 26, 30, 34, 83, 46, 48, 47, 31, 32, 32, 39, 54, 55, 28, 48, 44, 47, 43, 53, 42, 39, 37, 58, 31, 40, 38, 46, 31, 32, 46, 25, 28, 40, 31, 39, 36, 37, 48, 34, 24, 33, 54, 56, 41, 46, 27, 32, 40, 38, 36, 33, 31, 34, 34, 45, 52, 37, 30, 35, 31, 47, 32, 32, 39, 31, 31, 44, 40, 77, 34, 48, 31, 31, 32, 42, 35, 57, 56, 34, 52, 31, 42, 46, 32, 47, 46, 41, 42, 31, 47, 60, 35, 46, 40, 48, 52, 50, 34, 34, 44, 44, 28, 28, 29, 29, 55, 31, 31, 32, 31, 47, 34, 53, 29, 50, 49, 32, 42, 37, 36, 33, 36, 30, 57, 37, 25, 70, 41, 26, 53, 55, 23, 45, 34, 35, 30, 36, 48, 67, 35, 38, 44, 54, 42, 36, 26, 35, 33, 58, 21, 34, 38, 41, 36, 38, 25, 34, 34, 45, 30, 42, 50, 33, 35, 59, 38, 33, 45, 37, 35, 41, 33, 30, 49, 39, 44, 38, 27, 45, 40, 53, 32, 34, 29, 34, 40, 44, 36, 39, 58, 37, 31, 46, 26, 37, 39, 36, 29, 47, 47, 44, 24, 30, 55, 41, 57, 34, 31, 56, 53, 33, 67, 53, 41, 46, 58, 39, 30, 37, 45, 50, 30, 29, 32, 37, 26, 51, 38, 72, 42, 57, 22, 46, 32, 42, 37, 27, 30, 34, 56, 54, 57, 56, 41, 32, 34, 30, 42, 40, 36, 42, 34, 30, 35, 44, 46, 38, 32, 34, 53, 38, 54, 32, 33, 46, 49, 37, 40, 35, 26, 27, 39, 31, 53, 50, 36, 46, 36, 38, 44, 56, 37, 51, 41, 55, 38, 21, 40, 39, 30, 28, 58, 50, 32, 44, 47, 45, 48, 60, 45, 31, 30, 52, 31, 53, 47, 42, 57, 35, 47, 37, 55, 58, 33, 29, 41, 53, 41, 42, 52, 42, 33, 44, 53, 35, 47, 59, 31, 40, 55, 41, 46, 35, 46, 31, 49, 37, 41, 42, 34, 55, 49, 59, 26, 43, 35, 43, 51, 29, 42, 73, 39, 39, 44, 30, 32, 57, 55, 24, 41, 39, 52, 28, 56, 33, 41, 32, 50, 28, 36, 36, 53, 40, 34, 35, 55, 49, 57, 44, 57, 36, 53, 25, 26, 33, 33, 29, 34, 46, 25, 57, 50, 48, 32, 45, 32, 34, 56, 49, 54, 44, 35, 31, 34, 38, 56, 49, 44, 33, 33, 29, 34, 57, 34, 55, 47, 48, 36, 34, 43, 55, 58, 62, 32, 35, 33, 54, 41, 58, 34, 58, 30, 40, 35, 37, 32, 33, 48, 46, 35, 32, 30, 46, 27, 40, 60, 28, 27, 31, 28, 27, 36, 41, 30, 52, 35, 37, 37, 48, 38, 44, 43, 35, 40, 42, 51, 46, 40, 51]}],                        {\"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"age distribution in customer attrition \"}, \"xaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"age\"}, \"zerolinewidth\": 1}, \"yaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"percent\"}, \"zerolinewidth\": 1}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a0a25a5c-495c-4004-b98f-adae4033784c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Active",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        },
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Churn",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "default distribution in customer attrition "
        },
        "xaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "default"
         },
         "zerolinewidth": 1
        },
        "yaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "percent"
         },
         "zerolinewidth": 1
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"41cb922d-3e91-429d-9416-769d44490847\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"41cb922d-3e91-429d-9416-769d44490847\")) {                    Plotly.newPlot(                        \"41cb922d-3e91-429d-9416-769d44490847\",                        [{\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Active\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Churn\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}],                        {\"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"default distribution in customer attrition \"}, \"xaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"default\"}, \"zerolinewidth\": 1}, \"yaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"percent\"}, \"zerolinewidth\": 1}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('41cb922d-3e91-429d-9416-769d44490847');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Active",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          931,
          95,
          0,
          4367,
          68,
          1308,
          353,
          -40,
          0,
          -312,
          549,
          8837,
          1020,
          3540,
          663,
          2590,
          4982,
          6,
          118,
          -258,
          -221,
          2596,
          467,
          3100,
          722,
          797,
          476,
          558,
          162,
          1842,
          2348,
          11,
          2103,
          967,
          116,
          2805,
          1386,
          0,
          0,
          90,
          37,
          7,
          676,
          3817,
          144,
          230,
          0,
          1109,
          2925,
          397,
          865,
          816,
          396,
          0,
          241,
          243,
          233,
          43,
          290,
          101,
          1151,
          45,
          1410,
          1234,
          2767,
          5969,
          -272,
          1036,
          9324,
          1043,
          1473,
          183,
          86,
          0,
          2388,
          160,
          385,
          4,
          119,
          0,
          742,
          0,
          1724,
          0,
          503,
          68,
          5914,
          192,
          15,
          2157,
          3494,
          275,
          318,
          350,
          588,
          2008,
          35,
          2152,
          122,
          160,
          1934,
          0,
          613,
          574,
          0,
          341,
          158,
          1489,
          2911,
          1178,
          96,
          4157,
          -575,
          3419,
          1104,
          57,
          343,
          -326,
          0,
          3104,
          648,
          10041,
          -508,
          327,
          1150,
          1074,
          27,
          3720,
          92,
          7845,
          157,
          428,
          0,
          54,
          13,
          3230,
          1637,
          6981,
          1,
          631,
          7296,
          3014,
          920,
          735,
          1300,
          541,
          0,
          1257,
          4522,
          585,
          150,
          202,
          2137,
          2674,
          625,
          73,
          2656,
          3,
          0,
          95,
          106,
          477,
          1529,
          28,
          1743,
          924,
          457,
          0,
          736,
          0,
          787,
          -396,
          1573,
          0,
          524,
          1558,
          159,
          53,
          838,
          3144,
          737,
          4056,
          162,
          0,
          217,
          3324,
          2895,
          0,
          11254,
          -280,
          808,
          0,
          246,
          -972,
          750,
          2430,
          -179,
          1412,
          -1531,
          51,
          88,
          -209,
          799,
          4048,
          1310,
          573,
          6798,
          1214,
          75,
          387,
          2246,
          3,
          883,
          1503,
          -322,
          1301,
          950,
          2326,
          -78,
          2405,
          113,
          4564,
          387,
          1865,
          4105,
          258,
          19,
          3773,
          1808,
          1304,
          3702,
          163,
          -151,
          1602,
          1314,
          84,
          1906,
          291,
          223,
          207,
          518,
          241,
          -126,
          421,
          38,
          800,
          32,
          1477,
          160,
          4415,
          -397,
          590,
          778,
          19,
          0,
          -28,
          3119,
          122,
          0,
          4157,
          102,
          437,
          1,
          -354,
          1234,
          901,
          247,
          180,
          274,
          92,
          813,
          1170,
          -183,
          0,
          136,
          103,
          23,
          2242,
          137,
          1047,
          5731,
          2707,
          -329,
          158,
          219,
          2775,
          461,
          145,
          309,
          733,
          8417,
          1400,
          72,
          0,
          9710,
          1377,
          881,
          0,
          285,
          550,
          285,
          -360,
          0,
          72,
          930,
          717,
          67,
          6227,
          5252,
          561,
          1393,
          -330,
          3261,
          361,
          807,
          2305,
          -11,
          2223,
          29,
          1308,
          1337,
          17361,
          6,
          370,
          8,
          666,
          0,
          1336,
          17672,
          35,
          705,
          2087,
          4466,
          844,
          321,
          2781,
          34,
          29,
          2443,
          3975,
          256,
          696,
          63,
          316,
          -119,
          6570,
          418,
          0,
          8784,
          1970,
          204,
          643,
          304,
          1177,
          544,
          -498,
          0,
          209,
          665,
          310,
          2149,
          13818,
          526,
          -218,
          419,
          517,
          61,
          0,
          514,
          365,
          480,
          717,
          24780,
          1396,
          414,
          3100,
          0,
          1218,
          789,
          859,
          0,
          3868,
          232,
          1686,
          29184,
          144,
          194,
          7641,
          351,
          150,
          1532,
          1549,
          1057,
          839,
          880,
          4464,
          6,
          0,
          694,
          153,
          110,
          640,
          3060,
          2836,
          1127,
          11,
          982,
          222,
          627,
          5010,
          468,
          155,
          2478,
          668,
          35,
          3304,
          0,
          237,
          0,
          -54,
          0,
          240,
          759,
          992,
          1379,
          8564,
          0,
          119,
          450,
          746,
          2032,
          636,
          -938,
          2007,
          33,
          2887,
          1444,
          42,
          522,
          0,
          3884,
          472,
          2060,
          628,
          398,
          417,
          610,
          2585,
          321,
          0,
          4,
          29,
          892,
          0,
          1387,
          454,
          71,
          -35,
          2251,
          2460,
          34,
          1281,
          3021,
          141,
          500,
          1459,
          -244,
          1356,
          -289,
          1396,
          9,
          70,
          877,
          113,
          368,
          89,
          450,
          463,
          1397,
          310,
          270,
          65,
          865,
          7,
          145,
          327,
          429,
          1167,
          2995,
          974,
          784,
          316,
          0,
          211,
          342,
          628,
          0,
          769,
          2278,
          1351,
          49,
          1082,
          5691,
          916,
          325,
          0,
          2475,
          3717,
          288,
          1130,
          2715,
          23,
          0,
          -84,
          290,
          36,
          -488,
          -473,
          1272,
          0,
          -6,
          510,
          818,
          543,
          7867,
          1212,
          533,
          291,
          573,
          1279,
          280,
          574,
          658,
          2019,
          22815,
          887,
          303,
          326,
          453,
          0,
          4063,
          2403,
          123,
          270,
          424,
          1230,
          387,
          744,
          696,
          4594,
          1002,
          -173,
          1239,
          354,
          375,
          3,
          6116,
          4515,
          727,
          3846,
          1128,
          1173,
          27,
          328,
          0,
          -87,
          2830,
          4,
          0,
          80,
          2768,
          5,
          553,
          -102,
          2079,
          1,
          598,
          34,
          -19,
          292,
          144,
          1113,
          121,
          904,
          -498,
          0,
          67,
          3967,
          305,
          260,
          258,
          0,
          72,
          145,
          2564,
          7811,
          5744,
          673,
          602,
          9374,
          5016,
          508,
          156,
          0,
          12,
          7984,
          255,
          211,
          1854,
          -497,
          431,
          -314,
          552,
          465,
          544,
          -636,
          7780,
          274,
          1148,
          67,
          914,
          1293,
          111,
          937,
          605,
          1457,
          1617,
          29207,
          812,
          324,
          1327,
          482,
          134,
          347,
          1824,
          410,
          -38,
          581,
          3100,
          0,
          1138,
          0,
          304,
          313,
          1898,
          142,
          4696,
          135,
          574,
          1322,
          703,
          0,
          1,
          55,
          786,
          22,
          890,
          451,
          59,
          15,
          15341,
          -273,
          28,
          58,
          103,
          244,
          0,
          880,
          -141,
          447,
          62,
          148,
          79,
          9,
          578,
          -382,
          888,
          382,
          1390,
          -104,
          22,
          -36,
          130,
          77,
          1031,
          92,
          1063,
          0,
          282,
          1394,
          16,
          975,
          12276,
          313,
          1648,
          6,
          1127,
          1070,
          1040,
          50,
          62,
          -755,
          4707,
          925,
          0,
          867,
          -201,
          120,
          61,
          0,
          687,
          38,
          2806,
          -27,
          2498,
          390,
          -423,
          1587,
          288,
          822,
          -222,
          197,
          159,
          681,
          0,
          495,
          414,
          151,
          2572,
          320,
          304,
          1299,
          740,
          537,
          1581,
          708,
          178,
          671,
          3,
          981,
          1042,
          1021,
          1035,
          1114,
          98,
          24,
          486,
          727,
          -205,
          895,
          319,
          206,
          -414,
          -1489,
          0,
          249,
          927,
          515,
          1060,
          308,
          808,
          431,
          396,
          -89,
          -253,
          0,
          60,
          695,
          1141,
          791,
          1381,
          0,
          -28,
          412,
          310,
          1319,
          0,
          -314,
          511,
          -152,
          907,
          22,
          0,
          8044,
          1854,
          1794,
          2,
          2321,
          144,
          1,
          762,
          16,
          20,
          2070,
          308,
          1617,
          39,
          332,
          713,
          2885,
          968,
          4150,
          687,
          333,
          1821,
          -436,
          6929,
          126,
          2019,
          878,
          6791,
          0,
          110,
          28,
          443,
          236,
          2707,
          1753,
          1182,
          427,
          788,
          0,
          -509,
          -110,
          244,
          933,
          940,
          1631,
          -505,
          7336,
          -40,
          25,
          0,
          5,
          -92,
          497,
          2356,
          3,
          128,
          390,
          300,
          827,
          25,
          340,
          867,
          140,
          3,
          4920,
          3237,
          0,
          1096,
          869,
          1900,
          413,
          -588,
          0,
          2490,
          20928,
          560,
          0,
          826,
          115,
          107,
          320,
          -399,
          703,
          328,
          148,
          2298,
          64,
          4013,
          0,
          633,
          -1137,
          902,
          2850,
          313,
          923,
          8,
          945,
          2261,
          18,
          5828,
          6307,
          537,
          393,
          215,
          132,
          35,
          125,
          129,
          0,
          315,
          2567,
          883,
          1120,
          0,
          1117,
          773,
          390,
          1358,
          216,
          317,
          2369,
          125,
          6,
          15841,
          51,
          0,
          -79,
          1881,
          456,
          1873,
          0,
          445,
          2123,
          767,
          229,
          3783,
          140,
          6,
          859,
          261,
          50,
          -462,
          0,
          283,
          91,
          0,
          442,
          0,
          1182,
          316,
          316,
          402,
          8725,
          102,
          0,
          615,
          0,
          3060,
          990,
          1937,
          428,
          655,
          16,
          2044,
          20,
          29,
          -155,
          340,
          1776,
          23,
          863,
          7051,
          1049,
          174,
          137,
          783,
          116,
          2118,
          10269,
          141,
          13,
          0,
          1996,
          1129,
          561,
          539,
          -349,
          6971,
          497,
          362,
          4497,
          2496,
          2850,
          0,
          1199,
          66,
          3756,
          1170,
          944,
          50,
          171,
          122,
          1780,
          136,
          179,
          232,
          82,
          630,
          1202,
          867,
          6200,
          -422,
          426,
          107,
          -96,
          581,
          402,
          555,
          67,
          72,
          1942,
          373,
          84,
          394,
          925,
          0,
          6690,
          4178,
          20723,
          887,
          598,
          1,
          4314,
          2155,
          760,
          1248,
          5122,
          191,
          230,
          7010,
          1970,
          3935,
          437,
          2323,
          2213,
          84,
          4545,
          200,
          206,
          2937,
          221,
          1521,
          102,
          519,
          660,
          500,
          1661,
          961,
          -273,
          7067,
          285,
          221,
          479,
          1595,
          65,
          640,
          490,
          1440,
          918,
          6507,
          4,
          2289,
          131,
          1852,
          -749,
          719,
          4613,
          664,
          291,
          24,
          823,
          1235,
          1195,
          21,
          363,
          0,
          0,
          136,
          846,
          0,
          505,
          3986,
          11862,
          145,
          2037,
          441,
          105,
          465,
          320,
          -762,
          316,
          903,
          123,
          273,
          1345,
          1027,
          1855,
          1315,
          59,
          2682,
          2682,
          3164,
          342,
          1742,
          -106,
          67,
          430,
          -234,
          0,
          -206,
          656,
          603,
          1102,
          117,
          -13,
          1993,
          5920,
          881,
          0,
          953,
          781,
          1530,
          2667,
          5310,
          1468,
          0,
          144,
          247,
          258,
          595,
          259,
          2408,
          1356,
          292,
          2384,
          307,
          292,
          3131,
          0,
          2258,
          1348,
          741,
          0,
          741,
          521,
          5315,
          1660,
          163,
          1163,
          122,
          1,
          493,
          0,
          715,
          558,
          16992,
          179,
          198,
          152,
          2805,
          6332,
          2420,
          580,
          259,
          -451,
          261,
          323,
          0,
          235,
          526,
          4366,
          0,
          833,
          -13,
          405,
          3517,
          699,
          943,
          794,
          503,
          -700,
          644,
          335,
          719,
          -195,
          972,
          5050,
          232,
          2406,
          324,
          168,
          374,
          658,
          -1,
          280,
          0,
          0,
          2064,
          128,
          0,
          863,
          50,
          187,
          -614,
          841,
          206,
          984,
          201,
          585,
          8514,
          728,
          80,
          1246,
          818,
          1815,
          0,
          210,
          71,
          1869,
          1040,
          920,
          668,
          36,
          -972,
          382,
          -170,
          5172,
          1,
          922,
          3016,
          0,
          1101,
          3578,
          0,
          0,
          -190,
          185,
          156,
          503,
          485,
          392,
          625,
          468,
          280,
          181,
          1401,
          1349,
          5737,
          1187,
          5231,
          3185,
          3228,
          80,
          2800,
          10,
          331,
          223,
          0,
          35,
          299,
          240,
          3534,
          223,
          -194,
          1545,
          440,
          2538,
          337,
          -492,
          53,
          91,
          316,
          3196,
          2697,
          3,
          244,
          410,
          159,
          6432,
          -701,
          24,
          296,
          1,
          1965,
          501,
          3558,
          177,
          204,
          46,
          2047,
          810,
          1914,
          56831,
          386,
          469,
          69,
          -124,
          600,
          3842,
          -720,
          1127,
          423,
          -59,
          0,
          1489,
          0,
          1451,
          452,
          283,
          16,
          973,
          1257,
          -1701,
          567,
          -1,
          3310,
          115,
          321,
          71,
          759,
          23,
          3297,
          578,
          4279,
          234,
          1137,
          8669,
          488,
          1077,
          4790,
          256,
          974,
          231,
          196,
          2002,
          218,
          1011,
          2761,
          92,
          10005,
          1168,
          168,
          1710,
          207,
          18,
          306,
          8,
          -250,
          449,
          1654,
          714,
          1498,
          695,
          -15,
          14,
          -287,
          0,
          -978,
          0,
          20,
          700,
          -58,
          841,
          951,
          -63,
          641,
          195,
          11,
          120,
          450,
          8379,
          206,
          1544,
          2990,
          866,
          1744,
          -382,
          214,
          147,
          81,
          202,
          1314,
          -60,
          0,
          -331,
          1504,
          2363,
          773,
          941,
          2116,
          -971,
          5,
          454,
          53,
          2578,
          854,
          16397,
          687,
          1894,
          0,
          1150,
          136,
          7313,
          -239,
          182,
          1590,
          -565,
          930,
          1091,
          640,
          767,
          480,
          1363,
          446,
          167,
          0,
          4196,
          725,
          1716,
          -45,
          410,
          -97,
          12159,
          395,
          969,
          86,
          395,
          0,
          0,
          829,
          -15,
          135,
          1585,
          -910,
          8806,
          43,
          136,
          1195,
          11219,
          2003,
          494,
          419,
          -82,
          64,
          14522,
          3455,
          1076,
          -261,
          309,
          655,
          102,
          213,
          202,
          0,
          22,
          179,
          4769,
          -6,
          19358,
          5563,
          6641,
          747,
          49,
          287,
          437,
          1313,
          597,
          418,
          -41,
          449,
          3572,
          2428,
          177,
          80,
          1048,
          3874,
          5381,
          17,
          4040,
          427,
          385,
          266,
          1411,
          7747,
          2129,
          318,
          105,
          2603,
          443,
          -116,
          285,
          5092,
          719,
          0,
          2095,
          -390,
          108,
          1134,
          228,
          1,
          0,
          1453,
          28,
          1388,
          0,
          0,
          15,
          22,
          983,
          2879,
          -553,
          596,
          1402,
          223,
          46,
          0,
          1636,
          96,
          -295,
          1,
          60,
          84,
          4287,
          1,
          192,
          1172,
          105,
          4708,
          -1,
          802,
          22,
          3301,
          2227,
          2,
          445,
          241,
          429,
          11,
          50,
          2195,
          1270,
          2394,
          3536,
          -4,
          -1965,
          5879,
          469,
          502,
          777,
          1588,
          376,
          848,
          -393,
          4660,
          176,
          3335,
          -167,
          1591,
          2064,
          1310,
          1221,
          -26,
          337,
          2440,
          0,
          25947,
          1734,
          4,
          1032,
          8,
          3137,
          -522,
          21,
          455,
          4,
          1412,
          141,
          6619,
          0,
          0,
          202,
          2552,
          276,
          0,
          86,
          507,
          602,
          1492,
          0,
          2117,
          71,
          843,
          229,
          176,
          13,
          169,
          58,
          179,
          50,
          11854,
          1022,
          692,
          1937,
          26,
          0,
          900,
          260,
          -634,
          30,
          1694,
          167,
          4299,
          135,
          40,
          479,
          1945,
          1062,
          0,
          2406,
          337,
          413,
          362,
          284,
          -66,
          922,
          93,
          693,
          307,
          427,
          5024,
          463,
          4130,
          190,
          2682,
          144,
          1938,
          90,
          10465,
          917,
          1744,
          1198,
          371,
          575,
          123,
          145,
          312,
          893,
          105,
          0,
          186,
          -114,
          228,
          0,
          279,
          241,
          2328,
          728,
          4380,
          0,
          5432,
          222,
          3832,
          715,
          6215,
          2085,
          1398,
          463,
          -337,
          1182,
          94,
          5916,
          4556,
          373,
          170,
          146,
          1384,
          595,
          4903,
          4037,
          1746,
          18,
          1724,
          1735,
          0,
          367,
          105,
          537,
          0,
          653,
          487,
          1434,
          1683,
          530,
          -287,
          271,
          1945,
          57,
          5632,
          750,
          3992,
          11,
          383,
          598,
          3178,
          505,
          73,
          1436,
          264,
          1720,
          -25,
          634,
          648,
          0,
          314,
          2,
          2677,
          855,
          411,
          4874,
          397,
          61,
          177,
          985,
          767,
          496,
          459,
          1340,
          1187,
          4023,
          184,
          688,
          2064,
          862,
          137,
          51439,
          893,
          607,
          5521,
          1966,
          363,
          -49,
          1533,
          250,
          407,
          3549,
          0,
          1017,
          2652,
          2442,
          0,
          0,
          665,
          -1139,
          3396,
          542,
          1957,
          304,
          353,
          13107,
          1138,
          0,
          199,
          383,
          0,
          1844,
          42,
          1788,
          680,
          517,
          3,
          17,
          -328,
          1513,
          603,
          307,
          22,
          0,
          283,
          5215,
          847,
          5827,
          0,
          -759,
          267,
          2503,
          3630,
          218,
          1146,
          167,
          447,
          10378,
          -2712,
          191,
          17,
          2235,
          292,
          433,
          364,
          30,
          335,
          1287,
          128,
          133,
          -888,
          280,
          719,
          1261,
          1903,
          202,
          390,
          103,
          362,
          907,
          199,
          4943,
          6659,
          730,
          664,
          442,
          7162,
          61,
          2579,
          1583,
          224,
          851,
          2880,
          1597,
          0,
          771,
          30,
          346,
          184,
          580,
          913,
          1967,
          2929,
          103,
          661,
          19,
          258,
          361,
          1898,
          0,
          3579,
          1265,
          940,
          854,
          1464,
          3748,
          6839,
          -235,
          12,
          61,
          0,
          6269,
          142,
          24,
          1267,
          2346,
          133,
          16786,
          400,
          2033,
          1938,
          0,
          -164,
          1241,
          0,
          214,
          399,
          1388,
          805,
          1716,
          2228,
          1515,
          44,
          683,
          -473,
          4482,
          911,
          3771,
          1119,
          -6,
          -663,
          686,
          112,
          83,
          418,
          3840,
          7628,
          -10,
          3,
          1433,
          0,
          756,
          313,
          4996,
          74,
          431,
          522,
          0,
          1296,
          435,
          2128,
          -383,
          1019,
          1780,
          431,
          8097,
          0,
          138,
          292,
          606,
          0,
          26,
          322,
          336,
          36935,
          0,
          928,
          2,
          2220,
          1125,
          503,
          2920,
          4319,
          0,
          1080,
          36,
          803,
          8,
          833,
          3410,
          605,
          13342,
          586,
          559,
          4910,
          4527,
          -221,
          242,
          20,
          797,
          321,
          313,
          136,
          0,
          0,
          54,
          0,
          373,
          577,
          0,
          1296,
          2642,
          3237,
          1191,
          2278,
          -165,
          178,
          209,
          3560,
          490,
          3499,
          995,
          1128,
          163,
          103,
          70,
          191,
          1601,
          287,
          199,
          177,
          832,
          493,
          13578,
          -50,
          292,
          0,
          0,
          294,
          -558,
          -311,
          688,
          -548,
          94,
          299,
          6217,
          494,
          594,
          0,
          157,
          183,
          22867,
          1884,
          0,
          446,
          114,
          217,
          1783,
          208,
          1311,
          2813,
          783,
          243,
          5704,
          2895,
          377,
          2,
          153,
          -165,
          2764,
          6699,
          7,
          2590,
          14,
          2672,
          881,
          4793,
          370,
          6836,
          859,
          1940,
          1033,
          532,
          -321,
          383,
          865,
          1352,
          -722,
          1144,
          794,
          2061,
          36,
          734,
          99,
          879,
          0,
          4128,
          0,
          275,
          6525,
          9317,
          350,
          699,
          906,
          97,
          115,
          36,
          1076,
          -56,
          1504,
          482,
          208,
          441,
          -2,
          8278,
          407,
          197,
          348,
          101,
          294,
          3730,
          3885,
          4582,
          403,
          145,
          834,
          664,
          299,
          29,
          124,
          124,
          0,
          20,
          31868,
          599,
          248,
          0,
          879,
          34,
          2548,
          3,
          1395,
          -196,
          205,
          1442,
          393,
          -185,
          1158,
          1165,
          10721,
          1270,
          16,
          930,
          0,
          0,
          -175,
          328,
          0,
          2556,
          4969,
          726,
          -1451,
          221,
          3090,
          257,
          139,
          3168,
          260,
          4574,
          109,
          140,
          -45,
          6507,
          1291,
          -10,
          0,
          -20,
          309,
          250,
          370,
          183,
          8590,
          1011,
          1355,
          1609,
          75,
          12,
          3540,
          8486,
          20772,
          3297,
          9,
          -139,
          381,
          760,
          722,
          24,
          148,
          6691,
          0,
          787,
          4576,
          629,
          1193,
          -132,
          1438,
          -159,
          2235,
          10,
          805,
          -79,
          1664,
          0,
          690,
          616,
          200,
          8585,
          173,
          808,
          4822,
          966,
          141,
          531,
          1154,
          68,
          1499,
          2908,
          -238,
          1685,
          0,
          4344,
          0,
          1846,
          4,
          360,
          205,
          18777,
          218,
          312,
          15,
          0,
          112,
          9,
          -44,
          1023,
          3,
          77,
          0,
          137,
          -232,
          0,
          139,
          47,
          -58,
          34,
          590,
          435,
          441,
          212,
          621,
          162,
          4418,
          522,
          -547,
          0,
          726,
          435,
          530,
          106,
          590,
          2929,
          147,
          2000,
          143,
          193,
          658,
          -365,
          50,
          356,
          6718,
          21,
          3872,
          820,
          6102,
          516,
          0,
          199,
          584,
          1449,
          490,
          919,
          4243,
          6445,
          47,
          390,
          25,
          2257,
          677,
          0,
          209,
          2743,
          50,
          0,
          21,
          21,
          78,
          1034,
          6242,
          714,
          98,
          971,
          0,
          1097,
          0,
          25,
          604,
          434,
          784,
          95,
          616,
          631,
          67,
          555,
          184,
          382,
          -974,
          -516,
          0,
          5,
          428,
          0,
          54,
          1054,
          115,
          2171,
          350,
          594,
          276,
          -325,
          1594,
          2596,
          115,
          0,
          247,
          235,
          2593,
          2166,
          130,
          48,
          189,
          0,
          -134,
          23,
          0,
          1,
          3317,
          355,
          917,
          390,
          35,
          523,
          80,
          -72,
          1,
          733,
          29,
          0,
          0
         ]
        },
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Churn",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          2343,
          45,
          1270,
          2476,
          184,
          0,
          830,
          545,
          1,
          5090,
          100,
          309,
          199,
          460,
          703,
          3837,
          611,
          -8,
          55,
          168,
          785,
          2067,
          388,
          -192,
          381,
          40,
          22,
          3,
          307,
          759,
          -1,
          65,
          82,
          10,
          390,
          311,
          414,
          5,
          119,
          4,
          1262,
          1949,
          -395,
          1165,
          2240,
          300,
          3285,
          3923,
          1443,
          24,
          1618,
          517,
          1521,
          2823,
          1405,
          1535,
          1596,
          1542,
          3652,
          -1,
          7180,
          5291,
          1384,
          -191,
          320,
          146,
          341,
          -9,
          -306,
          4580,
          313,
          10576,
          -233,
          2453,
          1364,
          281,
          94,
          144,
          246,
          92,
          163,
          49,
          -416,
          409,
          363,
          3706,
          4393,
          863,
          695,
          792,
          1020,
          863,
          97,
          754,
          1040,
          122,
          880,
          501,
          4438,
          0,
          271,
          102,
          2,
          4170,
          85,
          431,
          982,
          408,
          4822,
          1250,
          216,
          1207,
          791,
          849,
          239,
          1211,
          599,
          825,
          2183,
          4499,
          1289,
          4665,
          3326,
          783,
          0,
          994,
          1354,
          239,
          -311,
          149,
          1464,
          5773,
          278,
          2910,
          541,
          1262,
          -538,
          125,
          0,
          620,
          316,
          2287,
          198,
          460,
          1145,
          -22,
          685,
          901,
          351,
          0,
          274,
          -213,
          97,
          51,
          314,
          6840,
          668,
          54,
          1242,
          292,
          665,
          1058,
          949,
          606,
          0,
          404,
          249,
          3354,
          0,
          589,
          12956,
          873,
          594,
          -20,
          4692,
          486,
          1593,
          7606,
          226,
          366,
          16,
          565,
          19,
          1082,
          1713,
          14481,
          5724,
          1451,
          -34,
          3674,
          698,
          4136,
          2656,
          2904,
          1004,
          410,
          0,
          0,
          62,
          416,
          2763,
          2984,
          143,
          0,
          696,
          152,
          -78,
          867,
          953,
          443,
          -1129,
          415,
          260,
          -411,
          1279,
          86,
          -754,
          0,
          178,
          278,
          425,
          168,
          -36,
          126,
          -87,
          6281,
          644,
          377,
          -271,
          5839,
          492,
          99,
          -713,
          32,
          543,
          403,
          437,
          1772,
          -88,
          0,
          853,
          240,
          849,
          318,
          703,
          61,
          104,
          1996,
          1238,
          879,
          425,
          416,
          844,
          860,
          639,
          710,
          1508,
          223,
          619,
          319,
          480,
          213,
          68,
          2788,
          7561,
          -46,
          1595,
          1046,
          1382,
          217,
          183,
          1321,
          25,
          4844,
          5345,
          812,
          105,
          -190,
          625,
          293,
          1033,
          1210,
          64,
          164,
          1766,
          1248,
          0,
          826,
          2630,
          3727,
          406,
          2999,
          1000,
          503,
          -203,
          361,
          274,
          3687,
          323,
          981,
          638,
          3229,
          1610,
          33,
          536,
          0,
          3057,
          2892,
          0,
          3316,
          106,
          2303,
          3301,
          426,
          9,
          -97,
          11008,
          4930,
          707,
          4,
          126,
          671,
          219,
          3622,
          1580,
          1319,
          2152,
          2843,
          0,
          1115,
          625,
          246,
          61,
          -191,
          519,
          522,
          -46,
          2269,
          386,
          0,
          9103,
          908,
          1694,
          -92,
          188,
          0,
          740,
          655,
          867,
          1009,
          6360,
          4145,
          -27,
          -122,
          149,
          52,
          335,
          526,
          -7,
          -413,
          -241,
          75,
          0,
          158,
          84,
          114,
          0,
          -127,
          49,
          216,
          506,
          213,
          170,
          776,
          879,
          1633,
          926,
          2559,
          909,
          -176,
          1085,
          1265,
          -239,
          726,
          296,
          -627,
          -468,
          551,
          0,
          101,
          517,
          37,
          854,
          30,
          81,
          0,
          663,
          102,
          370,
          189,
          111,
          665,
          214,
          0,
          1583,
          0,
          0,
          687,
          327,
          -119,
          794,
          -191,
          409,
          3,
          9,
          155,
          1330,
          -29,
          3133,
          -33,
          -170,
          1243,
          1598,
          5,
          367,
          658,
          1722,
          -315,
          2483,
          0,
          45,
          46,
          482,
          3728,
          1099,
          2222,
          983,
          204,
          1074,
          3466,
          -203,
          2436,
          976,
          146,
          156,
          189,
          163,
          242,
          1173,
          0,
          296,
          333,
          3864,
          2686,
          -109,
          65,
          597,
          -799,
          1775,
          0,
          -563,
          5,
          139,
          0,
          855,
          694,
          151,
          354,
          673,
          1265,
          597,
          -393,
          256,
          0,
          2870,
          184,
          1905,
          -36,
          -839,
          3465,
          119,
          36,
          113,
          118,
          251,
          -277,
          -189,
          4,
          1820,
          1904,
          201,
          0,
          178,
          3798,
          301,
          294,
          0,
          292,
          2998,
          2084,
          454,
          179,
          661,
          89,
          102,
          138,
          622,
          10,
          -251,
          104,
          173,
          -46,
          0,
          145,
          325,
          2311,
          158,
          1981,
          1423,
          2455,
          859,
          9004,
          970,
          486,
          127,
          409,
          664,
          223,
          -324,
          -30,
          1253,
          4758,
          30,
          81,
          0,
          328,
          145,
          465,
          751,
          -379,
          0,
          349,
          1110,
          2228,
          2166,
          -247,
          -123,
          44,
          131,
          476,
          1101,
          -168,
          -392,
          524,
          895,
          212,
          95,
          395,
          135,
          0,
          471,
          603,
          663,
          593,
          1224,
          2,
          720,
          990,
          1747,
          59,
          33,
          340,
          305,
          0,
          497,
          1371,
          1628,
          372,
          566,
          343,
          941,
          775,
          1759,
          1495,
          1444,
          470,
          1162,
          2,
          1628,
          1134,
          0,
          63,
          1221,
          5024,
          10685,
          1629,
          268,
          0,
          5613,
          513,
          362,
          3933,
          3899,
          473,
          1616,
          1817,
          129,
          1938,
          133,
          2263,
          1538,
          5,
          2625,
          1080,
          312,
          145,
          -190,
          2678,
          271,
          0,
          0,
          3161,
          -4,
          -701,
          -375,
          959,
          1091,
          -1,
          96,
          0,
          230,
          -156,
          114,
          -468,
          679,
          206,
          1,
          2518,
          378,
          391,
          -144,
          1240,
          1792,
          2284,
          264,
          0,
          -392,
          77,
          48,
          252,
          1954,
          755,
          755,
          341,
          1183,
          388,
          1919,
          1613,
          0,
          0,
          0,
          0,
          466,
          515,
          2313,
          -8,
          1634,
          100,
          5361,
          1257,
          3400,
          1377,
          2885,
          512,
          921,
          1000,
          202,
          169,
          3726,
          146,
          940,
          734,
          77,
          925,
          1836,
          660,
          1304,
          2956,
          146,
          36,
          1,
          2480,
          14282,
          0,
          311,
          560,
          3059,
          1423,
          874,
          706,
          7098,
          5389,
          1104,
          3070,
          0,
          4108,
          0,
          4291,
          6822,
          231,
          3176,
          -824,
          676,
          651,
          2657,
          801,
          405,
          82,
          179,
          1047,
          7138,
          165,
          37,
          823,
          1625,
          120,
          2957,
          2,
          1,
          1795,
          4,
          476,
          9,
          131,
          1120,
          2785,
          0,
          43,
          0,
          456,
          2018,
          0,
          354,
          719,
          828,
          2787,
          2551,
          2722,
          1880,
          2112,
          871,
          850,
          3243,
          1567,
          12,
          128,
          553,
          1694,
          408,
          1223,
          1858,
          520,
          271,
          4396,
          568,
          3665,
          244,
          568,
          42,
          309,
          -180,
          34646,
          59,
          4436,
          -242,
          10052,
          0,
          152,
          156,
          66,
          9827,
          6170,
          2383,
          635,
          16,
          362,
          191,
          455,
          3043,
          3334,
          8029,
          624,
          331,
          2929,
          1314,
          289,
          20,
          8163,
          1200,
          1732,
          0,
          576,
          493,
          682,
          0,
          3485,
          2944,
          1353,
          0,
          1541,
          548,
          254,
          4414,
          985,
          1241,
          3278,
          412,
          483,
          7135,
          61,
          1412,
          15,
          27,
          1196,
          19,
          432,
          988,
          1614,
          163,
          247,
          236,
          666,
          73,
          96,
          46,
          1501,
          -477,
          1694,
          196,
          462,
          1227,
          522,
          94,
          267,
          4570,
          25,
          0,
          1920,
          1575,
          234,
          3518,
          136,
          830,
          2054,
          -546,
          5188,
          1882,
          792,
          162,
          1035,
          746,
          17297,
          22,
          231,
          296,
          0,
          40,
          2396,
          2944,
          7084,
          569,
          0,
          1138,
          54,
          2637,
          2473,
          5603,
          225,
          -17,
          3490,
          769,
          -90,
          1785,
          -103,
          555,
          5303,
          1258,
          1679,
          1664,
          8749,
          -247,
          3444,
          5346,
          1144,
          2351,
          5514,
          123,
          429,
          5254,
          589,
          568,
          10250,
          3537,
          1947,
          1967,
          2329,
          12026,
          1757,
          107,
          1012,
          0,
          372,
          7506,
          314,
          860,
          0,
          3623,
          2470,
          1307,
          3237,
          5041,
          6619,
          1331,
          1970,
          1026,
          249,
          12857,
          2807,
          5060,
          2271,
          1,
          676,
          160,
          899,
          1230,
          0,
          700,
          0,
          67,
          127,
          2089,
          3585,
          4152,
          816,
          5447,
          275,
          1540,
          751,
          380,
          1504,
          1783,
          2269,
          5115,
          781,
          -188,
          761,
          19,
          4508,
          66,
          2266,
          4646,
          1494,
          1050,
          0,
          0,
          2171,
          3234,
          11887,
          0,
          5037,
          386,
          8781,
          538,
          5561,
          859,
          0,
          -617,
          1429,
          4189,
          580,
          2615,
          587,
          149,
          3,
          807,
          3354,
          1249,
          190,
          550,
          12737,
          0,
          1207,
          347,
          3713,
          3352,
          254,
          4654,
          -386,
          3043,
          1547,
          945,
          6402,
          1295,
          6993,
          4420,
          141,
          11462,
          6843,
          501,
          5806,
          20138,
          1315,
          0,
          7,
          3929,
          7,
          699,
          421,
          4333,
          300,
          7773,
          1794,
          2032,
          3401,
          1110,
          3068,
          983,
          2396,
          6888,
          2087,
          121,
          3469,
          5461,
          7119,
          -35,
          314,
          18508,
          5639,
          2918,
          12519,
          4069,
          4254,
          219,
          0,
          2263,
          2589,
          1982,
          7049,
          1968,
          7195,
          4117,
          3950,
          495,
          8629,
          455,
          4536,
          1777,
          3527,
          372,
          92,
          72,
          1533,
          4148,
          96,
          2245,
          834,
          4119,
          -525,
          39,
          0,
          2766,
          3300,
          36,
          22520,
          7724,
          111,
          2640,
          3436,
          0,
          3636,
          3288,
          2633,
          580,
          149,
          4329,
          1097,
          1275,
          1003,
          1004,
          814,
          808,
          859,
          4465,
          0,
          622,
          1709,
          394,
          1125,
          -462,
          550,
          540,
          941,
          1361,
          -452,
          518,
          -71,
          3234,
          98,
          196,
          936,
          300,
          217,
          163,
          110,
          1646,
          2040,
          -308,
          2325,
          86,
          1088,
          0,
          0,
          2103,
          2,
          51,
          96,
          1027,
          1308,
          840,
          5701,
          6016,
          630,
          432,
          -247,
          52,
          623,
          0,
          100,
          4089,
          1669,
          169,
          483,
          250,
          0,
          902,
          2156,
          2360,
          201,
          -92,
          514,
          -114,
          1331,
          732,
          1308,
          0,
          -970,
          1246,
          73,
          406,
          4391,
          168,
          1441,
          3881,
          678,
          141,
          -94,
          897,
          1214,
          324,
          2666,
          10613,
          697,
          953,
          198,
          1110,
          130,
          315,
          4383,
          313,
          273,
          840,
          309,
          2707,
          1109,
          101,
          725,
          309,
          541,
          0,
          1624,
          0,
          318,
          1908,
          676,
          1076,
          1228,
          0,
          13,
          12857,
          0,
          6101,
          158,
          346,
          697,
          616,
          1140,
          3444,
          1879,
          0,
          0,
          1493,
          313,
          1815,
          700,
          206,
          255,
          270,
          4987,
          673,
          320,
          0,
          292,
          757,
          40,
          1257,
          1693,
          5060,
          373,
          369,
          2346,
          1599,
          1066,
          2603,
          381,
          1314,
          5689,
          12114,
          0,
          2232,
          11862,
          127,
          2152,
          3552,
          1,
          1872,
          1341,
          7702,
          4657,
          397,
          0,
          348,
          2442,
          1543,
          805,
          2265,
          312,
          196,
          620,
          1714,
          2416,
          335,
          755,
          2850,
          1135,
          197,
          720,
          5355,
          445,
          5359,
          1443,
          4047,
          23878,
          167,
          1047,
          7613,
          117,
          2266,
          558,
          2509,
          1808,
          1130,
          653,
          2021,
          313,
          983,
          5561,
          8345,
          1231,
          3730,
          924,
          2489,
          995,
          280,
          3434,
          576,
          1363,
          581,
          1376,
          52,
          0,
          549,
          92,
          2881,
          250,
          602,
          730,
          523,
          293,
          215,
          0,
          42,
          316,
          4562,
          10180,
          0,
          281,
          250,
          0,
          404,
          506,
          1097,
          3698,
          883,
          2539,
          2326,
          811,
          256,
          682,
          483,
          1506,
          575,
          409,
          1316,
          671,
          2351,
          997,
          4808,
          6422,
          31,
          171,
          2681,
          -61,
          8929,
          715,
          1058,
          -34,
          258,
          0,
          71,
          5810,
          3473,
          238,
          1740,
          3407,
          1045,
          132,
          1000,
          115,
          180,
          201,
          8089,
          8866,
          2,
          259,
          1107,
          1730,
          447,
          2786,
          10185,
          978,
          -130,
          3049,
          11385,
          46,
          4126,
          212,
          1250,
          25,
          1277,
          390,
          9601,
          565,
          1384,
          -522,
          9676,
          134,
          2143,
          831,
          2308,
          1468,
          311,
          274,
          172,
          507,
          393,
          -39,
          1474,
          471,
          3161,
          1612,
          4099,
          68,
          414,
          5005,
          1756,
          1613,
          3123,
          4736,
          127,
          11891,
          4979,
          754,
          1207,
          712,
          297,
          3676,
          817,
          283,
          82,
          2847,
          1822,
          16,
          377,
          4415,
          303,
          12039,
          1443,
          1781,
          3415,
          318,
          423,
          2544,
          2614,
          1268,
          536,
          1293,
          1377,
          2580,
          314,
          206,
          3286,
          803,
          719,
          898,
          332,
          695,
          1819,
          2544,
          759,
          229,
          96,
          110,
          1406,
          0,
          2103,
          1599,
          925,
          -3,
          84,
          1367,
          703,
          0,
          2646,
          572,
          107,
          765,
          -253,
          0,
          2,
          6,
          0,
          448,
          4659,
          1528,
          330,
          731,
          25,
          587,
          2557,
          275,
          1129,
          2149,
          855,
          491,
          180,
          133,
          649,
          1411,
          11174,
          439,
          6158,
          3498,
          1801,
          937,
          26,
          -53,
          2337,
          1762,
          3469,
          545,
          3480,
          183,
          2416,
          424,
          0,
          294,
          1598,
          268,
          214,
          240,
          2511,
          4596,
          1273,
          3994,
          469,
          932,
          844,
          471,
          63,
          -779,
          293,
          294,
          13546,
          329,
          69,
          -3058,
          224,
          165,
          397,
          3154,
          1180,
          -487,
          879,
          806,
          674,
          0,
          323,
          921,
          5303,
          0,
          4004,
          386,
          284,
          -242,
          1086,
          2840,
          43,
          3229,
          341,
          0,
          -19,
          243,
          2575,
          105,
          0,
          -443,
          1524,
          1925,
          240,
          387,
          13,
          3203,
          606,
          0,
          527,
          309,
          4333,
          253,
          6807,
          83,
          819,
          1968,
          136,
          1995,
          285,
          621,
          597,
          0,
          1750,
          10086,
          3466,
          2193,
          2282,
          1973,
          10925,
          704,
          9480,
          3391,
          1134,
          6281,
          123,
          8556,
          0,
          2,
          1108,
          212,
          5110,
          17,
          942,
          0,
          882,
          81,
          276,
          2991,
          90,
          3754,
          395,
          676,
          8,
          0,
          2069,
          506,
          8,
          670,
          0,
          1939,
          1780,
          0,
          955,
          732,
          2022,
          4099,
          4917,
          2883,
          1374,
          398,
          3027,
          2974,
          1081,
          383,
          3913,
          3287,
          3629,
          132,
          726,
          149,
          640,
          7707,
          53,
          15,
          242,
          377,
          94,
          363,
          1970,
          1167,
          193,
          0,
          78,
          1291,
          1425,
          3382,
          5583,
          1230,
          96,
          224,
          0,
          1730,
          697,
          943,
          1458,
          95,
          2308,
          5701,
          1529,
          292,
          7468,
          1559,
          1693,
          757,
          201,
          1070,
          1330,
          314,
          677,
          2845,
          -32,
          398,
          0,
          1337,
          493,
          1750,
          4401,
          193,
          5106,
          994,
          1101,
          129,
          6060,
          55,
          1624,
          341,
          2016,
          255,
          4684,
          1052,
          1808,
          3511,
          518,
          227,
          270,
          4733,
          6657,
          613,
          5462,
          674,
          364,
          181,
          474,
          219,
          86,
          1170,
          18,
          197,
          938,
          1228,
          2156,
          2408,
          35,
          393,
          618,
          1608,
          699,
          5802,
          2749,
          1162,
          776,
          1619,
          139,
          2816,
          507,
          98,
          2753,
          0,
          498,
          700,
          4152,
          927,
          2252,
          0,
          0,
          728,
          354,
          382,
          36252,
          158,
          734,
          305,
          10252,
          3141,
          236,
          5437,
          133,
          640,
          253,
          5060,
          902,
          335,
          1,
          1451,
          1636,
          3221,
          316,
          2044,
          557,
          489,
          1138,
          188,
          1998,
          520,
          6368,
          2666,
          4182,
          3271,
          163,
          1515,
          803,
          400,
          2454,
          533,
          2352,
          9367,
          237,
          444,
          9407,
          387,
          2971,
          2643,
          261,
          514,
          5872,
          -368,
          188,
          61,
          2160,
          658,
          0,
          196,
          94,
          326,
          -395,
          9328,
          -522,
          1286,
          710,
          3,
          0,
          141,
          53,
          415,
          0,
          1979,
          0,
          7506,
          752,
          99,
          266,
          27,
          524,
          5990,
          3935,
          991,
          59,
          1598,
          0,
          802,
          2465,
          114,
          1461,
          475,
          4,
          1618,
          -271,
          2166,
          4031,
          5024,
          -1944,
          3025,
          2090,
          0,
          1429,
          0,
          311,
          66,
          2146,
          356,
          62,
          -106,
          608,
          291,
          -184,
          2345,
          4,
          6005,
          333,
          3041,
          4,
          603,
          0,
          830,
          962,
          185,
          -9,
          891,
          148,
          1374,
          -556,
          1612,
          0,
          -109,
          580,
          4500,
          -192,
          -438,
          -701,
          1400,
          5293,
          785,
          373,
          763,
          1236,
          -811,
          7279,
          479,
          199,
          1808,
          2,
          -106,
          670,
          -361,
          159,
          151,
          518,
          5073,
          607,
          -308,
          367,
          855,
          1240,
          663,
          895,
          804,
          -407,
          802,
          912,
          355,
          3,
          0,
          1836,
          1425,
          963,
          145,
          634,
          87,
          -522,
          319,
          0,
          478,
          -276,
          4227,
          2785,
          1401,
          0,
          1852,
          462,
          800,
          221,
          1089,
          451,
          496,
          1080,
          1064,
          297,
          3,
          76,
          398,
          83,
          879,
          210,
          -276,
          863,
          228,
          1396,
          -19,
          2253,
          0,
          265,
          -54,
          3821,
          0,
          -7,
          830,
          0,
          4406,
          437,
          54,
          -454,
          1006,
          2999,
          85,
          17924,
          8180,
          2431,
          341,
          4963,
          8781,
          -504,
          1738,
          214,
          684,
          580,
          273,
          811,
          0,
          1354,
          -254,
          -449,
          7863,
          336,
          1049,
          6089,
          -42,
          953,
          315,
          5845,
          378,
          765,
          5795,
          315,
          916,
          124,
          207,
          422,
          125,
          2,
          7066,
          146,
          496,
          -40,
          140,
          77,
          22,
          -51,
          26,
          154,
          534,
          1265,
          -466,
          38,
          703,
          685,
          780,
          -67,
          1085,
          29,
          560,
          22,
          6567,
          2139,
          582,
          194,
          353,
          341,
          -172,
          -639,
          864,
          134,
          50,
          507,
          480,
          -22,
          878,
          2574,
          2734,
          241,
          129,
          41,
          3387,
          148,
          249,
          5774,
          0,
          1841,
          2120,
          297,
          89,
          827,
          205,
          3743,
          117,
          -493,
          62,
          2040,
          344,
          230,
          1277,
          104,
          706,
          174,
          239,
          513,
          7290,
          5909,
          -195,
          3917,
          352,
          615,
          0,
          217,
          1015,
          409,
          1119,
          12198,
          235,
          170,
          -357,
          2145,
          -454,
          -887,
          292,
          139,
          2684,
          -1,
          1387,
          -522,
          -242,
          719,
          0,
          497,
          76,
          397,
          3723,
          1625,
          106,
          1543,
          2239,
          1977,
          0,
          2145,
          116,
          620,
          307,
          7702,
          3733,
          3993,
          428,
          5310,
          2509,
          255,
          616,
          733,
          439,
          590,
          567,
          689,
          386,
          0,
          957,
          312,
          167,
          13094,
          5086,
          707,
          466,
          152,
          1612,
          911,
          27,
          6,
          253,
          705,
          109,
          23878,
          3399,
          79,
          243,
          138,
          1317,
          2714,
          17964,
          846,
          326,
          100,
          304,
          39,
          312,
          906,
          524,
          1081,
          983,
          2232,
          1967,
          1406,
          157,
          0,
          2509,
          1368,
          3076,
          7818,
          4118,
          760,
          277,
          253,
          602,
          473,
          1167,
          343,
          1808,
          1199,
          70,
          1723,
          1323,
          2552,
          2613,
          0,
          938,
          284,
          1059,
          1684,
          534,
          293,
          262,
          0,
          2367,
          31,
          3842,
          3343,
          804,
          1011,
          -528,
          1840,
          1451,
          2351,
          1623,
          597,
          611,
          313,
          1954,
          307,
          7968,
          2160,
          1005,
          1988,
          1117,
          3370,
          109,
          1376,
          523,
          15161,
          653,
          64,
          1165,
          7019,
          1568,
          3109,
          4987,
          189,
          2467,
          5887,
          1354,
          23,
          0,
          978,
          294,
          409,
          261,
          27624,
          3846,
          2913,
          2240,
          5,
          150,
          882,
          366,
          1189,
          204,
          637,
          2383,
          1231,
          3340,
          200,
          48,
          2786,
          5313,
          201,
          389,
          3917,
          889,
          481,
          810,
          52,
          108,
          684,
          2071,
          339,
          572,
          1779,
          132,
          22086,
          0,
          205,
          317,
          4945,
          419,
          1130,
          1662,
          1871,
          1190,
          563,
          6,
          594,
          617,
          538,
          303,
          985,
          897,
          171,
          1633,
          1085,
          1980,
          1933,
          1841,
          42,
          794,
          905,
          8295,
          0,
          874,
          4321,
          0,
          745,
          924,
          216,
          1159,
          3290,
          582,
          380,
          682,
          92,
          66,
          252,
          6574,
          2094,
          294,
          1177,
          8866,
          3407,
          5205,
          3630,
          12,
          544,
          10346,
          1612,
          718,
          3547,
          914,
          1306,
          259,
          -1206,
          372,
          3518,
          505,
          1776,
          20585,
          1595,
          1000,
          138,
          561,
          170,
          502,
          379,
          14533,
          689,
          2567,
          443,
          2074,
          2374,
          1347,
          7443,
          10583,
          425,
          997,
          5966,
          2061,
          0,
          2887,
          1464,
          1315,
          466,
          2383,
          1649,
          132,
          482,
          203,
          88,
          2881,
          668,
          73,
          1309,
          2763,
          2959,
          930,
          12956,
          0,
          19,
          625,
          5511,
          1137,
          1329,
          1044,
          302,
          330,
          2857,
          68,
          1524,
          1633,
          0,
          507,
          643,
          2785,
          137,
          870,
          8603,
          18016,
          51,
          1032,
          981,
          431,
          203,
          3737,
          7005,
          111,
          565,
          889,
          202,
          307,
          0,
          429,
          4388,
          2557,
          3122,
          922,
          3796,
          185,
          151,
          829,
          103,
          79,
          1609,
          2489,
          989,
          58,
          323,
          5474,
          219,
          1240,
          1228,
          1660,
          914,
          1699,
          1167,
          19317,
          1646,
          242,
          382,
          2383,
          2666,
          1718,
          1147,
          216,
          153,
          1791,
          86,
          6791,
          292,
          1291,
          175,
          0,
          0,
          0,
          4717,
          701,
          3,
          405,
          168,
          4297,
          10532,
          4693,
          9064,
          6983,
          2,
          1536,
          1612,
          1211,
          96,
          0,
          1534,
          3403,
          687,
          7050,
          538,
          2600,
          0,
          1034,
          108,
          10185,
          3444,
          1815,
          52587,
          10884,
          201,
          190,
          94,
          1311,
          589,
          1853,
          4401,
          201,
          533,
          3840,
          994,
          96,
          1600,
          100,
          260,
          1013,
          413,
          181,
          123,
          353,
          2794,
          3850,
          2970,
          6991,
          2223,
          27696,
          116,
          2987,
          608,
          48,
          113,
          498,
          2643,
          1337,
          618,
          451,
          0,
          11,
          1165,
          2465,
          41,
          35,
          3169,
          14,
          431,
          616,
          3324,
          113,
          13014,
          9367,
          2067,
          376,
          842,
          1235,
          1,
          1276,
          44,
          255,
          926,
          1506,
          444,
          557,
          535,
          2671,
          896,
          528,
          209,
          226,
          1974,
          942,
          336,
          -306,
          4623,
          276,
          0,
          1189,
          431,
          2218,
          1495,
          828,
          12972,
          948,
          48,
          0,
          537,
          1938,
          9131,
          443,
          132,
          3411,
          1,
          10905,
          976,
          2531,
          1270,
          5359,
          10596,
          691,
          347,
          9962,
          11862,
          1756,
          4068,
          5310,
          215,
          106,
          0,
          469,
          2326,
          3370,
          144,
          733,
          -309,
          5584,
          61,
          1047,
          265,
          117,
          5473,
          106,
          127,
          380,
          0,
          255,
          2145,
          91,
          1853,
          11262,
          0,
          1129,
          108,
          1696,
          854,
          223,
          40,
          2048,
          3689,
          539,
          590,
          780,
          1464,
          1311,
          7,
          636,
          24277,
          1872,
          490,
          118,
          4119,
          1504,
          2087,
          5689,
          201,
          119,
          17964,
          1113,
          1596,
          2635,
          568,
          2626,
          4752,
          528,
          2552,
          3343,
          995,
          7968,
          0,
          2587,
          6468,
          4744,
          1231,
          3443,
          2568,
          1644,
          1354,
          602,
          1795,
          0,
          268,
          52,
          924,
          3033,
          6574,
          0,
          108,
          409,
          726,
          1871,
          1094,
          2915,
          480,
          1227,
          1205,
          161,
          802,
          723,
          6538,
          147,
          0,
          1416,
          14220,
          12569,
          555,
          158,
          261,
          786,
          27,
          0,
          2815,
          3082,
          1233,
          739,
          640,
          1843,
          322,
          7331,
          1765,
          1236,
          6046,
          934,
          1013,
          596,
          4775,
          3109,
          388,
          262,
          654,
          1855,
          810,
          1664,
          362,
          1293,
          874,
          10943,
          4539,
          2817,
          1646,
          808,
          44,
          425,
          2367,
          10252,
          1000,
          4380,
          20585,
          2601,
          238,
          0,
          3371,
          10583,
          16,
          1451,
          0,
          132,
          189,
          288,
          1027,
          6784,
          1464,
          8295,
          2917,
          158,
          1588,
          282,
          5437,
          1185,
          895,
          1857,
          4596,
          565,
          1781,
          3415,
          12039,
          12,
          139,
          2488,
          7111,
          361,
          2109,
          1093,
          2646,
          6,
          670,
          592,
          1965,
          313,
          40,
          4198,
          536,
          1311,
          766,
          2557,
          79,
          1318,
          3771,
          0,
          6138,
          129,
          469,
          893,
          519,
          4017,
          643,
          2338,
          0,
          6481,
          820,
          1784,
          1411,
          7529,
          206,
          1388,
          1807,
          480,
          571,
          3994,
          1693,
          80,
          267,
          2581,
          1134,
          309,
          6807,
          6746,
          375,
          658,
          371,
          2693,
          329,
          79,
          0,
          6281,
          775,
          411,
          2144,
          431,
          123,
          3792,
          4807,
          8603,
          2178,
          2991,
          4174,
          10925,
          2411,
          14646,
          86,
          11115,
          6900,
          5878,
          1780,
          183,
          3648,
          4112,
          542,
          243,
          942,
          254,
          0,
          0,
          992,
          23,
          732,
          4629,
          0,
          70,
          5063,
          4572,
          129,
          4984,
          1004,
          4787,
          914,
          2326,
          81,
          203,
          3951,
          240,
          338,
          0,
          0,
          944,
          5,
          697,
          1948,
          3654,
          551,
          997,
          477,
          3663,
          853,
          8278,
          7458,
          44,
          1193,
          230,
          8729,
          2384,
          3401,
          881,
          1522,
          4243,
          473,
          5462,
          4412,
          10332,
          0,
          238,
          62,
          2398,
          704,
          506,
          2352,
          938,
          1720,
          1492,
          1778,
          0,
          942,
          556,
          10541,
          1536,
          235,
          935,
          10861,
          2331,
          1228,
          125,
          2816,
          0,
          22867,
          1311,
          94,
          0,
          3371,
          943,
          9366,
          6746,
          1076,
          0,
          1624,
          801,
          1070,
          123,
          3623,
          533,
          994,
          386,
          1026,
          2424,
          1331,
          2206,
          0,
          4693,
          1230,
          3417,
          1767,
          5291,
          3403,
          133,
          4745,
          3324,
          5220,
          2600,
          260,
          608,
          462,
          14,
          1146,
          181,
          3114,
          376,
          9367,
          382,
          920,
          129,
          169,
          679,
          535,
          3115,
          1234,
          197,
          3186,
          3160,
          195,
          6027,
          764,
          583,
          0,
          5715,
          1204,
          4922,
          234,
          9421,
          4198,
          3576,
          1925,
          1129,
          265,
          38,
          1982,
          3399,
          691,
          330,
          318,
          644,
          4,
          1165,
          4708,
          2820,
          1147,
          980,
          2983,
          339,
          1940,
          1445,
          294,
          5887,
          11891,
          78,
          733,
          5028,
          481,
          314,
          684,
          1681,
          10250,
          254,
          0,
          307,
          957,
          155,
          614,
          426,
          8304,
          4721,
          395,
          3588,
          2540,
          883,
          4402,
          256,
          43,
          3856,
          3025,
          2326,
          925,
          1961,
          2037,
          459,
          572,
          452,
          5313,
          1533,
          1337,
          2986,
          1230,
          538,
          5108,
          1341,
          320,
          131,
          2074,
          1443,
          3338,
          661,
          7105,
          3735,
          538,
          4657,
          5,
          230,
          1808,
          81204,
          3518,
          25,
          2551,
          11303,
          2724,
          755,
          718,
          870,
          978,
          544,
          461,
          2795,
          139,
          1044,
          1633,
          6513,
          1388,
          15,
          2544,
          2007,
          2166,
          4579,
          2488,
          2190,
          254,
          1579,
          330,
          938,
          5372,
          3176,
          496,
          0,
          482,
          180,
          4198,
          817,
          5021,
          991,
          133,
          366,
          979,
          2939,
          4803,
          1255,
          8556,
          588,
          994,
          820,
          203,
          86,
          2855,
          474,
          203,
          7005,
          775,
          4112,
          255,
          243,
          942,
          1180,
          992,
          70,
          4629,
          829,
          0,
          2776,
          279,
          129,
          157,
          659,
          2493,
          3949,
          997,
          1699,
          1181,
          0,
          0,
          1948,
          306,
          18967,
          261,
          694,
          202,
          4831,
          264,
          278,
          7802,
          3629,
          547,
          4468,
          1989,
          2645,
          4060,
          303,
          2600,
          1818,
          728,
          955,
          265,
          123,
          4807,
          205,
          2812,
          6571,
          2758,
          2037,
          0,
          2223,
          1085,
          602,
          1646,
          415,
          2354,
          318,
          1730,
          448,
          994,
          19,
          1625,
          1144,
          897,
          704,
          1996,
          216,
          1206,
          1855,
          62,
          1536,
          824,
          235,
          108,
          2331,
          390,
          -32,
          172,
          10541,
          480,
          2374,
          479,
          94,
          417,
          2557,
          589,
          201,
          2885,
          2155,
          902,
          1791,
          2206,
          5291,
          638,
          4448,
          5267,
          3324,
          61,
          0,
          116,
          413,
          353,
          1712,
          523,
          1454,
          9367,
          291,
          846,
          341,
          113,
          13014,
          632,
          27696,
          3715,
          4675,
          3271,
          123,
          10185,
          1026,
          905,
          775,
          926,
          349,
          484,
          1830,
          2987,
          197,
          431,
          169,
          226,
          2424,
          1234,
          271,
          -69,
          1562,
          52587,
          535,
          2159,
          195,
          154,
          681,
          2326,
          961,
          2532,
          455,
          2013,
          1568,
          1215,
          121,
          1,
          597,
          763,
          326,
          1380,
          0,
          368,
          438,
          483,
          7620,
          805,
          0,
          302,
          215,
          5473,
          106,
          2346,
          3219,
          2227,
          6610,
          195,
          973,
          7766,
          948,
          2149,
          1413,
          117,
          0,
          71,
          1938,
          5359,
          519,
          -46,
          1075,
          91,
          598,
          1027,
          235,
          2,
          4945,
          311,
          154,
          712,
          7585,
          2489,
          2896,
          354,
          2667,
          144,
          760,
          1050,
          5296,
          12980,
          509,
          1644,
          451,
          53,
          334,
          6771,
          52,
          2240,
          4761,
          1570,
          588,
          957,
          224,
          2161,
          426,
          147,
          508,
          294,
          1825,
          158,
          0,
          4721,
          282,
          108,
          187,
          3764,
          1416,
          2801,
          494,
          1978,
          155,
          4608,
          951,
          495,
          1843,
          262,
          0,
          3561,
          465,
          76,
          1796,
          874,
          2140,
          295,
          304,
          624,
          303,
          273,
          3342,
          1227,
          283,
          3384,
          2367,
          5958,
          3735,
          2693,
          655,
          1681,
          866,
          774,
          2678,
          3371,
          10583,
          4176,
          334,
          393,
          916,
          832,
          3287,
          488,
          1947,
          558,
          235,
          288,
          6036,
          3,
          0,
          81204,
          5236,
          2466,
          1348,
          7929,
          70,
          459,
          2102,
          394,
          4126,
          224,
          9001,
          560,
          102,
          1088,
          562,
          718,
          230,
          -67,
          3848,
          579,
          1646,
          1229,
          129,
          197,
          3779,
          2016,
          436,
          413,
          116,
          62,
          107,
          497,
          2557,
          2646,
          3771,
          1066,
          1,
          232,
          1177,
          603,
          736,
          129,
          -115,
          104,
          589,
          3472,
          2845,
          7529,
          309,
          0,
          1672,
          1711,
          1295,
          2046,
          455,
          722,
          371,
          997,
          206,
          820,
          410,
          1216,
          11115,
          796,
          5037,
          1780,
          429,
          2779,
          222,
          0,
          1445,
          655,
          77,
          115,
          2722,
          1438,
          140,
          915,
          1058,
          0,
          4984,
          697,
          4787,
          962,
          1320,
          2955,
          0,
          12067,
          3006,
          388,
          0,
          565,
          329,
          4243,
          72,
          4328,
          395,
          0,
          205,
          1730,
          5,
          1965,
          19,
          7802,
          2352,
          155,
          401,
          79,
          1646,
          437,
          935,
          2929,
          2304,
          348,
          640,
          88,
          2283,
          473,
          0,
          397,
          62,
          3634,
          273,
          3417,
          133,
          1624,
          386,
          2530,
          2838,
          1026,
          2206,
          260,
          0,
          2987,
          1230,
          2335,
          8000,
          -27,
          1146,
          1004,
          197,
          169,
          393,
          29,
          920,
          3608,
          870,
          9367,
          1374,
          6027,
          1,
          349,
          39,
          583,
          1276,
          2364,
          53,
          1612,
          639,
          3114,
          1,
          1070,
          195,
          3144,
          3236,
          3208,
          1568,
          1191,
          706,
          2616,
          215,
          3230,
          726,
          3713,
          0,
          265,
          70,
          388,
          1727,
          205,
          2881,
          846,
          2266,
          742,
          2149,
          344,
          1430,
          265,
          38,
          3538,
          973,
          1831,
          641,
          1047,
          8103,
          523,
          0,
          515,
          8304,
          1368,
          272,
          -46,
          168,
          515,
          223,
          228,
          644,
          330,
          952,
          1165,
          0,
          2823,
          1490,
          48,
          187,
          8304,
          276,
          147,
          323,
          799,
          2540,
          322,
          495,
          358,
          2371,
          53,
          861,
          602,
          4041,
          275,
          12,
          1742,
          46,
          2027,
          3221,
          1720,
          2033,
          3189,
          2946,
          1230,
          572,
          1435,
          403,
          1071,
          3301,
          1723,
          705,
          955,
          303,
          3287,
          109,
          224,
          538,
          5397,
          1957,
          774,
          1188,
          1167,
          2611,
          3145,
          825,
          122,
          334,
          623,
          86,
          976,
          393,
          293,
          476,
          2724,
          2391,
          11,
          54,
          3109,
          3126,
          488,
          978,
          718,
          3255,
          1544,
          0,
          4071,
          29080,
          12569,
          2896,
          196,
          1027,
          463,
          3511,
          224,
          198,
          0,
          674,
          623,
          558,
          259,
          432,
          159,
          657,
          1808,
          392,
          6651,
          2795,
          1414,
          2666,
          668,
          502,
          1809,
          267,
          2473,
          372,
          484,
          1767,
          491,
          558,
          337,
          654,
          2427,
          386,
          251,
          1044,
          2907,
          828,
          1857,
          1962,
          2787,
          1123,
          643,
          62,
          2007,
          3463,
          482,
          1298,
          5329,
          6181,
          1255,
          132,
          2733,
          4189,
          724,
          205,
          154,
          232,
          979,
          5021,
          2469,
          180,
          658,
          2918,
          588,
          423,
          302,
          414,
          817,
          183,
          806,
          9299,
          478,
          1057,
          762,
          31472,
          1090,
          3738,
          1277,
          756,
          1636,
          300,
          321,
          1295,
          839,
          1974,
          2855,
          5475,
          536,
          1279,
          775,
          203,
          146,
          4124,
          14968,
          1451,
          3026,
          431,
          985,
          243,
          1707,
          5511,
          8,
          10861,
          373,
          1807,
          2776,
          1058,
          0,
          1381,
          204,
          267,
          311,
          306,
          752,
          1574,
          1181,
          1761,
          4629,
          154,
          4112,
          11650,
          6138,
          264,
          2144,
          482,
          0,
          4831,
          12067,
          1227,
          1232,
          275,
          369,
          240,
          2398,
          2785,
          62,
          1377,
          163,
          1199,
          569,
          0,
          1989,
          46,
          372,
          2812,
          2952,
          569,
          2812,
          1199,
          3904,
          156,
          205,
          1625,
          415,
          1812,
          775,
          180,
          46,
          4017,
          437,
          318,
          1646,
          470,
          3591,
          2037,
          2979,
          1092,
          661,
          2196,
          417,
          2077,
          2283,
          1536,
          446,
          5,
          216,
          565,
          476,
          -32,
          1348,
          228,
          1859,
          2306,
          2038,
          824,
          348,
          1515,
          1514,
          94,
          2325,
          417,
          3450,
          2543,
          464,
          1853,
          272,
          1925,
          2331,
          324,
          116,
          1779,
          346,
          353,
          27696,
          2155,
          413,
          2590,
          320,
          1454,
          1382,
          2427,
          2498,
          1003,
          2007,
          926,
          632,
          2206,
          1830,
          814,
          5275,
          2129,
          2717,
          341,
          2885,
          523,
          4256,
          484,
          535,
          431,
          1269,
          0,
          211,
          4012,
          625,
          1584,
          315,
          200,
          589,
          -205,
          -46,
          8304,
          308,
          1113,
          992,
          272,
          1413,
          1819,
          1623,
          630,
          2975,
          7968,
          154,
          5169,
          228,
          4945,
          1708,
          0,
          1584,
          1707,
          952,
          1978,
          1105,
          52,
          6771,
          890,
          56,
          1644,
          151,
          0,
          2240,
          957,
          689,
          2161,
          1527,
          8304,
          698,
          494,
          2801,
          4761,
          834,
          139,
          12,
          877,
          2066,
          555,
          48,
          235,
          1595,
          5741,
          426,
          187,
          166,
          1416,
          1189,
          486,
          0,
          2557,
          3138,
          1720,
          1765,
          1506,
          3585,
          262,
          1093,
          76,
          4680,
          1477,
          3561,
          3340,
          1071,
          247,
          112,
          527,
          2580,
          975,
          2986,
          2048,
          2785,
          127,
          0,
          4951,
          488,
          1154,
          624,
          4343,
          2367,
          283,
          6403,
          802,
          3735,
          2693,
          0,
          1027,
          3902,
          243,
          0,
          1055,
          60,
          5397,
          217,
          6551,
          303,
          2093,
          1167,
          2376,
          0,
          949,
          393,
          2678,
          11,
          4416,
          390,
          1934,
          3511,
          1696,
          1278,
          1011,
          2939,
          3466,
          372,
          549,
          1102,
          973,
          491,
          3337,
          368,
          326,
          12264,
          203,
          502,
          346,
          4328,
          279,
          131,
          2646,
          6,
          1579,
          2007,
          1,
          469,
          643,
          514,
          1289,
          3676,
          0,
          104,
          159,
          728,
          107,
          1946,
          1519,
          5112,
          1090,
          4833,
          1925,
          335,
          371,
          5475,
          203,
          50,
          11862,
          820,
          791,
          1279,
          455,
          536,
          1693,
          1495,
          985,
          935,
          410,
          51,
          679,
          1780,
          222,
          1210,
          1693,
          876,
          3308,
          0,
          822,
          0,
          3904,
          2488,
          88,
          121,
          0,
          0,
          464,
          79,
          2557,
          272,
          2543,
          1224,
          320,
          4256,
          1168,
          703,
          136,
          3810,
          765,
          1146,
          358,
          1511,
          557,
          583,
          557,
          113,
          2850,
          505,
          825,
          5715,
          -215,
          2239,
          376,
          15485,
          2918,
          67,
          1664,
          192,
          1519,
          0,
          0,
          4708,
          3283,
          1211,
          347,
          423,
          3982,
          2506,
          697,
          456,
          324,
          5768,
          1517,
          0,
          482,
          638,
          2524,
          4539,
          2039,
          128,
          4787,
          1147,
          1978,
          51,
          -122,
          294,
          922,
          51,
          694,
          607,
          2155,
          159,
          7050,
          1055,
          8165,
          376,
          3701,
          644,
          3623,
          360,
          284,
          1893,
          69,
          1355,
          1506,
          1393,
          487,
          655,
          1788,
          0,
          41,
          215,
          1187,
          0,
          1213,
          653,
          3426,
          238,
          1690,
          0,
          27,
          681,
          18188,
          1006,
          1352,
          2340,
          366,
          -374,
          1231,
          1347,
          1778,
          2201,
          0,
          2557,
          0,
          1320,
          0,
          0,
          22125,
          6188,
          1077,
          45248,
          3291,
          562,
          1046,
          87,
          10281,
          429,
          3648,
          863,
          3,
          2648,
          0,
          0,
          655,
          854,
          858,
          567,
          399,
          257,
          3735,
          57,
          1235,
          257,
          401,
          3290,
          297,
          33,
          1179,
          471,
          938,
          1636,
          1243,
          0,
          1679,
          2022,
          2033,
          869,
          3611,
          3624,
          2488,
          2228,
          4,
          2557,
          1390,
          1228,
          1265,
          6046,
          816,
          0,
          118,
          1165,
          99,
          1697,
          866,
          0,
          5301,
          7918,
          1,
          160,
          414,
          406,
          0,
          655,
          514,
          11766,
          0,
          2892,
          762,
          2037,
          255,
          1536,
          0,
          2,
          1435,
          1219,
          2971,
          440,
          3301,
          2321,
          213,
          0,
          108,
          920,
          1031,
          294,
          527,
          1030,
          2503,
          1547,
          1,
          1167,
          523,
          -207,
          3283,
          1147,
          1170,
          4642,
          2666,
          792,
          127,
          0,
          0,
          121,
          618,
          2495,
          569,
          473,
          474,
          0,
          1796,
          520,
          118,
          3342,
          3239,
          23,
          40,
          226,
          939,
          -20,
          940,
          5,
          0,
          447,
          126,
          4718,
          843,
          656,
          91,
          0,
          2984,
          870,
          7608,
          5115,
          247,
          154,
          1731,
          3634,
          19,
          1129,
          0,
          12264,
          478,
          2081,
          965,
          332,
          1163,
          1960,
          294,
          188,
          254,
          1377,
          523,
          600,
          387,
          849,
          1317,
          273,
          0,
          2538,
          133,
          146,
          11115,
          486,
          159,
          1201,
          2590,
          800,
          0,
          385,
          238,
          238,
          1812,
          413,
          152,
          1031,
          3143,
          613,
          168,
          2144,
          1331,
          2611,
          925,
          985,
          367,
          302,
          47,
          1946,
          2266,
          992,
          3544,
          76,
          6013,
          19,
          232,
          993,
          535,
          127,
          1528,
          324,
          1639,
          0,
          -370,
          11862,
          323,
          3940,
          8897,
          2002,
          668,
          2946,
          4216,
          0,
          23076,
          7459,
          5,
          4147,
          0,
          131,
          231,
          720,
          598,
          1819,
          587,
          1495,
          442,
          271,
          29340,
          1014,
          2366,
          26965,
          1411,
          617,
          4736,
          2540,
          403,
          423,
          131,
          4041,
          273,
          101,
          0,
          2544,
          1699,
          2689,
          3160,
          388,
          393,
          69,
          2658,
          -366,
          466,
          64,
          580,
          3317,
          955,
          7441,
          377,
          994,
          1766,
          -236,
          774,
          659,
          149,
          1405,
          290,
          595,
          4613,
          0,
          2204,
          576,
          3840,
          1358,
          5169,
          0,
          533,
          1788,
          1070,
          80,
          3254,
          1309,
          0,
          1534,
          535,
          368,
          415,
          3434,
          35,
          2948,
          2473,
          1133,
          306,
          -382,
          1224,
          5359,
          2166,
          925,
          495,
          2374,
          1925,
          279,
          842,
          6748,
          -242,
          3917,
          1711,
          298,
          335,
          2240,
          1478,
          390,
          1234,
          3704,
          890,
          3663,
          119,
          167,
          1022,
          305,
          172,
          3047,
          616,
          453,
          526,
          169,
          1347,
          378,
          2889,
          57,
          0,
          1293,
          2085,
          315,
          1627,
          1853,
          1429,
          3951,
          780,
          674,
          218,
          763,
          4151,
          2787,
          26,
          153,
          362,
          1184,
          6840,
          3145,
          660,
          1311,
          215,
          3324,
          0,
          747,
          448,
          1246,
          271,
          1271,
          962,
          3054,
          0,
          346,
          3550,
          1379,
          1093,
          295,
          129,
          636,
          4101,
          3352,
          334,
          341,
          8491,
          227,
          125,
          3608,
          2536,
          4688,
          0,
          1113,
          1857,
          2892,
          6,
          1076,
          -930,
          1382,
          2048,
          608,
          -274,
          132,
          344,
          2230,
          5679,
          885,
          3764,
          4017,
          368,
          569,
          272,
          167,
          1388,
          7895,
          2160,
          1587,
          1723,
          691,
          243,
          3696,
          71,
          0,
          3765,
          977,
          15161,
          0,
          776,
          1994,
          616,
          3512,
          6529,
          0,
          2129,
          874,
          496,
          1539,
          348,
          1287,
          12356,
          5763,
          -267,
          443,
          934,
          4859,
          1843,
          1289,
          19,
          8332,
          5517,
          949,
          -157,
          254,
          136,
          285,
          874,
          1641,
          1088,
          1138,
          13107,
          2557,
          142,
          733,
          -405,
          11008,
          246,
          0,
          536,
          1506,
          716,
          354,
          1757,
          7554,
          854,
          1230,
          1303,
          93,
          -291,
          69,
          771,
          -675,
          973,
          377,
          480,
          552,
          3733,
          -158,
          136,
          4590,
          283,
          0,
          419,
          462,
          4,
          363,
          7560,
          1297,
          40,
          436,
          403,
          5637,
          916,
          1942,
          385,
          -1042,
          3696,
          3384,
          556,
          315,
          2812,
          324,
          -519,
          188,
          267,
          21,
          2038,
          11115,
          194,
          169,
          0,
          2981,
          67,
          360,
          3376,
          4718,
          387,
          4266,
          396,
          2085,
          62,
          -268,
          1697,
          341,
          109,
          0,
          4158,
          156,
          390,
          36,
          0,
          50,
          102,
          57,
          973,
          3,
          1245,
          828,
          0,
          167,
          66,
          687,
          2688,
          625,
          1742,
          1729,
          889,
          224,
          768,
          1809,
          423,
          335,
          182,
          4401,
          607,
          1084,
          0,
          687,
          659,
          1435,
          8828,
          69,
          79,
          990,
          2290,
          679,
          -172,
          548,
          565,
          228,
          0,
          228,
          454,
          514,
          2885,
          567,
          2235,
          5267,
          29,
          291,
          4889,
          502,
          3608,
          3556,
          -41,
          3944,
          202,
          1175,
          974,
          3141,
          4079,
          0,
          3357,
          2587,
          434,
          1291,
          315,
          394,
          5561,
          3528,
          437,
          3119,
          264,
          3285,
          1396,
          510,
          -101,
          634,
          4210,
          0,
          84,
          278,
          849,
          1612,
          204,
          1854,
          4661,
          1579,
          1561,
          1468,
          2887,
          224,
          1760,
          3735,
          19,
          61,
          95,
          5,
          360,
          1730,
          0,
          7968,
          7005,
          3020,
          605,
          376,
          891,
          704,
          4348,
          2353,
          8000,
          2781,
          541,
          3576,
          2725,
          2354,
          775,
          622,
          4,
          5,
          3943,
          802,
          2351,
          1807,
          2,
          2297,
          779,
          108,
          459,
          110,
          13711,
          3415,
          2376,
          181,
          448,
          714,
          3608,
          5310,
          2144,
          2281,
          2679,
          1495,
          1304,
          78,
          437,
          7433,
          6112,
          155,
          9883,
          495,
          294,
          8990,
          3173,
          53,
          640,
          217,
          804,
          1374,
          3,
          502,
          61,
          153,
          8,
          780,
          630,
          2161,
          32685,
          3384,
          5310,
          383,
          772,
          2976,
          2729,
          520,
          205,
          2378,
          48,
          461,
          116,
          272,
          799,
          0,
          1534,
          4457,
          189,
          1880,
          0,
          11835,
          839,
          952,
          -197,
          1657,
          1947,
          30,
          2027,
          1332,
          72,
          153,
          70,
          36,
          -198,
          5,
          2326,
          67,
          598,
          1527,
          1623,
          253,
          1445,
          256,
          1181,
          1646,
          -309,
          670,
          2196,
          947,
          71,
          521,
          5115,
          -17,
          556,
          229,
          325,
          5112,
          11821,
          650,
          8648,
          168,
          2486,
          396,
          1791,
          168,
          3308,
          4721,
          18111,
          1146,
          237,
          618,
          1085,
          1028,
          114,
          0,
          631,
          660,
          2421,
          794,
          2138,
          2965,
          222,
          6997,
          665,
          285,
          473,
          2226,
          245,
          1133,
          294,
          451,
          19268,
          1461,
          465,
          0,
          654,
          79,
          4216,
          66,
          0,
          5539,
          2599,
          2187,
          2120,
          339,
          8229,
          346,
          158,
          7103,
          4855,
          3696,
          -357,
          8304,
          48,
          384,
          751,
          3546,
          73,
          71,
          974,
          306,
          3463,
          3676,
          459,
          286,
          -11,
          273,
          2665,
          3290,
          8535,
          1276,
          2736,
          10287,
          1624,
          0,
          446,
          395,
          315,
          484,
          0,
          507,
          2987,
          801,
          0,
          572,
          2,
          34,
          1075,
          508,
          926,
          10861,
          24,
          611,
          1857,
          329,
          2635,
          225,
          1604,
          4657,
          1406,
          6728,
          611,
          1805,
          -399,
          208,
          10884,
          3472,
          556,
          7192,
          13460,
          2988,
          102,
          1868,
          -578,
          241,
          162,
          5320,
          366,
          1504,
          262,
          191,
          73,
          358,
          483,
          5047,
          5862,
          0,
          2843,
          351,
          372,
          27696,
          133,
          3,
          15459,
          2551,
          571,
          7803,
          0,
          24,
          2104,
          3158,
          10,
          147,
          12857,
          4389,
          474,
          2326,
          3644,
          1988,
          1595,
          331,
          4623,
          12018,
          154,
          190,
          3904,
          882,
          53,
          6542,
          687,
          3064,
          9228,
          800,
          3505,
          5958,
          667,
          36,
          1059,
          915,
          1547,
          3050,
          7050,
          2,
          14144,
          705,
          1758,
          476,
          348,
          5969,
          3195,
          2013,
          121,
          82,
          935,
          3754,
          104,
          4751,
          1529,
          102,
          1796,
          935,
          365,
          222,
          0,
          1015,
          107,
          82,
          877,
          2557,
          1673,
          2467,
          980,
          183,
          387,
          459,
          1227,
          302,
          175,
          182,
          -367,
          876,
          201,
          796,
          0,
          1559,
          70,
          2734,
          3528,
          469,
          3311,
          152,
          1047,
          484,
          1587,
          4461,
          3551,
          495,
          3792,
          489,
          159,
          985,
          369,
          257,
          2779,
          638,
          833,
          9314,
          4745,
          2693,
          160,
          1697,
          0,
          -770,
          20806,
          3136,
          1430,
          6831,
          664,
          49,
          604,
          931,
          699,
          212,
          362,
          2667,
          -754,
          517,
          258,
          3140,
          658,
          255,
          5350,
          9756,
          3646,
          368,
          1,
          1435,
          122,
          2027,
          496,
          83,
          467,
          0,
          471,
          5715,
          -389,
          364,
          25,
          3450,
          2374,
          5514,
          455,
          739,
          336,
          436,
          536,
          0,
          0,
          1181,
          265,
          2573,
          -537,
          1128,
          2780,
          405,
          706,
          2661,
          2734,
          196,
          533,
          6728,
          1227,
          5,
          197,
          13,
          284,
          33,
          2333,
          1705,
          2039,
          381,
          3738,
          4401,
          553,
          -725,
          67,
          330,
          5943,
          25,
          316,
          4389,
          0,
          1389,
          398,
          247,
          0,
          131,
          604,
          0,
          238,
          673,
          7944,
          3696,
          347,
          -199,
          337,
          5091,
          970,
          258,
          614,
          1929,
          2362,
          8121,
          608,
          3723,
          117,
          363,
          130,
          2225,
          -105,
          7687,
          222,
          736,
          84,
          196,
          2123,
          -482,
          700,
          1388,
          222,
          592,
          0,
          411,
          -104,
          475,
          181,
          653,
          0,
          0,
          942,
          13,
          200,
          660,
          1309,
          -364,
          -616,
          2596,
          5802,
          568,
          296,
          561,
          2894,
          2344,
          1345,
          171,
          1838,
          549,
          1097,
          1148,
          3818,
          934,
          -637,
          65,
          24,
          907,
          100,
          686,
          3,
          4692,
          -197,
          4335,
          664,
          307,
          5,
          -67,
          3754,
          1139,
          250,
          3695,
          1025,
          328,
          0,
          6203,
          1904,
          347,
          1289,
          129,
          178,
          81,
          599,
          190,
          0,
          563,
          105,
          3518,
          248,
          961,
          640,
          1144,
          0,
          4381,
          1134,
          570,
          0,
          357,
          593,
          229,
          0,
          1157,
          234,
          322,
          113,
          2139,
          2251,
          0,
          2756,
          7132,
          2047,
          0,
          1151,
          5244,
          1,
          203,
          2162,
          337,
          0,
          2,
          1365,
          2366,
          -803,
          873,
          -76,
          2990,
          538,
          6388,
          840,
          1847,
          40,
          30,
          860,
          123,
          667,
          601,
          616,
          259,
          307,
          -29,
          849,
          47,
          2194,
          3413,
          1299,
          622,
          1901,
          166,
          1717,
          -160,
          89,
          493,
          2644,
          596,
          0,
          606,
          -346,
          550,
          237,
          415,
          -125,
          1984,
          121,
          266,
          1723,
          1049,
          21,
          341,
          233,
          247,
          3117,
          242,
          24,
          7007,
          1851,
          573,
          1178,
          748,
          -407,
          4695,
          2500,
          86,
          968,
          192,
          93,
          0,
          0,
          -491,
          1517,
          0,
          775,
          276,
          5781,
          0,
          901,
          431,
          911,
          96,
          0,
          -288,
          2923,
          16,
          744,
          551,
          322,
          199,
          0,
          0,
          633,
          203,
          583,
          2749,
          769,
          1924,
          1064,
          1099,
          2971,
          2657,
          21,
          0,
          0,
          9447,
          433,
          375,
          2399,
          148,
          224,
          147,
          744,
          -94,
          9,
          1218,
          1106,
          0,
          167,
          0,
          324,
          127,
          230,
          344,
          105,
          634,
          317,
          2234,
          223,
          230,
          0,
          313,
          289,
          0,
          3676,
          -184,
          3954,
          2951,
          78,
          1526,
          0,
          233,
          67,
          38,
          1948,
          5296,
          612,
          4872,
          131,
          4943,
          -257,
          331,
          105,
          1160,
          4930,
          471,
          35,
          1750,
          5275,
          728,
          0,
          263,
          601,
          6429,
          1076,
          -57,
          2,
          514,
          -375,
          28,
          404,
          3430,
          99,
          2669,
          202,
          715,
          674,
          312,
          1463,
          789,
          7,
          473,
          3150,
          12223,
          1541,
          142,
          3,
          1207,
          351,
          777,
          997,
          15,
          2819,
          126,
          953,
          10,
          -577,
          1159,
          1350,
          88,
          -245,
          161,
          41,
          450,
          782,
          -666,
          -151,
          55,
          -666,
          87,
          4629,
          2176,
          3431,
          208,
          0,
          240,
          1388,
          5807,
          17,
          0,
          1743,
          1008,
          151,
          3196,
          1411,
          630,
          0,
          561,
          0,
          281,
          40,
          0,
          1032,
          -207,
          235,
          0,
          367,
          505,
          804,
          -301,
          -813,
          79,
          50,
          1205,
          365,
          -1415,
          223,
          1897,
          22867,
          949,
          1165,
          1655,
          260,
          0,
          -880,
          107,
          1624,
          1257,
          355,
          579,
          1403,
          -176,
          -513,
          586,
          1840,
          720,
          97,
          13,
          140,
          521,
          364,
          416,
          3304,
          1880,
          982,
          0,
          173,
          911,
          0,
          549,
          704,
          476,
          3738,
          634,
          1134,
          -128,
          7102,
          186,
          1918,
          0,
          250,
          31,
          631,
          62,
          479,
          1121,
          138,
          0,
          239,
          866,
          1466,
          3570,
          440,
          714,
          674,
          127,
          588,
          -413,
          882,
          877,
          337,
          192,
          3350,
          99,
          806,
          6739,
          228,
          -232,
          556,
          761,
          207,
          221,
          865,
          0,
          1851,
          -272,
          367,
          0,
          230,
          0,
          1954,
          379,
          -341,
          179,
          13,
          839,
          192,
          576,
          643,
          342,
          1329,
          219,
          141,
          402,
          8023,
          1131,
          1925,
          0,
          103,
          1395,
          621,
          1161,
          24,
          2,
          295,
          -208,
          258,
          119,
          967,
          -90,
          0,
          0,
          392,
          23,
          2486,
          81,
          1563,
          1602,
          2144,
          273,
          455,
          1038,
          -384,
          307,
          93,
          127,
          -150,
          20718,
          -218,
          5222,
          484,
          75,
          1480,
          1111,
          639,
          29,
          6374,
          363,
          -94,
          92,
          222,
          249,
          1450,
          2290,
          0,
          1026,
          3953,
          1426,
          53,
          160,
          171,
          3237,
          192,
          0,
          385,
          2591,
          39,
          44,
          5432,
          349,
          20,
          381,
          4025,
          167,
          278,
          263,
          4968,
          581,
          682,
          779,
          2139,
          5888,
          0,
          3431,
          188,
          3214,
          873,
          -32,
          1958,
          76,
          1013,
          2731,
          93,
          173,
          384,
          491,
          728,
          411,
          265,
          -3,
          586,
          -196,
          700,
          -67,
          1600,
          508,
          486,
          243,
          203,
          1944,
          882,
          2924,
          20,
          1417,
          340,
          4062,
          133,
          0,
          8876,
          423,
          58,
          5,
          175,
          1349,
          6596,
          1741,
          25,
          0,
          4374,
          16,
          574,
          615,
          0,
          395,
          678,
          25,
          626,
          165,
          746,
          376,
          -55,
          0,
          21,
          3003,
          92,
          511,
          6904,
          795,
          0,
          166,
          157,
          0,
          0,
          174,
          1536,
          1114,
          816,
          1202,
          600,
          6,
          2655,
          499,
          0,
          197,
          289,
          5336,
          434,
          691,
          -250,
          3696,
          2595,
          1077,
          501,
          21,
          797,
          1659,
          97,
          2,
          384,
          63,
          1327,
          284,
          3395,
          2038,
          15,
          1187,
          3047,
          11650,
          1387,
          0,
          1387,
          446,
          275,
          1655,
          196,
          0,
          1327,
          0,
          206,
          523,
          0,
          444,
          1401,
          205,
          3957,
          9645,
          620,
          1541,
          -37,
          -601,
          197,
          -276,
          824,
          636,
          19,
          641,
          2983,
          0,
          252,
          119,
          171,
          0,
          -249,
          455,
          10,
          529,
          1859,
          613,
          751,
          0,
          1471,
          7503,
          68,
          530,
          1319,
          479,
          360,
          2163,
          136,
          -196,
          4087,
          1354,
          2,
          2337,
          514,
          6212,
          424,
          499,
          8,
          1305,
          300,
          976,
          2700,
          -497,
          2424,
          -2049,
          2081,
          189,
          0,
          327,
          426,
          1291,
          3473,
          434,
          105,
          975,
          -479,
          62,
          0,
          0,
          1202,
          2854,
          9,
          203,
          0,
          2467,
          3850,
          47,
          1895,
          522,
          -918,
          -193,
          48,
          0,
          201,
          598,
          1506,
          26,
          1582,
          559,
          0,
          -411,
          36,
          512,
          786,
          3658,
          -227,
          423,
          947,
          324,
          0,
          1694,
          2894,
          2693,
          644,
          185,
          1583,
          2399,
          1322,
          194,
          1221,
          -302,
          256,
          0,
          1335,
          682,
          0,
          3834,
          5312,
          4095,
          0,
          -1386,
          525,
          2,
          0,
          1500,
          378,
          552,
          -84,
          -240,
          501,
          5345,
          1559,
          0,
          0,
          1989,
          598,
          94,
          3473,
          349,
          3876,
          1934,
          2,
          9,
          11,
          60,
          882,
          499,
          198,
          2093,
          74,
          1220,
          1342,
          6134,
          3680,
          605,
          104,
          683,
          48,
          15,
          6714,
          623,
          234,
          599,
          9916,
          3160,
          0,
          2306,
          2550,
          701,
          3244,
          402,
          6014,
          -81,
          898,
          863,
          235,
          12,
          298,
          3463,
          13338,
          391,
          569,
          54,
          875,
          631,
          3434,
          1610,
          505,
          11528,
          102,
          -581,
          0,
          7298,
          0,
          909,
          337,
          829,
          301,
          147,
          282,
          1294,
          0,
          995,
          532,
          785,
          3028,
          1022,
          0,
          792,
          490,
          417,
          2071,
          1187,
          280,
          1236,
          910,
          -994,
          1333,
          850,
          3773,
          460,
          3140,
          1028,
          4264,
          3932,
          -1,
          1682,
          814,
          819,
          1265,
          514,
          1764,
          275,
          592,
          1143,
          546,
          1164,
          1381,
          0,
          659,
          192,
          -12,
          186,
          2246,
          124,
          2087,
          1298,
          101,
          46,
          262,
          0,
          17924,
          167,
          967,
          0,
          1547,
          388,
          1,
          0,
          80,
          4168,
          0,
          781,
          804,
          993,
          34,
          0,
          520,
          567,
          7177,
          3629,
          2199,
          223,
          185,
          5809,
          210,
          -199,
          897,
          0,
          7378,
          593,
          1224,
          3031,
          708,
          542,
          1141,
          -42,
          -556,
          482,
          1501,
          1850,
          2266,
          3283,
          300,
          1,
          178,
          187,
          229,
          42,
          -367,
          8,
          256,
          2410,
          2976,
          -426,
          39,
          1428,
          2030,
          -1636,
          108,
          2094,
          47,
          618,
          162,
          -671,
          136,
          6958,
          953,
          821,
          131,
          1272,
          -306,
          3201,
          1114,
          4168,
          0,
          0,
          3309,
          1377,
          150,
          553,
          458,
          565,
          0,
          2278,
          210,
          15578,
          236,
          1704,
          1693,
          372,
          1147,
          3166,
          1074,
          23,
          1431,
          814,
          142,
          810,
          1604,
          0,
          488,
          4110,
          1746,
          -208,
          152,
          0,
          399,
          318,
          9072,
          3071,
          177,
          10786,
          0,
          879,
          1887,
          3331,
          3,
          7668,
          55,
          0,
          2998,
          524,
          106,
          85,
          249,
          10,
          523,
          1321,
          -194,
          4517,
          964,
          923,
          2677,
          873,
          -1050,
          361,
          1466,
          782,
          3576,
          3,
          -112,
          759,
          195,
          15,
          -1053,
          488,
          287,
          578,
          1509,
          3794,
          4324,
          793,
          943,
          0,
          19447,
          565,
          1541,
          1079,
          0,
          1310,
          98,
          5,
          0,
          9216,
          2551,
          0,
          154,
          -150,
          427,
          664,
          0,
          680,
          4,
          196,
          290,
          221,
          180,
          -139,
          79,
          236,
          1120,
          -2282,
          2200,
          -461,
          385,
          2291,
          54,
          2850,
          1235,
          360,
          1352,
          905,
          138,
          0,
          221,
          -254,
          1510,
          1757,
          0,
          934,
          0,
          1217,
          -57,
          82,
          438,
          729,
          0,
          374,
          -341,
          -367,
          592,
          457,
          4819,
          -85,
          467,
          -309,
          21522,
          467,
          870,
          -93,
          75,
          292,
          0,
          263,
          134,
          91,
          0,
          229,
          892,
          197,
          -213,
          -321,
          604,
          606,
          1518,
          192,
          0,
          289,
          535,
          33,
          243,
          0,
          1279,
          1327,
          297,
          740,
          278,
          553,
          406,
          5,
          2071,
          0,
          932,
          -119,
          381,
          1309,
          542,
          30,
          0,
          -15,
          318,
          1405,
          5271,
          -51,
          3914,
          4976,
          16,
          1595,
          306,
          215,
          0,
          2628,
          1778,
          1423,
          659,
          -31,
          -204,
          1462,
          723,
          3954,
          33,
          4151,
          0,
          219,
          2728,
          76,
          4388,
          -210,
          2427,
          45,
          -74,
          2173,
          8422,
          120,
          1320,
          245,
          9569,
          0,
          2770,
          1221,
          298,
          967,
          0,
          0,
          1368,
          751,
          2597,
          -35,
          629,
          1330,
          277,
          1018,
          11,
          0,
          905,
          15474,
          54,
          -246,
          0,
          1951,
          65,
          368,
          51,
          3492,
          21,
          8403,
          978,
          165,
          350,
          385,
          2058,
          -11,
          505,
          -547,
          562,
          4222,
          105,
          0,
          66,
          0,
          13578,
          11222,
          1737,
          59,
          54,
          2282,
          299,
          396,
          184,
          475,
          -496,
          117,
          713,
          45,
          1496,
          0,
          886,
          4592,
          -38,
          431,
          -292,
          2571,
          191,
          1689,
          8494,
          159,
          7,
          35,
          1007,
          106,
          0,
          0,
          216,
          746,
          764,
          193,
          368,
          205,
          96,
          8016,
          221,
          157,
          4830,
          1039,
          71,
          1214,
          0,
          0,
          116,
          100,
          91,
          -398,
          2820,
          223,
          2278,
          -782,
          8919,
          104,
          3387,
          0,
          449,
          4787,
          1240,
          191,
          2185,
          0,
          2857,
          3782,
          83,
          6,
          88,
          294,
          84,
          84,
          602,
          0,
          -470,
          -37,
          2145,
          937,
          2059,
          3598,
          1206,
          1273,
          46,
          534,
          -300,
          4,
          386,
          -396,
          0,
          14930,
          -600,
          3733,
          465,
          994,
          758,
          256,
          3461,
          124,
          355,
          1514,
          40,
          -320,
          566,
          388,
          577,
          17,
          260,
          0,
          1823,
          1463,
          3390,
          2769,
          215,
          12531,
          80,
          317,
          217,
          4874,
          275,
          738,
          1839,
          556,
          207,
          -461,
          789,
          1133,
          4253,
          255,
          582,
          743,
          570,
          786,
          295,
          2583,
          1285,
          287,
          658,
          246,
          3463,
          291,
          19,
          5700,
          0,
          -346,
          -456,
          5953,
          1693,
          350,
          -181,
          1735,
          157,
          67,
          -444,
          0,
          171,
          262,
          0,
          965,
          0,
          6392,
          194,
          2308,
          941,
          46,
          131,
          1844,
          281,
          109,
          268,
          298,
          -525,
          738,
          94,
          7702,
          857,
          -354,
          0,
          112,
          5735,
          1807,
          -30,
          216,
          127,
          1096,
          5193,
          6,
          339,
          0,
          625,
          3776,
          1239,
          93,
          304,
          1409,
          191,
          79,
          2263,
          10438,
          81,
          964,
          0,
          3839,
          586,
          600,
          1343,
          113,
          240,
          194,
          278,
          260,
          1013,
          0,
          2363,
          913,
          686,
          149,
          500,
          11766,
          137,
          0,
          335,
          16,
          0,
          9851,
          525,
          788,
          1016,
          292,
          999,
          2870,
          689,
          -100,
          1084,
          154,
          1653,
          2378,
          1515,
          1636,
          1516,
          127,
          2876,
          1384,
          228,
          2481,
          475,
          1401,
          425,
          244,
          -6847,
          0,
          386,
          332,
          1367,
          120,
          19,
          3,
          -98,
          303,
          -752,
          97,
          794,
          1880,
          732,
          5261,
          712,
          28,
          0,
          827,
          110,
          377,
          -344,
          261,
          104,
          610,
          4328,
          1371,
          193,
          258,
          56,
          136,
          1222,
          3672,
          2188,
          924,
          11431,
          140,
          27,
          -468,
          438,
          -43,
          1161,
          458,
          -441,
          2278,
          0,
          702,
          69,
          182,
          318,
          -181,
          261,
          244,
          880,
          2283,
          550,
          7752,
          6,
          4471,
          1969,
          219,
          76,
          60,
          8,
          19,
          20011,
          608,
          1068,
          -269,
          494,
          159,
          -754,
          157,
          2110,
          2603,
          992,
          593,
          327,
          6766,
          432,
          25,
          5299,
          291,
          517,
          97,
          1636,
          996,
          0,
          710,
          1049,
          556,
          629,
          186,
          0,
          1264,
          64,
          2497,
          -97,
          161,
          732,
          94,
          2404,
          21,
          2658,
          131,
          0,
          351,
          564,
          2240,
          46,
          132,
          71,
          0,
          267,
          841,
          51,
          7735,
          417,
          -14,
          2490,
          0,
          184,
          19343,
          947,
          90,
          10218,
          72,
          120,
          130,
          41,
          0,
          673,
          -131,
          2607,
          5,
          23,
          180,
          68,
          0,
          565,
          598,
          1609,
          1047,
          5641,
          557,
          1066,
          35,
          54,
          1806,
          39,
          0,
          930,
          0,
          2764,
          4492,
          2166,
          0,
          1575,
          201,
          318,
          1682,
          242,
          291,
          304,
          108,
          1303,
          292,
          89,
          788,
          573,
          1318,
          151,
          -37,
          522,
          2315,
          -718,
          4,
          -43,
          596,
          670,
          430,
          60,
          0,
          678,
          159,
          1345,
          483,
          2443,
          1258,
          389,
          0,
          2067,
          332,
          80,
          84,
          2702,
          37127,
          0,
          1813,
          1612,
          198,
          1106,
          2321,
          33,
          101,
          -59,
          486,
          136,
          193,
          5083,
          0,
          8,
          21,
          570,
          644,
          1104,
          6004,
          1503,
          896,
          177,
          665,
          380,
          151,
          179,
          4,
          116,
          0,
          -57,
          894,
          478,
          10,
          30,
          578,
          2360,
          -47,
          -805,
          782,
          1282,
          766,
          348,
          296,
          908,
          0,
          5276,
          658,
          1089,
          1042,
          544,
          134,
          73,
          13156,
          30,
          -66,
          1373,
          8564,
          81,
          5426,
          16,
          297,
          734,
          1514,
          153,
          3503,
          -849,
          1441,
          2487,
          927,
          598,
          345,
          197,
          220,
          9,
          5678,
          0,
          169,
          388,
          61,
          7100,
          219,
          -76,
          1432,
          879,
          0,
          1498,
          543,
          1797,
          2994,
          1690,
          1573,
          0,
          173,
          989,
          335,
          17,
          -74,
          8319,
          15341,
          52,
          -56,
          1670,
          370,
          466,
          2583,
          1177,
          311,
          842,
          2744,
          503,
          62,
          4841,
          253,
          842,
          378,
          637,
          -372,
          135,
          7,
          446,
          1328,
          252,
          125,
          1324,
          -244,
          657,
          2140,
          5571,
          103,
          2133,
          8876,
          22,
          3854,
          32,
          337,
          228,
          1125,
          1612,
          1310,
          -210,
          0,
          975,
          306,
          170,
          154,
          1972,
          994,
          380,
          54,
          512,
          1027,
          1770,
          13,
          438,
          162,
          -183,
          698,
          165,
          3499,
          378,
          855,
          32,
          0,
          0,
          2994,
          1238,
          1717,
          224,
          1048,
          592,
          396,
          268,
          0,
          5223,
          1611,
          780,
          399,
          1649,
          974,
          22,
          507,
          10787,
          -259,
          414,
          67,
          328,
          4,
          471,
          3038,
          2276,
          -848,
          2662,
          2597,
          550,
          357,
          1153,
          4912,
          194,
          0,
          565,
          649,
          -45,
          1310,
          2308,
          4,
          1297,
          1808,
          2080,
          5034,
          322,
          679,
          1742,
          154,
          140,
          377,
          3644,
          -50,
          453,
          45,
          46,
          2060,
          1316,
          -361,
          349,
          -68,
          -157,
          320,
          86,
          681,
          0,
          4567,
          40,
          706,
          420,
          0,
          65,
          -27,
          114,
          678,
          3105,
          281,
          1932,
          481,
          156,
          0,
          557,
          1467,
          428,
          994,
          769,
          108,
          19,
          199,
          448,
          473,
          726,
          0,
          536,
          2194,
          1406,
          44,
          -6,
          1631,
          4054,
          75,
          172,
          356,
          1508,
          49,
          544,
          -383,
          606,
          -410,
          469,
          2,
          241,
          0,
          322,
          2193,
          144,
          0,
          3428,
          10,
          1841,
          84,
          622,
          309,
          154,
          1949,
          2605,
          5163,
          557,
          119,
          0,
          356,
          1395,
          20,
          84,
          609,
          1856,
          608,
          0,
          1859,
          282,
          154,
          222,
          897,
          131,
          0,
          37,
          1704,
          314,
          399,
          300,
          382,
          1301,
          0,
          930,
          21,
          111,
          1199,
          1085,
          230,
          558,
          127,
          4844,
          826,
          30,
          2708,
          108,
          577,
          2764,
          2834,
          -324,
          4,
          559,
          271,
          5774,
          816,
          3,
          274,
          11265,
          70,
          405,
          22755,
          557,
          1554,
          89,
          444,
          4951,
          30,
          1,
          0,
          0,
          1093,
          1561,
          1861,
          333,
          342,
          388,
          1515,
          654,
          -467,
          398,
          0,
          12848,
          5205,
          -181,
          250,
          110,
          445,
          175,
          458,
          5127,
          928,
          105,
          18558,
          375,
          133,
          2278,
          443,
          23592,
          61,
          192,
          628,
          372,
          222,
          2469,
          459,
          254,
          563,
          -29,
          494,
          781,
          2793,
          1134,
          4568,
          0,
          1189,
          692,
          716,
          520,
          2133,
          0,
          70,
          872,
          2458,
          -30,
          409,
          60,
          667,
          620,
          -60,
          237,
          373,
          1221,
          0,
          999,
          67,
          -463,
          7934,
          816,
          2557,
          546,
          78,
          24450,
          765,
          91,
          703,
          589,
          0,
          1755,
          269,
          432,
          4382,
          109,
          52,
          922,
          1779,
          8,
          -271,
          0,
          14,
          215,
          307,
          265,
          323,
          800,
          392,
          406,
          104,
          1453,
          1168,
          766,
          1784,
          3,
          1412,
          -312,
          167,
          1239,
          776,
          -13,
          4519,
          5769,
          414,
          958,
          922,
          1585,
          910,
          3738,
          1387,
          -3,
          144,
          103,
          0,
          -547,
          9326,
          1281,
          267,
          0,
          512,
          3531,
          4358,
          222,
          22,
          37,
          591,
          12,
          131,
          228,
          1794,
          -34,
          416,
          1170,
          0,
          466,
          1463,
          6835,
          -397,
          5222,
          0,
          526,
          2422,
          430,
          20,
          -211,
          1337,
          4798,
          1077,
          9713,
          5547,
          2433,
          105,
          275,
          366,
          -315,
          -55,
          0,
          2993,
          3715,
          -8,
          1577,
          588,
          0,
          944,
          9,
          144,
          1222,
          0,
          668,
          568,
          0,
          -861,
          877,
          1107,
          757,
          30,
          287,
          3079,
          3238,
          1012,
          203,
          2880,
          486,
          1708,
          10635,
          1728,
          67,
          387,
          56,
          18,
          0,
          174,
          162,
          -16,
          113,
          2940,
          1954,
          182,
          247,
          -58,
          1685,
          410,
          -367,
          6835,
          2398,
          -312,
          2079,
          573,
          399,
          57,
          641,
          1575,
          -475,
          128,
          305,
          -80,
          198,
          1170,
          683,
          2312,
          28,
          -15,
          476,
          0,
          217,
          157,
          83,
          1871,
          11016,
          4023,
          -57,
          91,
          506,
          161,
          633,
          41,
          -375,
          -284,
          0,
          0,
          2413,
          134,
          127,
          -253,
          659,
          314,
          6362,
          33,
          392,
          454,
          23,
          2034,
          185,
          0,
          738,
          0,
          862,
          1256,
          371,
          2503,
          393,
          557,
          61,
          2991,
          557,
          173,
          80,
          3611,
          -482,
          2982,
          458,
          11,
          0,
          328,
          111,
          2223,
          37,
          -15,
          9051,
          284,
          183,
          109,
          934,
          93,
          811,
          753,
          538,
          1098,
          3086,
          2054,
          2501,
          209,
          972,
          3,
          326,
          3323,
          4121,
          13849,
          630,
          624,
          7503,
          0,
          2850,
          672,
          492,
          778,
          106,
          4929,
          0,
          545,
          1728,
          1401,
          1943,
          11,
          -170,
          239,
          272,
          113,
          -120,
          -388,
          51,
          1689,
          108,
          25290,
          1,
          -22,
          167,
          1796,
          677,
          593,
          316,
          855,
          3096,
          325,
          1023,
          991,
          876,
          1271,
          165,
          578,
          2595,
          8226,
          657,
          0,
          1875,
          254,
          1252,
          790,
          656,
          329,
          83,
          138,
          0,
          39,
          426,
          693,
          2854,
          787,
          102,
          855,
          1675,
          1996,
          473,
          1,
          1064,
          215,
          138,
          221,
          720,
          51,
          0,
          606,
          6551,
          -294,
          64,
          169,
          230,
          2485,
          209,
          71,
          137,
          2547,
          1800,
          6512,
          1476,
          534,
          1696,
          219,
          557,
          1277,
          -30,
          175,
          1595,
          723,
          297,
          310,
          1012,
          -103,
          621,
          548,
          182,
          484,
          1342,
          0,
          66,
          21,
          608,
          5345,
          461,
          5801,
          0,
          78,
          -1415,
          30,
          100,
          1371,
          62,
          1005,
          517,
          0,
          382,
          468,
          178,
          211,
          57,
          114,
          1222,
          1660,
          -9,
          0,
          30,
          0,
          369,
          1571,
          -187,
          0,
          -257,
          2459,
          1219,
          5784,
          162,
          1909,
          -195,
          4,
          350,
          87,
          777,
          3140,
          1510,
          756,
          1086,
          339,
          205,
          2717,
          138,
          1943,
          744,
          79,
          2400,
          419,
          414,
          27,
          2823,
          4692,
          1261,
          0,
          0,
          483,
          7369,
          804,
          2644,
          175,
          0,
          272,
          252,
          4053,
          2158,
          299,
          2731,
          50,
          36,
          220,
          5746,
          0,
          238,
          76,
          679,
          1390,
          418,
          2891,
          497,
          2933,
          948,
          0,
          0,
          687,
          -230,
          2575,
          7649,
          59,
          23495,
          365,
          97,
          5,
          739,
          -177,
          -454,
          1267,
          1639,
          900,
          -481,
          528,
          34,
          223,
          218,
          130,
          1156,
          314,
          1374,
          22,
          1129,
          130,
          -202,
          293,
          275,
          9224,
          515,
          760,
          2302,
          -389,
          133,
          1580,
          1020,
          298,
          13901,
          563,
          -27,
          2113,
          980,
          352,
          96,
          3842,
          3870,
          589,
          7426,
          1,
          6101,
          231,
          876,
          86,
          363,
          8298,
          0,
          93,
          596,
          168,
          29,
          6,
          -934,
          297,
          533,
          175,
          -292,
          2,
          39,
          396,
          -338,
          2050,
          510,
          477,
          4176,
          0,
          0,
          297,
          167,
          -353,
          1525,
          211,
          416,
          0,
          1001,
          75,
          7602,
          0,
          163,
          673,
          -154,
          3723,
          559,
          13774,
          403,
          1696,
          722,
          296,
          257,
          418,
          488,
          900,
          2945,
          -327,
          115,
          218,
          286,
          293,
          201,
          4331,
          388,
          483,
          502,
          306,
          -337,
          950,
          2410,
          -633,
          0,
          265,
          698,
          459,
          0,
          1307,
          8785,
          302,
          484,
          7,
          1539,
          3098,
          745,
          2050,
          2766,
          4641,
          1167,
          322,
          750,
          1559,
          0,
          0,
          46,
          0,
          1862,
          7831,
          21614,
          2137,
          3247,
          14,
          0,
          637,
          10655,
          94,
          997,
          2084,
          1734,
          805,
          317,
          369,
          100,
          -44,
          3296,
          -50,
          -411,
          922,
          1128,
          5423,
          1537,
          0,
          519,
          186,
          83,
          1365,
          1734,
          51,
          1567,
          96,
          8982,
          -209,
          -6,
          355,
          0,
          0,
          1310,
          2520,
          1629,
          573,
          1655,
          26,
          186,
          13658,
          23,
          87,
          49,
          3810,
          56,
          384,
          8,
          2671,
          3240,
          147,
          63,
          7863,
          946,
          237,
          0,
          245,
          315,
          1139,
          0,
          -102,
          3984,
          1988,
          125,
          505,
          834,
          494,
          947,
          262,
          141,
          -236,
          2555,
          298,
          1179,
          31,
          357,
          1129,
          861,
          16,
          895,
          504,
          2955,
          122,
          2204,
          111,
          1566,
          661,
          2485,
          3756,
          43,
          1529,
          84,
          311,
          408,
          1866,
          42,
          260,
          2994,
          0,
          1133,
          2305,
          143,
          4721,
          5366,
          71,
          1437,
          1093,
          817,
          196,
          205,
          0,
          1002,
          2916,
          4513,
          7,
          0,
          253,
          28,
          -65,
          376,
          1018,
          260,
          5539,
          758,
          262,
          236,
          396,
          1857,
          4722,
          935,
          4567,
          220,
          8,
          109,
          0,
          456,
          5553,
          137,
          299,
          162,
          1533,
          301,
          1697,
          441,
          898,
          945,
          0,
          1830,
          6138,
          442,
          0,
          1556,
          83,
          430,
          -336,
          528,
          111,
          1556,
          408,
          1067,
          277,
          257,
          0,
          2549,
          4612,
          433,
          680,
          3851,
          442,
          642,
          487,
          171,
          13711,
          187,
          101,
          343,
          315,
          205,
          106,
          477,
          2875,
          1000,
          -825,
          534,
          452,
          623,
          310,
          454,
          2306,
          -126,
          399,
          0,
          2109,
          0,
          -470,
          81,
          1104,
          457,
          0,
          941,
          -466,
          2600,
          3,
          90,
          2570,
          181,
          66653,
          556,
          110,
          9,
          0,
          5249,
          2499,
          276,
          3343,
          9994,
          613,
          925,
          393,
          339,
          1216,
          1818,
          2109,
          46,
          980,
          3060,
          46,
          274,
          805,
          3151,
          160,
          0,
          694,
          509,
          237,
          0,
          16,
          504,
          -274,
          0,
          0,
          2565,
          91,
          558,
          1032,
          351,
          231,
          3706,
          5,
          342,
          588,
          3165,
          0,
          464,
          837,
          34230,
          5435,
          102,
          641,
          334,
          149,
          179,
          61,
          375,
          2088,
          29,
          3839,
          2910,
          3770,
          141,
          0,
          65,
          -306,
          3740,
          0,
          131,
          23189,
          -1,
          6979,
          187,
          1203,
          6298,
          -243,
          181,
          661,
          270,
          1919,
          492,
          91,
          775,
          2669,
          2605,
          1277,
          568,
          2344,
          4321,
          1064,
          241,
          183,
          9214,
          513,
          14850,
          279,
          4,
          -298,
          522,
          0,
          8,
          432,
          -378,
          338,
          2878,
          62,
          1303,
          255,
          0,
          1125,
          1238,
          370,
          2417,
          4,
          444,
          28,
          2139,
          194,
          195,
          0,
          189,
          220,
          189,
          746,
          13054,
          0,
          23421,
          41,
          7408,
          101,
          384,
          2556,
          808,
          0,
          1158,
          334,
          296,
          1653,
          1283,
          2295,
          -42,
          55,
          365,
          772,
          334,
          1060,
          38,
          146,
          439,
          124,
          190,
          211,
          871,
          1513,
          871,
          1298,
          360,
          597,
          457,
          2457,
          2069,
          47,
          13,
          -71,
          27,
          -114,
          2645,
          1025,
          2044,
          10,
          0,
          644,
          4293,
          -238,
          -613,
          1573,
          1728,
          15,
          -160,
          312,
          33,
          80,
          445,
          4565,
          3262,
          559,
          4840,
          -302,
          73,
          4790,
          1078,
          284,
          493,
          100,
          1042,
          3665,
          -9,
          576,
          945,
          108,
          93,
          -97,
          3768,
          8,
          66,
          0,
          -637,
          0,
          26,
          4726,
          1782,
          3430,
          332,
          554,
          0,
          542,
          393,
          134,
          178,
          0,
          23,
          0,
          202,
          647,
          143,
          1928,
          1630,
          102,
          0,
          1790,
          0,
          1675,
          -745,
          -56,
          0,
          2396,
          223,
          5956,
          122,
          4888,
          183,
          0,
          -262,
          169,
          3364,
          15,
          126,
          1953,
          1385,
          261,
          178,
          1819,
          0,
          6337,
          214,
          0,
          1153,
          415,
          101,
          1490,
          779,
          2036,
          333,
          963,
          4709,
          1074,
          90,
          243,
          414,
          0,
          2196,
          1416,
          281,
          518,
          3,
          36,
          9301,
          883,
          21,
          3091,
          2791,
          217,
          1579,
          0,
          314,
          655,
          212,
          523,
          111,
          1166,
          582,
          513,
          75,
          1992,
          2910,
          66,
          599,
          89,
          796,
          571,
          215,
          1088,
          271,
          11494,
          2741,
          1694,
          383,
          298,
          155,
          694,
          179,
          356,
          0,
          179,
          244,
          46,
          246,
          7,
          1746,
          992,
          4312,
          103,
          506,
          461,
          268,
          -454,
          365,
          2,
          1667,
          2265,
          4622,
          0,
          188,
          0,
          592,
          -276,
          1254,
          652,
          192,
          148,
          1812,
          3478,
          264,
          71,
          1376,
          668,
          703,
          -25,
          48,
          2453,
          0,
          133,
          277,
          78,
          517,
          1,
          7944,
          3,
          623,
          290,
          8918,
          41,
          199,
          100,
          313,
          419,
          0,
          0,
          437,
          1357,
          1610,
          0,
          709,
          10086,
          0,
          -311,
          9324,
          7832,
          191,
          6637,
          0,
          172,
          1131,
          797,
          120,
          215,
          425,
          476,
          29,
          527,
          0,
          1093,
          70,
          39,
          296,
          1464,
          2226,
          1286,
          -738,
          3643,
          5260,
          3096,
          -312,
          286,
          89,
          -72,
          4305,
          1487,
          0,
          -78,
          3495,
          3317,
          1906,
          2996,
          446,
          1590,
          5214,
          134,
          8,
          3120,
          171,
          39,
          171,
          -140,
          728,
          1637,
          -143,
          802,
          795,
          265,
          2567,
          3402,
          1336,
          306,
          10984,
          526,
          563,
          0,
          0,
          3107,
          0,
          1146,
          5252,
          1195,
          488,
          1887,
          49,
          280,
          -461,
          633,
          3955,
          -233,
          1626,
          1103,
          223,
          -175,
          -3,
          0,
          3918,
          284,
          4577,
          876,
          3754,
          128,
          0,
          547,
          243,
          3571,
          130,
          -529,
          1636,
          12180,
          297,
          413,
          285,
          19,
          44,
          62,
          79,
          226,
          2388,
          98,
          48,
          255,
          2,
          643,
          13410,
          0,
          3443,
          2946,
          21,
          1212,
          2183,
          135,
          5903,
          271,
          303,
          7264,
          0,
          204,
          444,
          -27,
          287,
          800,
          4645,
          1481,
          1575,
          5691,
          294,
          154,
          3950,
          624,
          -504,
          111,
          1131,
          322,
          0,
          -89,
          3,
          1313,
          462,
          11,
          3163,
          2,
          3157,
          4348,
          -391,
          7816,
          5763,
          240,
          508,
          2,
          1654,
          860,
          154,
          281,
          427,
          467,
          91,
          1319,
          259,
          -488,
          60,
          405,
          177,
          -412,
          282,
          6181,
          373,
          -92,
          49,
          0,
          -283,
          1313,
          2452,
          517,
          350,
          1623,
          -435,
          333,
          3770,
          778,
          483,
          2170,
          541,
          205,
          2269,
          0,
          973,
          894,
          -12,
          0,
          5154,
          -32,
          999,
          24,
          0,
          771,
          1350,
          19,
          709,
          2040,
          303,
          -311,
          690,
          2551,
          -142,
          -448,
          99,
          18,
          37,
          3868,
          5041,
          260,
          67,
          157,
          1513,
          1044,
          113,
          245,
          505,
          92,
          3685,
          1636,
          0,
          349,
          3067,
          338,
          282,
          1190,
          73,
          384,
          -388,
          354,
          192,
          70,
          22,
          335,
          2775,
          592,
          1271,
          48,
          0,
          4869,
          737,
          83,
          1104,
          5,
          23,
          1137,
          181,
          8267,
          871,
          1824,
          4,
          421,
          561,
          1498,
          657,
          0,
          9192,
          7876,
          0,
          192,
          803,
          1694,
          5559,
          737,
          174
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "balance distribution in customer attrition "
        },
        "xaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "balance"
         },
         "zerolinewidth": 1
        },
        "yaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "percent"
         },
         "zerolinewidth": 1
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"da9664bd-1336-4e2c-b2a6-79e4446f017b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"da9664bd-1336-4e2c-b2a6-79e4446f017b\")) {                    Plotly.newPlot(                        \"da9664bd-1336-4e2c-b2a6-79e4446f017b\",                        [{\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Active\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [931, 95, 0, 4367, 68, 1308, 353, -40, 0, -312, 549, 8837, 1020, 3540, 663, 2590, 4982, 6, 118, -258, -221, 2596, 467, 3100, 722, 797, 476, 558, 162, 1842, 2348, 11, 2103, 967, 116, 2805, 1386, 0, 0, 90, 37, 7, 676, 3817, 144, 230, 0, 1109, 2925, 397, 865, 816, 396, 0, 241, 243, 233, 43, 290, 101, 1151, 45, 1410, 1234, 2767, 5969, -272, 1036, 9324, 1043, 1473, 183, 86, 0, 2388, 160, 385, 4, 119, 0, 742, 0, 1724, 0, 503, 68, 5914, 192, 15, 2157, 3494, 275, 318, 350, 588, 2008, 35, 2152, 122, 160, 1934, 0, 613, 574, 0, 341, 158, 1489, 2911, 1178, 96, 4157, -575, 3419, 1104, 57, 343, -326, 0, 3104, 648, 10041, -508, 327, 1150, 1074, 27, 3720, 92, 7845, 157, 428, 0, 54, 13, 3230, 1637, 6981, 1, 631, 7296, 3014, 920, 735, 1300, 541, 0, 1257, 4522, 585, 150, 202, 2137, 2674, 625, 73, 2656, 3, 0, 95, 106, 477, 1529, 28, 1743, 924, 457, 0, 736, 0, 787, -396, 1573, 0, 524, 1558, 159, 53, 838, 3144, 737, 4056, 162, 0, 217, 3324, 2895, 0, 11254, -280, 808, 0, 246, -972, 750, 2430, -179, 1412, -1531, 51, 88, -209, 799, 4048, 1310, 573, 6798, 1214, 75, 387, 2246, 3, 883, 1503, -322, 1301, 950, 2326, -78, 2405, 113, 4564, 387, 1865, 4105, 258, 19, 3773, 1808, 1304, 3702, 163, -151, 1602, 1314, 84, 1906, 291, 223, 207, 518, 241, -126, 421, 38, 800, 32, 1477, 160, 4415, -397, 590, 778, 19, 0, -28, 3119, 122, 0, 4157, 102, 437, 1, -354, 1234, 901, 247, 180, 274, 92, 813, 1170, -183, 0, 136, 103, 23, 2242, 137, 1047, 5731, 2707, -329, 158, 219, 2775, 461, 145, 309, 733, 8417, 1400, 72, 0, 9710, 1377, 881, 0, 285, 550, 285, -360, 0, 72, 930, 717, 67, 6227, 5252, 561, 1393, -330, 3261, 361, 807, 2305, -11, 2223, 29, 1308, 1337, 17361, 6, 370, 8, 666, 0, 1336, 17672, 35, 705, 2087, 4466, 844, 321, 2781, 34, 29, 2443, 3975, 256, 696, 63, 316, -119, 6570, 418, 0, 8784, 1970, 204, 643, 304, 1177, 544, -498, 0, 209, 665, 310, 2149, 13818, 526, -218, 419, 517, 61, 0, 514, 365, 480, 717, 24780, 1396, 414, 3100, 0, 1218, 789, 859, 0, 3868, 232, 1686, 29184, 144, 194, 7641, 351, 150, 1532, 1549, 1057, 839, 880, 4464, 6, 0, 694, 153, 110, 640, 3060, 2836, 1127, 11, 982, 222, 627, 5010, 468, 155, 2478, 668, 35, 3304, 0, 237, 0, -54, 0, 240, 759, 992, 1379, 8564, 0, 119, 450, 746, 2032, 636, -938, 2007, 33, 2887, 1444, 42, 522, 0, 3884, 472, 2060, 628, 398, 417, 610, 2585, 321, 0, 4, 29, 892, 0, 1387, 454, 71, -35, 2251, 2460, 34, 1281, 3021, 141, 500, 1459, -244, 1356, -289, 1396, 9, 70, 877, 113, 368, 89, 450, 463, 1397, 310, 270, 65, 865, 7, 145, 327, 429, 1167, 2995, 974, 784, 316, 0, 211, 342, 628, 0, 769, 2278, 1351, 49, 1082, 5691, 916, 325, 0, 2475, 3717, 288, 1130, 2715, 23, 0, -84, 290, 36, -488, -473, 1272, 0, -6, 510, 818, 543, 7867, 1212, 533, 291, 573, 1279, 280, 574, 658, 2019, 22815, 887, 303, 326, 453, 0, 4063, 2403, 123, 270, 424, 1230, 387, 744, 696, 4594, 1002, -173, 1239, 354, 375, 3, 6116, 4515, 727, 3846, 1128, 1173, 27, 328, 0, -87, 2830, 4, 0, 80, 2768, 5, 553, -102, 2079, 1, 598, 34, -19, 292, 144, 1113, 121, 904, -498, 0, 67, 3967, 305, 260, 258, 0, 72, 145, 2564, 7811, 5744, 673, 602, 9374, 5016, 508, 156, 0, 12, 7984, 255, 211, 1854, -497, 431, -314, 552, 465, 544, -636, 7780, 274, 1148, 67, 914, 1293, 111, 937, 605, 1457, 1617, 29207, 812, 324, 1327, 482, 134, 347, 1824, 410, -38, 581, 3100, 0, 1138, 0, 304, 313, 1898, 142, 4696, 135, 574, 1322, 703, 0, 1, 55, 786, 22, 890, 451, 59, 15, 15341, -273, 28, 58, 103, 244, 0, 880, -141, 447, 62, 148, 79, 9, 578, -382, 888, 382, 1390, -104, 22, -36, 130, 77, 1031, 92, 1063, 0, 282, 1394, 16, 975, 12276, 313, 1648, 6, 1127, 1070, 1040, 50, 62, -755, 4707, 925, 0, 867, -201, 120, 61, 0, 687, 38, 2806, -27, 2498, 390, -423, 1587, 288, 822, -222, 197, 159, 681, 0, 495, 414, 151, 2572, 320, 304, 1299, 740, 537, 1581, 708, 178, 671, 3, 981, 1042, 1021, 1035, 1114, 98, 24, 486, 727, -205, 895, 319, 206, -414, -1489, 0, 249, 927, 515, 1060, 308, 808, 431, 396, -89, -253, 0, 60, 695, 1141, 791, 1381, 0, -28, 412, 310, 1319, 0, -314, 511, -152, 907, 22, 0, 8044, 1854, 1794, 2, 2321, 144, 1, 762, 16, 20, 2070, 308, 1617, 39, 332, 713, 2885, 968, 4150, 687, 333, 1821, -436, 6929, 126, 2019, 878, 6791, 0, 110, 28, 443, 236, 2707, 1753, 1182, 427, 788, 0, -509, -110, 244, 933, 940, 1631, -505, 7336, -40, 25, 0, 5, -92, 497, 2356, 3, 128, 390, 300, 827, 25, 340, 867, 140, 3, 4920, 3237, 0, 1096, 869, 1900, 413, -588, 0, 2490, 20928, 560, 0, 826, 115, 107, 320, -399, 703, 328, 148, 2298, 64, 4013, 0, 633, -1137, 902, 2850, 313, 923, 8, 945, 2261, 18, 5828, 6307, 537, 393, 215, 132, 35, 125, 129, 0, 315, 2567, 883, 1120, 0, 1117, 773, 390, 1358, 216, 317, 2369, 125, 6, 15841, 51, 0, -79, 1881, 456, 1873, 0, 445, 2123, 767, 229, 3783, 140, 6, 859, 261, 50, -462, 0, 283, 91, 0, 442, 0, 1182, 316, 316, 402, 8725, 102, 0, 615, 0, 3060, 990, 1937, 428, 655, 16, 2044, 20, 29, -155, 340, 1776, 23, 863, 7051, 1049, 174, 137, 783, 116, 2118, 10269, 141, 13, 0, 1996, 1129, 561, 539, -349, 6971, 497, 362, 4497, 2496, 2850, 0, 1199, 66, 3756, 1170, 944, 50, 171, 122, 1780, 136, 179, 232, 82, 630, 1202, 867, 6200, -422, 426, 107, -96, 581, 402, 555, 67, 72, 1942, 373, 84, 394, 925, 0, 6690, 4178, 20723, 887, 598, 1, 4314, 2155, 760, 1248, 5122, 191, 230, 7010, 1970, 3935, 437, 2323, 2213, 84, 4545, 200, 206, 2937, 221, 1521, 102, 519, 660, 500, 1661, 961, -273, 7067, 285, 221, 479, 1595, 65, 640, 490, 1440, 918, 6507, 4, 2289, 131, 1852, -749, 719, 4613, 664, 291, 24, 823, 1235, 1195, 21, 363, 0, 0, 136, 846, 0, 505, 3986, 11862, 145, 2037, 441, 105, 465, 320, -762, 316, 903, 123, 273, 1345, 1027, 1855, 1315, 59, 2682, 2682, 3164, 342, 1742, -106, 67, 430, -234, 0, -206, 656, 603, 1102, 117, -13, 1993, 5920, 881, 0, 953, 781, 1530, 2667, 5310, 1468, 0, 144, 247, 258, 595, 259, 2408, 1356, 292, 2384, 307, 292, 3131, 0, 2258, 1348, 741, 0, 741, 521, 5315, 1660, 163, 1163, 122, 1, 493, 0, 715, 558, 16992, 179, 198, 152, 2805, 6332, 2420, 580, 259, -451, 261, 323, 0, 235, 526, 4366, 0, 833, -13, 405, 3517, 699, 943, 794, 503, -700, 644, 335, 719, -195, 972, 5050, 232, 2406, 324, 168, 374, 658, -1, 280, 0, 0, 2064, 128, 0, 863, 50, 187, -614, 841, 206, 984, 201, 585, 8514, 728, 80, 1246, 818, 1815, 0, 210, 71, 1869, 1040, 920, 668, 36, -972, 382, -170, 5172, 1, 922, 3016, 0, 1101, 3578, 0, 0, -190, 185, 156, 503, 485, 392, 625, 468, 280, 181, 1401, 1349, 5737, 1187, 5231, 3185, 3228, 80, 2800, 10, 331, 223, 0, 35, 299, 240, 3534, 223, -194, 1545, 440, 2538, 337, -492, 53, 91, 316, 3196, 2697, 3, 244, 410, 159, 6432, -701, 24, 296, 1, 1965, 501, 3558, 177, 204, 46, 2047, 810, 1914, 56831, 386, 469, 69, -124, 600, 3842, -720, 1127, 423, -59, 0, 1489, 0, 1451, 452, 283, 16, 973, 1257, -1701, 567, -1, 3310, 115, 321, 71, 759, 23, 3297, 578, 4279, 234, 1137, 8669, 488, 1077, 4790, 256, 974, 231, 196, 2002, 218, 1011, 2761, 92, 10005, 1168, 168, 1710, 207, 18, 306, 8, -250, 449, 1654, 714, 1498, 695, -15, 14, -287, 0, -978, 0, 20, 700, -58, 841, 951, -63, 641, 195, 11, 120, 450, 8379, 206, 1544, 2990, 866, 1744, -382, 214, 147, 81, 202, 1314, -60, 0, -331, 1504, 2363, 773, 941, 2116, -971, 5, 454, 53, 2578, 854, 16397, 687, 1894, 0, 1150, 136, 7313, -239, 182, 1590, -565, 930, 1091, 640, 767, 480, 1363, 446, 167, 0, 4196, 725, 1716, -45, 410, -97, 12159, 395, 969, 86, 395, 0, 0, 829, -15, 135, 1585, -910, 8806, 43, 136, 1195, 11219, 2003, 494, 419, -82, 64, 14522, 3455, 1076, -261, 309, 655, 102, 213, 202, 0, 22, 179, 4769, -6, 19358, 5563, 6641, 747, 49, 287, 437, 1313, 597, 418, -41, 449, 3572, 2428, 177, 80, 1048, 3874, 5381, 17, 4040, 427, 385, 266, 1411, 7747, 2129, 318, 105, 2603, 443, -116, 285, 5092, 719, 0, 2095, -390, 108, 1134, 228, 1, 0, 1453, 28, 1388, 0, 0, 15, 22, 983, 2879, -553, 596, 1402, 223, 46, 0, 1636, 96, -295, 1, 60, 84, 4287, 1, 192, 1172, 105, 4708, -1, 802, 22, 3301, 2227, 2, 445, 241, 429, 11, 50, 2195, 1270, 2394, 3536, -4, -1965, 5879, 469, 502, 777, 1588, 376, 848, -393, 4660, 176, 3335, -167, 1591, 2064, 1310, 1221, -26, 337, 2440, 0, 25947, 1734, 4, 1032, 8, 3137, -522, 21, 455, 4, 1412, 141, 6619, 0, 0, 202, 2552, 276, 0, 86, 507, 602, 1492, 0, 2117, 71, 843, 229, 176, 13, 169, 58, 179, 50, 11854, 1022, 692, 1937, 26, 0, 900, 260, -634, 30, 1694, 167, 4299, 135, 40, 479, 1945, 1062, 0, 2406, 337, 413, 362, 284, -66, 922, 93, 693, 307, 427, 5024, 463, 4130, 190, 2682, 144, 1938, 90, 10465, 917, 1744, 1198, 371, 575, 123, 145, 312, 893, 105, 0, 186, -114, 228, 0, 279, 241, 2328, 728, 4380, 0, 5432, 222, 3832, 715, 6215, 2085, 1398, 463, -337, 1182, 94, 5916, 4556, 373, 170, 146, 1384, 595, 4903, 4037, 1746, 18, 1724, 1735, 0, 367, 105, 537, 0, 653, 487, 1434, 1683, 530, -287, 271, 1945, 57, 5632, 750, 3992, 11, 383, 598, 3178, 505, 73, 1436, 264, 1720, -25, 634, 648, 0, 314, 2, 2677, 855, 411, 4874, 397, 61, 177, 985, 767, 496, 459, 1340, 1187, 4023, 184, 688, 2064, 862, 137, 51439, 893, 607, 5521, 1966, 363, -49, 1533, 250, 407, 3549, 0, 1017, 2652, 2442, 0, 0, 665, -1139, 3396, 542, 1957, 304, 353, 13107, 1138, 0, 199, 383, 0, 1844, 42, 1788, 680, 517, 3, 17, -328, 1513, 603, 307, 22, 0, 283, 5215, 847, 5827, 0, -759, 267, 2503, 3630, 218, 1146, 167, 447, 10378, -2712, 191, 17, 2235, 292, 433, 364, 30, 335, 1287, 128, 133, -888, 280, 719, 1261, 1903, 202, 390, 103, 362, 907, 199, 4943, 6659, 730, 664, 442, 7162, 61, 2579, 1583, 224, 851, 2880, 1597, 0, 771, 30, 346, 184, 580, 913, 1967, 2929, 103, 661, 19, 258, 361, 1898, 0, 3579, 1265, 940, 854, 1464, 3748, 6839, -235, 12, 61, 0, 6269, 142, 24, 1267, 2346, 133, 16786, 400, 2033, 1938, 0, -164, 1241, 0, 214, 399, 1388, 805, 1716, 2228, 1515, 44, 683, -473, 4482, 911, 3771, 1119, -6, -663, 686, 112, 83, 418, 3840, 7628, -10, 3, 1433, 0, 756, 313, 4996, 74, 431, 522, 0, 1296, 435, 2128, -383, 1019, 1780, 431, 8097, 0, 138, 292, 606, 0, 26, 322, 336, 36935, 0, 928, 2, 2220, 1125, 503, 2920, 4319, 0, 1080, 36, 803, 8, 833, 3410, 605, 13342, 586, 559, 4910, 4527, -221, 242, 20, 797, 321, 313, 136, 0, 0, 54, 0, 373, 577, 0, 1296, 2642, 3237, 1191, 2278, -165, 178, 209, 3560, 490, 3499, 995, 1128, 163, 103, 70, 191, 1601, 287, 199, 177, 832, 493, 13578, -50, 292, 0, 0, 294, -558, -311, 688, -548, 94, 299, 6217, 494, 594, 0, 157, 183, 22867, 1884, 0, 446, 114, 217, 1783, 208, 1311, 2813, 783, 243, 5704, 2895, 377, 2, 153, -165, 2764, 6699, 7, 2590, 14, 2672, 881, 4793, 370, 6836, 859, 1940, 1033, 532, -321, 383, 865, 1352, -722, 1144, 794, 2061, 36, 734, 99, 879, 0, 4128, 0, 275, 6525, 9317, 350, 699, 906, 97, 115, 36, 1076, -56, 1504, 482, 208, 441, -2, 8278, 407, 197, 348, 101, 294, 3730, 3885, 4582, 403, 145, 834, 664, 299, 29, 124, 124, 0, 20, 31868, 599, 248, 0, 879, 34, 2548, 3, 1395, -196, 205, 1442, 393, -185, 1158, 1165, 10721, 1270, 16, 930, 0, 0, -175, 328, 0, 2556, 4969, 726, -1451, 221, 3090, 257, 139, 3168, 260, 4574, 109, 140, -45, 6507, 1291, -10, 0, -20, 309, 250, 370, 183, 8590, 1011, 1355, 1609, 75, 12, 3540, 8486, 20772, 3297, 9, -139, 381, 760, 722, 24, 148, 6691, 0, 787, 4576, 629, 1193, -132, 1438, -159, 2235, 10, 805, -79, 1664, 0, 690, 616, 200, 8585, 173, 808, 4822, 966, 141, 531, 1154, 68, 1499, 2908, -238, 1685, 0, 4344, 0, 1846, 4, 360, 205, 18777, 218, 312, 15, 0, 112, 9, -44, 1023, 3, 77, 0, 137, -232, 0, 139, 47, -58, 34, 590, 435, 441, 212, 621, 162, 4418, 522, -547, 0, 726, 435, 530, 106, 590, 2929, 147, 2000, 143, 193, 658, -365, 50, 356, 6718, 21, 3872, 820, 6102, 516, 0, 199, 584, 1449, 490, 919, 4243, 6445, 47, 390, 25, 2257, 677, 0, 209, 2743, 50, 0, 21, 21, 78, 1034, 6242, 714, 98, 971, 0, 1097, 0, 25, 604, 434, 784, 95, 616, 631, 67, 555, 184, 382, -974, -516, 0, 5, 428, 0, 54, 1054, 115, 2171, 350, 594, 276, -325, 1594, 2596, 115, 0, 247, 235, 2593, 2166, 130, 48, 189, 0, -134, 23, 0, 1, 3317, 355, 917, 390, 35, 523, 80, -72, 1, 733, 29, 0, 0]}, {\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Churn\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [2343, 45, 1270, 2476, 184, 0, 830, 545, 1, 5090, 100, 309, 199, 460, 703, 3837, 611, -8, 55, 168, 785, 2067, 388, -192, 381, 40, 22, 3, 307, 759, -1, 65, 82, 10, 390, 311, 414, 5, 119, 4, 1262, 1949, -395, 1165, 2240, 300, 3285, 3923, 1443, 24, 1618, 517, 1521, 2823, 1405, 1535, 1596, 1542, 3652, -1, 7180, 5291, 1384, -191, 320, 146, 341, -9, -306, 4580, 313, 10576, -233, 2453, 1364, 281, 94, 144, 246, 92, 163, 49, -416, 409, 363, 3706, 4393, 863, 695, 792, 1020, 863, 97, 754, 1040, 122, 880, 501, 4438, 0, 271, 102, 2, 4170, 85, 431, 982, 408, 4822, 1250, 216, 1207, 791, 849, 239, 1211, 599, 825, 2183, 4499, 1289, 4665, 3326, 783, 0, 994, 1354, 239, -311, 149, 1464, 5773, 278, 2910, 541, 1262, -538, 125, 0, 620, 316, 2287, 198, 460, 1145, -22, 685, 901, 351, 0, 274, -213, 97, 51, 314, 6840, 668, 54, 1242, 292, 665, 1058, 949, 606, 0, 404, 249, 3354, 0, 589, 12956, 873, 594, -20, 4692, 486, 1593, 7606, 226, 366, 16, 565, 19, 1082, 1713, 14481, 5724, 1451, -34, 3674, 698, 4136, 2656, 2904, 1004, 410, 0, 0, 62, 416, 2763, 2984, 143, 0, 696, 152, -78, 867, 953, 443, -1129, 415, 260, -411, 1279, 86, -754, 0, 178, 278, 425, 168, -36, 126, -87, 6281, 644, 377, -271, 5839, 492, 99, -713, 32, 543, 403, 437, 1772, -88, 0, 853, 240, 849, 318, 703, 61, 104, 1996, 1238, 879, 425, 416, 844, 860, 639, 710, 1508, 223, 619, 319, 480, 213, 68, 2788, 7561, -46, 1595, 1046, 1382, 217, 183, 1321, 25, 4844, 5345, 812, 105, -190, 625, 293, 1033, 1210, 64, 164, 1766, 1248, 0, 826, 2630, 3727, 406, 2999, 1000, 503, -203, 361, 274, 3687, 323, 981, 638, 3229, 1610, 33, 536, 0, 3057, 2892, 0, 3316, 106, 2303, 3301, 426, 9, -97, 11008, 4930, 707, 4, 126, 671, 219, 3622, 1580, 1319, 2152, 2843, 0, 1115, 625, 246, 61, -191, 519, 522, -46, 2269, 386, 0, 9103, 908, 1694, -92, 188, 0, 740, 655, 867, 1009, 6360, 4145, -27, -122, 149, 52, 335, 526, -7, -413, -241, 75, 0, 158, 84, 114, 0, -127, 49, 216, 506, 213, 170, 776, 879, 1633, 926, 2559, 909, -176, 1085, 1265, -239, 726, 296, -627, -468, 551, 0, 101, 517, 37, 854, 30, 81, 0, 663, 102, 370, 189, 111, 665, 214, 0, 1583, 0, 0, 687, 327, -119, 794, -191, 409, 3, 9, 155, 1330, -29, 3133, -33, -170, 1243, 1598, 5, 367, 658, 1722, -315, 2483, 0, 45, 46, 482, 3728, 1099, 2222, 983, 204, 1074, 3466, -203, 2436, 976, 146, 156, 189, 163, 242, 1173, 0, 296, 333, 3864, 2686, -109, 65, 597, -799, 1775, 0, -563, 5, 139, 0, 855, 694, 151, 354, 673, 1265, 597, -393, 256, 0, 2870, 184, 1905, -36, -839, 3465, 119, 36, 113, 118, 251, -277, -189, 4, 1820, 1904, 201, 0, 178, 3798, 301, 294, 0, 292, 2998, 2084, 454, 179, 661, 89, 102, 138, 622, 10, -251, 104, 173, -46, 0, 145, 325, 2311, 158, 1981, 1423, 2455, 859, 9004, 970, 486, 127, 409, 664, 223, -324, -30, 1253, 4758, 30, 81, 0, 328, 145, 465, 751, -379, 0, 349, 1110, 2228, 2166, -247, -123, 44, 131, 476, 1101, -168, -392, 524, 895, 212, 95, 395, 135, 0, 471, 603, 663, 593, 1224, 2, 720, 990, 1747, 59, 33, 340, 305, 0, 497, 1371, 1628, 372, 566, 343, 941, 775, 1759, 1495, 1444, 470, 1162, 2, 1628, 1134, 0, 63, 1221, 5024, 10685, 1629, 268, 0, 5613, 513, 362, 3933, 3899, 473, 1616, 1817, 129, 1938, 133, 2263, 1538, 5, 2625, 1080, 312, 145, -190, 2678, 271, 0, 0, 3161, -4, -701, -375, 959, 1091, -1, 96, 0, 230, -156, 114, -468, 679, 206, 1, 2518, 378, 391, -144, 1240, 1792, 2284, 264, 0, -392, 77, 48, 252, 1954, 755, 755, 341, 1183, 388, 1919, 1613, 0, 0, 0, 0, 466, 515, 2313, -8, 1634, 100, 5361, 1257, 3400, 1377, 2885, 512, 921, 1000, 202, 169, 3726, 146, 940, 734, 77, 925, 1836, 660, 1304, 2956, 146, 36, 1, 2480, 14282, 0, 311, 560, 3059, 1423, 874, 706, 7098, 5389, 1104, 3070, 0, 4108, 0, 4291, 6822, 231, 3176, -824, 676, 651, 2657, 801, 405, 82, 179, 1047, 7138, 165, 37, 823, 1625, 120, 2957, 2, 1, 1795, 4, 476, 9, 131, 1120, 2785, 0, 43, 0, 456, 2018, 0, 354, 719, 828, 2787, 2551, 2722, 1880, 2112, 871, 850, 3243, 1567, 12, 128, 553, 1694, 408, 1223, 1858, 520, 271, 4396, 568, 3665, 244, 568, 42, 309, -180, 34646, 59, 4436, -242, 10052, 0, 152, 156, 66, 9827, 6170, 2383, 635, 16, 362, 191, 455, 3043, 3334, 8029, 624, 331, 2929, 1314, 289, 20, 8163, 1200, 1732, 0, 576, 493, 682, 0, 3485, 2944, 1353, 0, 1541, 548, 254, 4414, 985, 1241, 3278, 412, 483, 7135, 61, 1412, 15, 27, 1196, 19, 432, 988, 1614, 163, 247, 236, 666, 73, 96, 46, 1501, -477, 1694, 196, 462, 1227, 522, 94, 267, 4570, 25, 0, 1920, 1575, 234, 3518, 136, 830, 2054, -546, 5188, 1882, 792, 162, 1035, 746, 17297, 22, 231, 296, 0, 40, 2396, 2944, 7084, 569, 0, 1138, 54, 2637, 2473, 5603, 225, -17, 3490, 769, -90, 1785, -103, 555, 5303, 1258, 1679, 1664, 8749, -247, 3444, 5346, 1144, 2351, 5514, 123, 429, 5254, 589, 568, 10250, 3537, 1947, 1967, 2329, 12026, 1757, 107, 1012, 0, 372, 7506, 314, 860, 0, 3623, 2470, 1307, 3237, 5041, 6619, 1331, 1970, 1026, 249, 12857, 2807, 5060, 2271, 1, 676, 160, 899, 1230, 0, 700, 0, 67, 127, 2089, 3585, 4152, 816, 5447, 275, 1540, 751, 380, 1504, 1783, 2269, 5115, 781, -188, 761, 19, 4508, 66, 2266, 4646, 1494, 1050, 0, 0, 2171, 3234, 11887, 0, 5037, 386, 8781, 538, 5561, 859, 0, -617, 1429, 4189, 580, 2615, 587, 149, 3, 807, 3354, 1249, 190, 550, 12737, 0, 1207, 347, 3713, 3352, 254, 4654, -386, 3043, 1547, 945, 6402, 1295, 6993, 4420, 141, 11462, 6843, 501, 5806, 20138, 1315, 0, 7, 3929, 7, 699, 421, 4333, 300, 7773, 1794, 2032, 3401, 1110, 3068, 983, 2396, 6888, 2087, 121, 3469, 5461, 7119, -35, 314, 18508, 5639, 2918, 12519, 4069, 4254, 219, 0, 2263, 2589, 1982, 7049, 1968, 7195, 4117, 3950, 495, 8629, 455, 4536, 1777, 3527, 372, 92, 72, 1533, 4148, 96, 2245, 834, 4119, -525, 39, 0, 2766, 3300, 36, 22520, 7724, 111, 2640, 3436, 0, 3636, 3288, 2633, 580, 149, 4329, 1097, 1275, 1003, 1004, 814, 808, 859, 4465, 0, 622, 1709, 394, 1125, -462, 550, 540, 941, 1361, -452, 518, -71, 3234, 98, 196, 936, 300, 217, 163, 110, 1646, 2040, -308, 2325, 86, 1088, 0, 0, 2103, 2, 51, 96, 1027, 1308, 840, 5701, 6016, 630, 432, -247, 52, 623, 0, 100, 4089, 1669, 169, 483, 250, 0, 902, 2156, 2360, 201, -92, 514, -114, 1331, 732, 1308, 0, -970, 1246, 73, 406, 4391, 168, 1441, 3881, 678, 141, -94, 897, 1214, 324, 2666, 10613, 697, 953, 198, 1110, 130, 315, 4383, 313, 273, 840, 309, 2707, 1109, 101, 725, 309, 541, 0, 1624, 0, 318, 1908, 676, 1076, 1228, 0, 13, 12857, 0, 6101, 158, 346, 697, 616, 1140, 3444, 1879, 0, 0, 1493, 313, 1815, 700, 206, 255, 270, 4987, 673, 320, 0, 292, 757, 40, 1257, 1693, 5060, 373, 369, 2346, 1599, 1066, 2603, 381, 1314, 5689, 12114, 0, 2232, 11862, 127, 2152, 3552, 1, 1872, 1341, 7702, 4657, 397, 0, 348, 2442, 1543, 805, 2265, 312, 196, 620, 1714, 2416, 335, 755, 2850, 1135, 197, 720, 5355, 445, 5359, 1443, 4047, 23878, 167, 1047, 7613, 117, 2266, 558, 2509, 1808, 1130, 653, 2021, 313, 983, 5561, 8345, 1231, 3730, 924, 2489, 995, 280, 3434, 576, 1363, 581, 1376, 52, 0, 549, 92, 2881, 250, 602, 730, 523, 293, 215, 0, 42, 316, 4562, 10180, 0, 281, 250, 0, 404, 506, 1097, 3698, 883, 2539, 2326, 811, 256, 682, 483, 1506, 575, 409, 1316, 671, 2351, 997, 4808, 6422, 31, 171, 2681, -61, 8929, 715, 1058, -34, 258, 0, 71, 5810, 3473, 238, 1740, 3407, 1045, 132, 1000, 115, 180, 201, 8089, 8866, 2, 259, 1107, 1730, 447, 2786, 10185, 978, -130, 3049, 11385, 46, 4126, 212, 1250, 25, 1277, 390, 9601, 565, 1384, -522, 9676, 134, 2143, 831, 2308, 1468, 311, 274, 172, 507, 393, -39, 1474, 471, 3161, 1612, 4099, 68, 414, 5005, 1756, 1613, 3123, 4736, 127, 11891, 4979, 754, 1207, 712, 297, 3676, 817, 283, 82, 2847, 1822, 16, 377, 4415, 303, 12039, 1443, 1781, 3415, 318, 423, 2544, 2614, 1268, 536, 1293, 1377, 2580, 314, 206, 3286, 803, 719, 898, 332, 695, 1819, 2544, 759, 229, 96, 110, 1406, 0, 2103, 1599, 925, -3, 84, 1367, 703, 0, 2646, 572, 107, 765, -253, 0, 2, 6, 0, 448, 4659, 1528, 330, 731, 25, 587, 2557, 275, 1129, 2149, 855, 491, 180, 133, 649, 1411, 11174, 439, 6158, 3498, 1801, 937, 26, -53, 2337, 1762, 3469, 545, 3480, 183, 2416, 424, 0, 294, 1598, 268, 214, 240, 2511, 4596, 1273, 3994, 469, 932, 844, 471, 63, -779, 293, 294, 13546, 329, 69, -3058, 224, 165, 397, 3154, 1180, -487, 879, 806, 674, 0, 323, 921, 5303, 0, 4004, 386, 284, -242, 1086, 2840, 43, 3229, 341, 0, -19, 243, 2575, 105, 0, -443, 1524, 1925, 240, 387, 13, 3203, 606, 0, 527, 309, 4333, 253, 6807, 83, 819, 1968, 136, 1995, 285, 621, 597, 0, 1750, 10086, 3466, 2193, 2282, 1973, 10925, 704, 9480, 3391, 1134, 6281, 123, 8556, 0, 2, 1108, 212, 5110, 17, 942, 0, 882, 81, 276, 2991, 90, 3754, 395, 676, 8, 0, 2069, 506, 8, 670, 0, 1939, 1780, 0, 955, 732, 2022, 4099, 4917, 2883, 1374, 398, 3027, 2974, 1081, 383, 3913, 3287, 3629, 132, 726, 149, 640, 7707, 53, 15, 242, 377, 94, 363, 1970, 1167, 193, 0, 78, 1291, 1425, 3382, 5583, 1230, 96, 224, 0, 1730, 697, 943, 1458, 95, 2308, 5701, 1529, 292, 7468, 1559, 1693, 757, 201, 1070, 1330, 314, 677, 2845, -32, 398, 0, 1337, 493, 1750, 4401, 193, 5106, 994, 1101, 129, 6060, 55, 1624, 341, 2016, 255, 4684, 1052, 1808, 3511, 518, 227, 270, 4733, 6657, 613, 5462, 674, 364, 181, 474, 219, 86, 1170, 18, 197, 938, 1228, 2156, 2408, 35, 393, 618, 1608, 699, 5802, 2749, 1162, 776, 1619, 139, 2816, 507, 98, 2753, 0, 498, 700, 4152, 927, 2252, 0, 0, 728, 354, 382, 36252, 158, 734, 305, 10252, 3141, 236, 5437, 133, 640, 253, 5060, 902, 335, 1, 1451, 1636, 3221, 316, 2044, 557, 489, 1138, 188, 1998, 520, 6368, 2666, 4182, 3271, 163, 1515, 803, 400, 2454, 533, 2352, 9367, 237, 444, 9407, 387, 2971, 2643, 261, 514, 5872, -368, 188, 61, 2160, 658, 0, 196, 94, 326, -395, 9328, -522, 1286, 710, 3, 0, 141, 53, 415, 0, 1979, 0, 7506, 752, 99, 266, 27, 524, 5990, 3935, 991, 59, 1598, 0, 802, 2465, 114, 1461, 475, 4, 1618, -271, 2166, 4031, 5024, -1944, 3025, 2090, 0, 1429, 0, 311, 66, 2146, 356, 62, -106, 608, 291, -184, 2345, 4, 6005, 333, 3041, 4, 603, 0, 830, 962, 185, -9, 891, 148, 1374, -556, 1612, 0, -109, 580, 4500, -192, -438, -701, 1400, 5293, 785, 373, 763, 1236, -811, 7279, 479, 199, 1808, 2, -106, 670, -361, 159, 151, 518, 5073, 607, -308, 367, 855, 1240, 663, 895, 804, -407, 802, 912, 355, 3, 0, 1836, 1425, 963, 145, 634, 87, -522, 319, 0, 478, -276, 4227, 2785, 1401, 0, 1852, 462, 800, 221, 1089, 451, 496, 1080, 1064, 297, 3, 76, 398, 83, 879, 210, -276, 863, 228, 1396, -19, 2253, 0, 265, -54, 3821, 0, -7, 830, 0, 4406, 437, 54, -454, 1006, 2999, 85, 17924, 8180, 2431, 341, 4963, 8781, -504, 1738, 214, 684, 580, 273, 811, 0, 1354, -254, -449, 7863, 336, 1049, 6089, -42, 953, 315, 5845, 378, 765, 5795, 315, 916, 124, 207, 422, 125, 2, 7066, 146, 496, -40, 140, 77, 22, -51, 26, 154, 534, 1265, -466, 38, 703, 685, 780, -67, 1085, 29, 560, 22, 6567, 2139, 582, 194, 353, 341, -172, -639, 864, 134, 50, 507, 480, -22, 878, 2574, 2734, 241, 129, 41, 3387, 148, 249, 5774, 0, 1841, 2120, 297, 89, 827, 205, 3743, 117, -493, 62, 2040, 344, 230, 1277, 104, 706, 174, 239, 513, 7290, 5909, -195, 3917, 352, 615, 0, 217, 1015, 409, 1119, 12198, 235, 170, -357, 2145, -454, -887, 292, 139, 2684, -1, 1387, -522, -242, 719, 0, 497, 76, 397, 3723, 1625, 106, 1543, 2239, 1977, 0, 2145, 116, 620, 307, 7702, 3733, 3993, 428, 5310, 2509, 255, 616, 733, 439, 590, 567, 689, 386, 0, 957, 312, 167, 13094, 5086, 707, 466, 152, 1612, 911, 27, 6, 253, 705, 109, 23878, 3399, 79, 243, 138, 1317, 2714, 17964, 846, 326, 100, 304, 39, 312, 906, 524, 1081, 983, 2232, 1967, 1406, 157, 0, 2509, 1368, 3076, 7818, 4118, 760, 277, 253, 602, 473, 1167, 343, 1808, 1199, 70, 1723, 1323, 2552, 2613, 0, 938, 284, 1059, 1684, 534, 293, 262, 0, 2367, 31, 3842, 3343, 804, 1011, -528, 1840, 1451, 2351, 1623, 597, 611, 313, 1954, 307, 7968, 2160, 1005, 1988, 1117, 3370, 109, 1376, 523, 15161, 653, 64, 1165, 7019, 1568, 3109, 4987, 189, 2467, 5887, 1354, 23, 0, 978, 294, 409, 261, 27624, 3846, 2913, 2240, 5, 150, 882, 366, 1189, 204, 637, 2383, 1231, 3340, 200, 48, 2786, 5313, 201, 389, 3917, 889, 481, 810, 52, 108, 684, 2071, 339, 572, 1779, 132, 22086, 0, 205, 317, 4945, 419, 1130, 1662, 1871, 1190, 563, 6, 594, 617, 538, 303, 985, 897, 171, 1633, 1085, 1980, 1933, 1841, 42, 794, 905, 8295, 0, 874, 4321, 0, 745, 924, 216, 1159, 3290, 582, 380, 682, 92, 66, 252, 6574, 2094, 294, 1177, 8866, 3407, 5205, 3630, 12, 544, 10346, 1612, 718, 3547, 914, 1306, 259, -1206, 372, 3518, 505, 1776, 20585, 1595, 1000, 138, 561, 170, 502, 379, 14533, 689, 2567, 443, 2074, 2374, 1347, 7443, 10583, 425, 997, 5966, 2061, 0, 2887, 1464, 1315, 466, 2383, 1649, 132, 482, 203, 88, 2881, 668, 73, 1309, 2763, 2959, 930, 12956, 0, 19, 625, 5511, 1137, 1329, 1044, 302, 330, 2857, 68, 1524, 1633, 0, 507, 643, 2785, 137, 870, 8603, 18016, 51, 1032, 981, 431, 203, 3737, 7005, 111, 565, 889, 202, 307, 0, 429, 4388, 2557, 3122, 922, 3796, 185, 151, 829, 103, 79, 1609, 2489, 989, 58, 323, 5474, 219, 1240, 1228, 1660, 914, 1699, 1167, 19317, 1646, 242, 382, 2383, 2666, 1718, 1147, 216, 153, 1791, 86, 6791, 292, 1291, 175, 0, 0, 0, 4717, 701, 3, 405, 168, 4297, 10532, 4693, 9064, 6983, 2, 1536, 1612, 1211, 96, 0, 1534, 3403, 687, 7050, 538, 2600, 0, 1034, 108, 10185, 3444, 1815, 52587, 10884, 201, 190, 94, 1311, 589, 1853, 4401, 201, 533, 3840, 994, 96, 1600, 100, 260, 1013, 413, 181, 123, 353, 2794, 3850, 2970, 6991, 2223, 27696, 116, 2987, 608, 48, 113, 498, 2643, 1337, 618, 451, 0, 11, 1165, 2465, 41, 35, 3169, 14, 431, 616, 3324, 113, 13014, 9367, 2067, 376, 842, 1235, 1, 1276, 44, 255, 926, 1506, 444, 557, 535, 2671, 896, 528, 209, 226, 1974, 942, 336, -306, 4623, 276, 0, 1189, 431, 2218, 1495, 828, 12972, 948, 48, 0, 537, 1938, 9131, 443, 132, 3411, 1, 10905, 976, 2531, 1270, 5359, 10596, 691, 347, 9962, 11862, 1756, 4068, 5310, 215, 106, 0, 469, 2326, 3370, 144, 733, -309, 5584, 61, 1047, 265, 117, 5473, 106, 127, 380, 0, 255, 2145, 91, 1853, 11262, 0, 1129, 108, 1696, 854, 223, 40, 2048, 3689, 539, 590, 780, 1464, 1311, 7, 636, 24277, 1872, 490, 118, 4119, 1504, 2087, 5689, 201, 119, 17964, 1113, 1596, 2635, 568, 2626, 4752, 528, 2552, 3343, 995, 7968, 0, 2587, 6468, 4744, 1231, 3443, 2568, 1644, 1354, 602, 1795, 0, 268, 52, 924, 3033, 6574, 0, 108, 409, 726, 1871, 1094, 2915, 480, 1227, 1205, 161, 802, 723, 6538, 147, 0, 1416, 14220, 12569, 555, 158, 261, 786, 27, 0, 2815, 3082, 1233, 739, 640, 1843, 322, 7331, 1765, 1236, 6046, 934, 1013, 596, 4775, 3109, 388, 262, 654, 1855, 810, 1664, 362, 1293, 874, 10943, 4539, 2817, 1646, 808, 44, 425, 2367, 10252, 1000, 4380, 20585, 2601, 238, 0, 3371, 10583, 16, 1451, 0, 132, 189, 288, 1027, 6784, 1464, 8295, 2917, 158, 1588, 282, 5437, 1185, 895, 1857, 4596, 565, 1781, 3415, 12039, 12, 139, 2488, 7111, 361, 2109, 1093, 2646, 6, 670, 592, 1965, 313, 40, 4198, 536, 1311, 766, 2557, 79, 1318, 3771, 0, 6138, 129, 469, 893, 519, 4017, 643, 2338, 0, 6481, 820, 1784, 1411, 7529, 206, 1388, 1807, 480, 571, 3994, 1693, 80, 267, 2581, 1134, 309, 6807, 6746, 375, 658, 371, 2693, 329, 79, 0, 6281, 775, 411, 2144, 431, 123, 3792, 4807, 8603, 2178, 2991, 4174, 10925, 2411, 14646, 86, 11115, 6900, 5878, 1780, 183, 3648, 4112, 542, 243, 942, 254, 0, 0, 992, 23, 732, 4629, 0, 70, 5063, 4572, 129, 4984, 1004, 4787, 914, 2326, 81, 203, 3951, 240, 338, 0, 0, 944, 5, 697, 1948, 3654, 551, 997, 477, 3663, 853, 8278, 7458, 44, 1193, 230, 8729, 2384, 3401, 881, 1522, 4243, 473, 5462, 4412, 10332, 0, 238, 62, 2398, 704, 506, 2352, 938, 1720, 1492, 1778, 0, 942, 556, 10541, 1536, 235, 935, 10861, 2331, 1228, 125, 2816, 0, 22867, 1311, 94, 0, 3371, 943, 9366, 6746, 1076, 0, 1624, 801, 1070, 123, 3623, 533, 994, 386, 1026, 2424, 1331, 2206, 0, 4693, 1230, 3417, 1767, 5291, 3403, 133, 4745, 3324, 5220, 2600, 260, 608, 462, 14, 1146, 181, 3114, 376, 9367, 382, 920, 129, 169, 679, 535, 3115, 1234, 197, 3186, 3160, 195, 6027, 764, 583, 0, 5715, 1204, 4922, 234, 9421, 4198, 3576, 1925, 1129, 265, 38, 1982, 3399, 691, 330, 318, 644, 4, 1165, 4708, 2820, 1147, 980, 2983, 339, 1940, 1445, 294, 5887, 11891, 78, 733, 5028, 481, 314, 684, 1681, 10250, 254, 0, 307, 957, 155, 614, 426, 8304, 4721, 395, 3588, 2540, 883, 4402, 256, 43, 3856, 3025, 2326, 925, 1961, 2037, 459, 572, 452, 5313, 1533, 1337, 2986, 1230, 538, 5108, 1341, 320, 131, 2074, 1443, 3338, 661, 7105, 3735, 538, 4657, 5, 230, 1808, 81204, 3518, 25, 2551, 11303, 2724, 755, 718, 870, 978, 544, 461, 2795, 139, 1044, 1633, 6513, 1388, 15, 2544, 2007, 2166, 4579, 2488, 2190, 254, 1579, 330, 938, 5372, 3176, 496, 0, 482, 180, 4198, 817, 5021, 991, 133, 366, 979, 2939, 4803, 1255, 8556, 588, 994, 820, 203, 86, 2855, 474, 203, 7005, 775, 4112, 255, 243, 942, 1180, 992, 70, 4629, 829, 0, 2776, 279, 129, 157, 659, 2493, 3949, 997, 1699, 1181, 0, 0, 1948, 306, 18967, 261, 694, 202, 4831, 264, 278, 7802, 3629, 547, 4468, 1989, 2645, 4060, 303, 2600, 1818, 728, 955, 265, 123, 4807, 205, 2812, 6571, 2758, 2037, 0, 2223, 1085, 602, 1646, 415, 2354, 318, 1730, 448, 994, 19, 1625, 1144, 897, 704, 1996, 216, 1206, 1855, 62, 1536, 824, 235, 108, 2331, 390, -32, 172, 10541, 480, 2374, 479, 94, 417, 2557, 589, 201, 2885, 2155, 902, 1791, 2206, 5291, 638, 4448, 5267, 3324, 61, 0, 116, 413, 353, 1712, 523, 1454, 9367, 291, 846, 341, 113, 13014, 632, 27696, 3715, 4675, 3271, 123, 10185, 1026, 905, 775, 926, 349, 484, 1830, 2987, 197, 431, 169, 226, 2424, 1234, 271, -69, 1562, 52587, 535, 2159, 195, 154, 681, 2326, 961, 2532, 455, 2013, 1568, 1215, 121, 1, 597, 763, 326, 1380, 0, 368, 438, 483, 7620, 805, 0, 302, 215, 5473, 106, 2346, 3219, 2227, 6610, 195, 973, 7766, 948, 2149, 1413, 117, 0, 71, 1938, 5359, 519, -46, 1075, 91, 598, 1027, 235, 2, 4945, 311, 154, 712, 7585, 2489, 2896, 354, 2667, 144, 760, 1050, 5296, 12980, 509, 1644, 451, 53, 334, 6771, 52, 2240, 4761, 1570, 588, 957, 224, 2161, 426, 147, 508, 294, 1825, 158, 0, 4721, 282, 108, 187, 3764, 1416, 2801, 494, 1978, 155, 4608, 951, 495, 1843, 262, 0, 3561, 465, 76, 1796, 874, 2140, 295, 304, 624, 303, 273, 3342, 1227, 283, 3384, 2367, 5958, 3735, 2693, 655, 1681, 866, 774, 2678, 3371, 10583, 4176, 334, 393, 916, 832, 3287, 488, 1947, 558, 235, 288, 6036, 3, 0, 81204, 5236, 2466, 1348, 7929, 70, 459, 2102, 394, 4126, 224, 9001, 560, 102, 1088, 562, 718, 230, -67, 3848, 579, 1646, 1229, 129, 197, 3779, 2016, 436, 413, 116, 62, 107, 497, 2557, 2646, 3771, 1066, 1, 232, 1177, 603, 736, 129, -115, 104, 589, 3472, 2845, 7529, 309, 0, 1672, 1711, 1295, 2046, 455, 722, 371, 997, 206, 820, 410, 1216, 11115, 796, 5037, 1780, 429, 2779, 222, 0, 1445, 655, 77, 115, 2722, 1438, 140, 915, 1058, 0, 4984, 697, 4787, 962, 1320, 2955, 0, 12067, 3006, 388, 0, 565, 329, 4243, 72, 4328, 395, 0, 205, 1730, 5, 1965, 19, 7802, 2352, 155, 401, 79, 1646, 437, 935, 2929, 2304, 348, 640, 88, 2283, 473, 0, 397, 62, 3634, 273, 3417, 133, 1624, 386, 2530, 2838, 1026, 2206, 260, 0, 2987, 1230, 2335, 8000, -27, 1146, 1004, 197, 169, 393, 29, 920, 3608, 870, 9367, 1374, 6027, 1, 349, 39, 583, 1276, 2364, 53, 1612, 639, 3114, 1, 1070, 195, 3144, 3236, 3208, 1568, 1191, 706, 2616, 215, 3230, 726, 3713, 0, 265, 70, 388, 1727, 205, 2881, 846, 2266, 742, 2149, 344, 1430, 265, 38, 3538, 973, 1831, 641, 1047, 8103, 523, 0, 515, 8304, 1368, 272, -46, 168, 515, 223, 228, 644, 330, 952, 1165, 0, 2823, 1490, 48, 187, 8304, 276, 147, 323, 799, 2540, 322, 495, 358, 2371, 53, 861, 602, 4041, 275, 12, 1742, 46, 2027, 3221, 1720, 2033, 3189, 2946, 1230, 572, 1435, 403, 1071, 3301, 1723, 705, 955, 303, 3287, 109, 224, 538, 5397, 1957, 774, 1188, 1167, 2611, 3145, 825, 122, 334, 623, 86, 976, 393, 293, 476, 2724, 2391, 11, 54, 3109, 3126, 488, 978, 718, 3255, 1544, 0, 4071, 29080, 12569, 2896, 196, 1027, 463, 3511, 224, 198, 0, 674, 623, 558, 259, 432, 159, 657, 1808, 392, 6651, 2795, 1414, 2666, 668, 502, 1809, 267, 2473, 372, 484, 1767, 491, 558, 337, 654, 2427, 386, 251, 1044, 2907, 828, 1857, 1962, 2787, 1123, 643, 62, 2007, 3463, 482, 1298, 5329, 6181, 1255, 132, 2733, 4189, 724, 205, 154, 232, 979, 5021, 2469, 180, 658, 2918, 588, 423, 302, 414, 817, 183, 806, 9299, 478, 1057, 762, 31472, 1090, 3738, 1277, 756, 1636, 300, 321, 1295, 839, 1974, 2855, 5475, 536, 1279, 775, 203, 146, 4124, 14968, 1451, 3026, 431, 985, 243, 1707, 5511, 8, 10861, 373, 1807, 2776, 1058, 0, 1381, 204, 267, 311, 306, 752, 1574, 1181, 1761, 4629, 154, 4112, 11650, 6138, 264, 2144, 482, 0, 4831, 12067, 1227, 1232, 275, 369, 240, 2398, 2785, 62, 1377, 163, 1199, 569, 0, 1989, 46, 372, 2812, 2952, 569, 2812, 1199, 3904, 156, 205, 1625, 415, 1812, 775, 180, 46, 4017, 437, 318, 1646, 470, 3591, 2037, 2979, 1092, 661, 2196, 417, 2077, 2283, 1536, 446, 5, 216, 565, 476, -32, 1348, 228, 1859, 2306, 2038, 824, 348, 1515, 1514, 94, 2325, 417, 3450, 2543, 464, 1853, 272, 1925, 2331, 324, 116, 1779, 346, 353, 27696, 2155, 413, 2590, 320, 1454, 1382, 2427, 2498, 1003, 2007, 926, 632, 2206, 1830, 814, 5275, 2129, 2717, 341, 2885, 523, 4256, 484, 535, 431, 1269, 0, 211, 4012, 625, 1584, 315, 200, 589, -205, -46, 8304, 308, 1113, 992, 272, 1413, 1819, 1623, 630, 2975, 7968, 154, 5169, 228, 4945, 1708, 0, 1584, 1707, 952, 1978, 1105, 52, 6771, 890, 56, 1644, 151, 0, 2240, 957, 689, 2161, 1527, 8304, 698, 494, 2801, 4761, 834, 139, 12, 877, 2066, 555, 48, 235, 1595, 5741, 426, 187, 166, 1416, 1189, 486, 0, 2557, 3138, 1720, 1765, 1506, 3585, 262, 1093, 76, 4680, 1477, 3561, 3340, 1071, 247, 112, 527, 2580, 975, 2986, 2048, 2785, 127, 0, 4951, 488, 1154, 624, 4343, 2367, 283, 6403, 802, 3735, 2693, 0, 1027, 3902, 243, 0, 1055, 60, 5397, 217, 6551, 303, 2093, 1167, 2376, 0, 949, 393, 2678, 11, 4416, 390, 1934, 3511, 1696, 1278, 1011, 2939, 3466, 372, 549, 1102, 973, 491, 3337, 368, 326, 12264, 203, 502, 346, 4328, 279, 131, 2646, 6, 1579, 2007, 1, 469, 643, 514, 1289, 3676, 0, 104, 159, 728, 107, 1946, 1519, 5112, 1090, 4833, 1925, 335, 371, 5475, 203, 50, 11862, 820, 791, 1279, 455, 536, 1693, 1495, 985, 935, 410, 51, 679, 1780, 222, 1210, 1693, 876, 3308, 0, 822, 0, 3904, 2488, 88, 121, 0, 0, 464, 79, 2557, 272, 2543, 1224, 320, 4256, 1168, 703, 136, 3810, 765, 1146, 358, 1511, 557, 583, 557, 113, 2850, 505, 825, 5715, -215, 2239, 376, 15485, 2918, 67, 1664, 192, 1519, 0, 0, 4708, 3283, 1211, 347, 423, 3982, 2506, 697, 456, 324, 5768, 1517, 0, 482, 638, 2524, 4539, 2039, 128, 4787, 1147, 1978, 51, -122, 294, 922, 51, 694, 607, 2155, 159, 7050, 1055, 8165, 376, 3701, 644, 3623, 360, 284, 1893, 69, 1355, 1506, 1393, 487, 655, 1788, 0, 41, 215, 1187, 0, 1213, 653, 3426, 238, 1690, 0, 27, 681, 18188, 1006, 1352, 2340, 366, -374, 1231, 1347, 1778, 2201, 0, 2557, 0, 1320, 0, 0, 22125, 6188, 1077, 45248, 3291, 562, 1046, 87, 10281, 429, 3648, 863, 3, 2648, 0, 0, 655, 854, 858, 567, 399, 257, 3735, 57, 1235, 257, 401, 3290, 297, 33, 1179, 471, 938, 1636, 1243, 0, 1679, 2022, 2033, 869, 3611, 3624, 2488, 2228, 4, 2557, 1390, 1228, 1265, 6046, 816, 0, 118, 1165, 99, 1697, 866, 0, 5301, 7918, 1, 160, 414, 406, 0, 655, 514, 11766, 0, 2892, 762, 2037, 255, 1536, 0, 2, 1435, 1219, 2971, 440, 3301, 2321, 213, 0, 108, 920, 1031, 294, 527, 1030, 2503, 1547, 1, 1167, 523, -207, 3283, 1147, 1170, 4642, 2666, 792, 127, 0, 0, 121, 618, 2495, 569, 473, 474, 0, 1796, 520, 118, 3342, 3239, 23, 40, 226, 939, -20, 940, 5, 0, 447, 126, 4718, 843, 656, 91, 0, 2984, 870, 7608, 5115, 247, 154, 1731, 3634, 19, 1129, 0, 12264, 478, 2081, 965, 332, 1163, 1960, 294, 188, 254, 1377, 523, 600, 387, 849, 1317, 273, 0, 2538, 133, 146, 11115, 486, 159, 1201, 2590, 800, 0, 385, 238, 238, 1812, 413, 152, 1031, 3143, 613, 168, 2144, 1331, 2611, 925, 985, 367, 302, 47, 1946, 2266, 992, 3544, 76, 6013, 19, 232, 993, 535, 127, 1528, 324, 1639, 0, -370, 11862, 323, 3940, 8897, 2002, 668, 2946, 4216, 0, 23076, 7459, 5, 4147, 0, 131, 231, 720, 598, 1819, 587, 1495, 442, 271, 29340, 1014, 2366, 26965, 1411, 617, 4736, 2540, 403, 423, 131, 4041, 273, 101, 0, 2544, 1699, 2689, 3160, 388, 393, 69, 2658, -366, 466, 64, 580, 3317, 955, 7441, 377, 994, 1766, -236, 774, 659, 149, 1405, 290, 595, 4613, 0, 2204, 576, 3840, 1358, 5169, 0, 533, 1788, 1070, 80, 3254, 1309, 0, 1534, 535, 368, 415, 3434, 35, 2948, 2473, 1133, 306, -382, 1224, 5359, 2166, 925, 495, 2374, 1925, 279, 842, 6748, -242, 3917, 1711, 298, 335, 2240, 1478, 390, 1234, 3704, 890, 3663, 119, 167, 1022, 305, 172, 3047, 616, 453, 526, 169, 1347, 378, 2889, 57, 0, 1293, 2085, 315, 1627, 1853, 1429, 3951, 780, 674, 218, 763, 4151, 2787, 26, 153, 362, 1184, 6840, 3145, 660, 1311, 215, 3324, 0, 747, 448, 1246, 271, 1271, 962, 3054, 0, 346, 3550, 1379, 1093, 295, 129, 636, 4101, 3352, 334, 341, 8491, 227, 125, 3608, 2536, 4688, 0, 1113, 1857, 2892, 6, 1076, -930, 1382, 2048, 608, -274, 132, 344, 2230, 5679, 885, 3764, 4017, 368, 569, 272, 167, 1388, 7895, 2160, 1587, 1723, 691, 243, 3696, 71, 0, 3765, 977, 15161, 0, 776, 1994, 616, 3512, 6529, 0, 2129, 874, 496, 1539, 348, 1287, 12356, 5763, -267, 443, 934, 4859, 1843, 1289, 19, 8332, 5517, 949, -157, 254, 136, 285, 874, 1641, 1088, 1138, 13107, 2557, 142, 733, -405, 11008, 246, 0, 536, 1506, 716, 354, 1757, 7554, 854, 1230, 1303, 93, -291, 69, 771, -675, 973, 377, 480, 552, 3733, -158, 136, 4590, 283, 0, 419, 462, 4, 363, 7560, 1297, 40, 436, 403, 5637, 916, 1942, 385, -1042, 3696, 3384, 556, 315, 2812, 324, -519, 188, 267, 21, 2038, 11115, 194, 169, 0, 2981, 67, 360, 3376, 4718, 387, 4266, 396, 2085, 62, -268, 1697, 341, 109, 0, 4158, 156, 390, 36, 0, 50, 102, 57, 973, 3, 1245, 828, 0, 167, 66, 687, 2688, 625, 1742, 1729, 889, 224, 768, 1809, 423, 335, 182, 4401, 607, 1084, 0, 687, 659, 1435, 8828, 69, 79, 990, 2290, 679, -172, 548, 565, 228, 0, 228, 454, 514, 2885, 567, 2235, 5267, 29, 291, 4889, 502, 3608, 3556, -41, 3944, 202, 1175, 974, 3141, 4079, 0, 3357, 2587, 434, 1291, 315, 394, 5561, 3528, 437, 3119, 264, 3285, 1396, 510, -101, 634, 4210, 0, 84, 278, 849, 1612, 204, 1854, 4661, 1579, 1561, 1468, 2887, 224, 1760, 3735, 19, 61, 95, 5, 360, 1730, 0, 7968, 7005, 3020, 605, 376, 891, 704, 4348, 2353, 8000, 2781, 541, 3576, 2725, 2354, 775, 622, 4, 5, 3943, 802, 2351, 1807, 2, 2297, 779, 108, 459, 110, 13711, 3415, 2376, 181, 448, 714, 3608, 5310, 2144, 2281, 2679, 1495, 1304, 78, 437, 7433, 6112, 155, 9883, 495, 294, 8990, 3173, 53, 640, 217, 804, 1374, 3, 502, 61, 153, 8, 780, 630, 2161, 32685, 3384, 5310, 383, 772, 2976, 2729, 520, 205, 2378, 48, 461, 116, 272, 799, 0, 1534, 4457, 189, 1880, 0, 11835, 839, 952, -197, 1657, 1947, 30, 2027, 1332, 72, 153, 70, 36, -198, 5, 2326, 67, 598, 1527, 1623, 253, 1445, 256, 1181, 1646, -309, 670, 2196, 947, 71, 521, 5115, -17, 556, 229, 325, 5112, 11821, 650, 8648, 168, 2486, 396, 1791, 168, 3308, 4721, 18111, 1146, 237, 618, 1085, 1028, 114, 0, 631, 660, 2421, 794, 2138, 2965, 222, 6997, 665, 285, 473, 2226, 245, 1133, 294, 451, 19268, 1461, 465, 0, 654, 79, 4216, 66, 0, 5539, 2599, 2187, 2120, 339, 8229, 346, 158, 7103, 4855, 3696, -357, 8304, 48, 384, 751, 3546, 73, 71, 974, 306, 3463, 3676, 459, 286, -11, 273, 2665, 3290, 8535, 1276, 2736, 10287, 1624, 0, 446, 395, 315, 484, 0, 507, 2987, 801, 0, 572, 2, 34, 1075, 508, 926, 10861, 24, 611, 1857, 329, 2635, 225, 1604, 4657, 1406, 6728, 611, 1805, -399, 208, 10884, 3472, 556, 7192, 13460, 2988, 102, 1868, -578, 241, 162, 5320, 366, 1504, 262, 191, 73, 358, 483, 5047, 5862, 0, 2843, 351, 372, 27696, 133, 3, 15459, 2551, 571, 7803, 0, 24, 2104, 3158, 10, 147, 12857, 4389, 474, 2326, 3644, 1988, 1595, 331, 4623, 12018, 154, 190, 3904, 882, 53, 6542, 687, 3064, 9228, 800, 3505, 5958, 667, 36, 1059, 915, 1547, 3050, 7050, 2, 14144, 705, 1758, 476, 348, 5969, 3195, 2013, 121, 82, 935, 3754, 104, 4751, 1529, 102, 1796, 935, 365, 222, 0, 1015, 107, 82, 877, 2557, 1673, 2467, 980, 183, 387, 459, 1227, 302, 175, 182, -367, 876, 201, 796, 0, 1559, 70, 2734, 3528, 469, 3311, 152, 1047, 484, 1587, 4461, 3551, 495, 3792, 489, 159, 985, 369, 257, 2779, 638, 833, 9314, 4745, 2693, 160, 1697, 0, -770, 20806, 3136, 1430, 6831, 664, 49, 604, 931, 699, 212, 362, 2667, -754, 517, 258, 3140, 658, 255, 5350, 9756, 3646, 368, 1, 1435, 122, 2027, 496, 83, 467, 0, 471, 5715, -389, 364, 25, 3450, 2374, 5514, 455, 739, 336, 436, 536, 0, 0, 1181, 265, 2573, -537, 1128, 2780, 405, 706, 2661, 2734, 196, 533, 6728, 1227, 5, 197, 13, 284, 33, 2333, 1705, 2039, 381, 3738, 4401, 553, -725, 67, 330, 5943, 25, 316, 4389, 0, 1389, 398, 247, 0, 131, 604, 0, 238, 673, 7944, 3696, 347, -199, 337, 5091, 970, 258, 614, 1929, 2362, 8121, 608, 3723, 117, 363, 130, 2225, -105, 7687, 222, 736, 84, 196, 2123, -482, 700, 1388, 222, 592, 0, 411, -104, 475, 181, 653, 0, 0, 942, 13, 200, 660, 1309, -364, -616, 2596, 5802, 568, 296, 561, 2894, 2344, 1345, 171, 1838, 549, 1097, 1148, 3818, 934, -637, 65, 24, 907, 100, 686, 3, 4692, -197, 4335, 664, 307, 5, -67, 3754, 1139, 250, 3695, 1025, 328, 0, 6203, 1904, 347, 1289, 129, 178, 81, 599, 190, 0, 563, 105, 3518, 248, 961, 640, 1144, 0, 4381, 1134, 570, 0, 357, 593, 229, 0, 1157, 234, 322, 113, 2139, 2251, 0, 2756, 7132, 2047, 0, 1151, 5244, 1, 203, 2162, 337, 0, 2, 1365, 2366, -803, 873, -76, 2990, 538, 6388, 840, 1847, 40, 30, 860, 123, 667, 601, 616, 259, 307, -29, 849, 47, 2194, 3413, 1299, 622, 1901, 166, 1717, -160, 89, 493, 2644, 596, 0, 606, -346, 550, 237, 415, -125, 1984, 121, 266, 1723, 1049, 21, 341, 233, 247, 3117, 242, 24, 7007, 1851, 573, 1178, 748, -407, 4695, 2500, 86, 968, 192, 93, 0, 0, -491, 1517, 0, 775, 276, 5781, 0, 901, 431, 911, 96, 0, -288, 2923, 16, 744, 551, 322, 199, 0, 0, 633, 203, 583, 2749, 769, 1924, 1064, 1099, 2971, 2657, 21, 0, 0, 9447, 433, 375, 2399, 148, 224, 147, 744, -94, 9, 1218, 1106, 0, 167, 0, 324, 127, 230, 344, 105, 634, 317, 2234, 223, 230, 0, 313, 289, 0, 3676, -184, 3954, 2951, 78, 1526, 0, 233, 67, 38, 1948, 5296, 612, 4872, 131, 4943, -257, 331, 105, 1160, 4930, 471, 35, 1750, 5275, 728, 0, 263, 601, 6429, 1076, -57, 2, 514, -375, 28, 404, 3430, 99, 2669, 202, 715, 674, 312, 1463, 789, 7, 473, 3150, 12223, 1541, 142, 3, 1207, 351, 777, 997, 15, 2819, 126, 953, 10, -577, 1159, 1350, 88, -245, 161, 41, 450, 782, -666, -151, 55, -666, 87, 4629, 2176, 3431, 208, 0, 240, 1388, 5807, 17, 0, 1743, 1008, 151, 3196, 1411, 630, 0, 561, 0, 281, 40, 0, 1032, -207, 235, 0, 367, 505, 804, -301, -813, 79, 50, 1205, 365, -1415, 223, 1897, 22867, 949, 1165, 1655, 260, 0, -880, 107, 1624, 1257, 355, 579, 1403, -176, -513, 586, 1840, 720, 97, 13, 140, 521, 364, 416, 3304, 1880, 982, 0, 173, 911, 0, 549, 704, 476, 3738, 634, 1134, -128, 7102, 186, 1918, 0, 250, 31, 631, 62, 479, 1121, 138, 0, 239, 866, 1466, 3570, 440, 714, 674, 127, 588, -413, 882, 877, 337, 192, 3350, 99, 806, 6739, 228, -232, 556, 761, 207, 221, 865, 0, 1851, -272, 367, 0, 230, 0, 1954, 379, -341, 179, 13, 839, 192, 576, 643, 342, 1329, 219, 141, 402, 8023, 1131, 1925, 0, 103, 1395, 621, 1161, 24, 2, 295, -208, 258, 119, 967, -90, 0, 0, 392, 23, 2486, 81, 1563, 1602, 2144, 273, 455, 1038, -384, 307, 93, 127, -150, 20718, -218, 5222, 484, 75, 1480, 1111, 639, 29, 6374, 363, -94, 92, 222, 249, 1450, 2290, 0, 1026, 3953, 1426, 53, 160, 171, 3237, 192, 0, 385, 2591, 39, 44, 5432, 349, 20, 381, 4025, 167, 278, 263, 4968, 581, 682, 779, 2139, 5888, 0, 3431, 188, 3214, 873, -32, 1958, 76, 1013, 2731, 93, 173, 384, 491, 728, 411, 265, -3, 586, -196, 700, -67, 1600, 508, 486, 243, 203, 1944, 882, 2924, 20, 1417, 340, 4062, 133, 0, 8876, 423, 58, 5, 175, 1349, 6596, 1741, 25, 0, 4374, 16, 574, 615, 0, 395, 678, 25, 626, 165, 746, 376, -55, 0, 21, 3003, 92, 511, 6904, 795, 0, 166, 157, 0, 0, 174, 1536, 1114, 816, 1202, 600, 6, 2655, 499, 0, 197, 289, 5336, 434, 691, -250, 3696, 2595, 1077, 501, 21, 797, 1659, 97, 2, 384, 63, 1327, 284, 3395, 2038, 15, 1187, 3047, 11650, 1387, 0, 1387, 446, 275, 1655, 196, 0, 1327, 0, 206, 523, 0, 444, 1401, 205, 3957, 9645, 620, 1541, -37, -601, 197, -276, 824, 636, 19, 641, 2983, 0, 252, 119, 171, 0, -249, 455, 10, 529, 1859, 613, 751, 0, 1471, 7503, 68, 530, 1319, 479, 360, 2163, 136, -196, 4087, 1354, 2, 2337, 514, 6212, 424, 499, 8, 1305, 300, 976, 2700, -497, 2424, -2049, 2081, 189, 0, 327, 426, 1291, 3473, 434, 105, 975, -479, 62, 0, 0, 1202, 2854, 9, 203, 0, 2467, 3850, 47, 1895, 522, -918, -193, 48, 0, 201, 598, 1506, 26, 1582, 559, 0, -411, 36, 512, 786, 3658, -227, 423, 947, 324, 0, 1694, 2894, 2693, 644, 185, 1583, 2399, 1322, 194, 1221, -302, 256, 0, 1335, 682, 0, 3834, 5312, 4095, 0, -1386, 525, 2, 0, 1500, 378, 552, -84, -240, 501, 5345, 1559, 0, 0, 1989, 598, 94, 3473, 349, 3876, 1934, 2, 9, 11, 60, 882, 499, 198, 2093, 74, 1220, 1342, 6134, 3680, 605, 104, 683, 48, 15, 6714, 623, 234, 599, 9916, 3160, 0, 2306, 2550, 701, 3244, 402, 6014, -81, 898, 863, 235, 12, 298, 3463, 13338, 391, 569, 54, 875, 631, 3434, 1610, 505, 11528, 102, -581, 0, 7298, 0, 909, 337, 829, 301, 147, 282, 1294, 0, 995, 532, 785, 3028, 1022, 0, 792, 490, 417, 2071, 1187, 280, 1236, 910, -994, 1333, 850, 3773, 460, 3140, 1028, 4264, 3932, -1, 1682, 814, 819, 1265, 514, 1764, 275, 592, 1143, 546, 1164, 1381, 0, 659, 192, -12, 186, 2246, 124, 2087, 1298, 101, 46, 262, 0, 17924, 167, 967, 0, 1547, 388, 1, 0, 80, 4168, 0, 781, 804, 993, 34, 0, 520, 567, 7177, 3629, 2199, 223, 185, 5809, 210, -199, 897, 0, 7378, 593, 1224, 3031, 708, 542, 1141, -42, -556, 482, 1501, 1850, 2266, 3283, 300, 1, 178, 187, 229, 42, -367, 8, 256, 2410, 2976, -426, 39, 1428, 2030, -1636, 108, 2094, 47, 618, 162, -671, 136, 6958, 953, 821, 131, 1272, -306, 3201, 1114, 4168, 0, 0, 3309, 1377, 150, 553, 458, 565, 0, 2278, 210, 15578, 236, 1704, 1693, 372, 1147, 3166, 1074, 23, 1431, 814, 142, 810, 1604, 0, 488, 4110, 1746, -208, 152, 0, 399, 318, 9072, 3071, 177, 10786, 0, 879, 1887, 3331, 3, 7668, 55, 0, 2998, 524, 106, 85, 249, 10, 523, 1321, -194, 4517, 964, 923, 2677, 873, -1050, 361, 1466, 782, 3576, 3, -112, 759, 195, 15, -1053, 488, 287, 578, 1509, 3794, 4324, 793, 943, 0, 19447, 565, 1541, 1079, 0, 1310, 98, 5, 0, 9216, 2551, 0, 154, -150, 427, 664, 0, 680, 4, 196, 290, 221, 180, -139, 79, 236, 1120, -2282, 2200, -461, 385, 2291, 54, 2850, 1235, 360, 1352, 905, 138, 0, 221, -254, 1510, 1757, 0, 934, 0, 1217, -57, 82, 438, 729, 0, 374, -341, -367, 592, 457, 4819, -85, 467, -309, 21522, 467, 870, -93, 75, 292, 0, 263, 134, 91, 0, 229, 892, 197, -213, -321, 604, 606, 1518, 192, 0, 289, 535, 33, 243, 0, 1279, 1327, 297, 740, 278, 553, 406, 5, 2071, 0, 932, -119, 381, 1309, 542, 30, 0, -15, 318, 1405, 5271, -51, 3914, 4976, 16, 1595, 306, 215, 0, 2628, 1778, 1423, 659, -31, -204, 1462, 723, 3954, 33, 4151, 0, 219, 2728, 76, 4388, -210, 2427, 45, -74, 2173, 8422, 120, 1320, 245, 9569, 0, 2770, 1221, 298, 967, 0, 0, 1368, 751, 2597, -35, 629, 1330, 277, 1018, 11, 0, 905, 15474, 54, -246, 0, 1951, 65, 368, 51, 3492, 21, 8403, 978, 165, 350, 385, 2058, -11, 505, -547, 562, 4222, 105, 0, 66, 0, 13578, 11222, 1737, 59, 54, 2282, 299, 396, 184, 475, -496, 117, 713, 45, 1496, 0, 886, 4592, -38, 431, -292, 2571, 191, 1689, 8494, 159, 7, 35, 1007, 106, 0, 0, 216, 746, 764, 193, 368, 205, 96, 8016, 221, 157, 4830, 1039, 71, 1214, 0, 0, 116, 100, 91, -398, 2820, 223, 2278, -782, 8919, 104, 3387, 0, 449, 4787, 1240, 191, 2185, 0, 2857, 3782, 83, 6, 88, 294, 84, 84, 602, 0, -470, -37, 2145, 937, 2059, 3598, 1206, 1273, 46, 534, -300, 4, 386, -396, 0, 14930, -600, 3733, 465, 994, 758, 256, 3461, 124, 355, 1514, 40, -320, 566, 388, 577, 17, 260, 0, 1823, 1463, 3390, 2769, 215, 12531, 80, 317, 217, 4874, 275, 738, 1839, 556, 207, -461, 789, 1133, 4253, 255, 582, 743, 570, 786, 295, 2583, 1285, 287, 658, 246, 3463, 291, 19, 5700, 0, -346, -456, 5953, 1693, 350, -181, 1735, 157, 67, -444, 0, 171, 262, 0, 965, 0, 6392, 194, 2308, 941, 46, 131, 1844, 281, 109, 268, 298, -525, 738, 94, 7702, 857, -354, 0, 112, 5735, 1807, -30, 216, 127, 1096, 5193, 6, 339, 0, 625, 3776, 1239, 93, 304, 1409, 191, 79, 2263, 10438, 81, 964, 0, 3839, 586, 600, 1343, 113, 240, 194, 278, 260, 1013, 0, 2363, 913, 686, 149, 500, 11766, 137, 0, 335, 16, 0, 9851, 525, 788, 1016, 292, 999, 2870, 689, -100, 1084, 154, 1653, 2378, 1515, 1636, 1516, 127, 2876, 1384, 228, 2481, 475, 1401, 425, 244, -6847, 0, 386, 332, 1367, 120, 19, 3, -98, 303, -752, 97, 794, 1880, 732, 5261, 712, 28, 0, 827, 110, 377, -344, 261, 104, 610, 4328, 1371, 193, 258, 56, 136, 1222, 3672, 2188, 924, 11431, 140, 27, -468, 438, -43, 1161, 458, -441, 2278, 0, 702, 69, 182, 318, -181, 261, 244, 880, 2283, 550, 7752, 6, 4471, 1969, 219, 76, 60, 8, 19, 20011, 608, 1068, -269, 494, 159, -754, 157, 2110, 2603, 992, 593, 327, 6766, 432, 25, 5299, 291, 517, 97, 1636, 996, 0, 710, 1049, 556, 629, 186, 0, 1264, 64, 2497, -97, 161, 732, 94, 2404, 21, 2658, 131, 0, 351, 564, 2240, 46, 132, 71, 0, 267, 841, 51, 7735, 417, -14, 2490, 0, 184, 19343, 947, 90, 10218, 72, 120, 130, 41, 0, 673, -131, 2607, 5, 23, 180, 68, 0, 565, 598, 1609, 1047, 5641, 557, 1066, 35, 54, 1806, 39, 0, 930, 0, 2764, 4492, 2166, 0, 1575, 201, 318, 1682, 242, 291, 304, 108, 1303, 292, 89, 788, 573, 1318, 151, -37, 522, 2315, -718, 4, -43, 596, 670, 430, 60, 0, 678, 159, 1345, 483, 2443, 1258, 389, 0, 2067, 332, 80, 84, 2702, 37127, 0, 1813, 1612, 198, 1106, 2321, 33, 101, -59, 486, 136, 193, 5083, 0, 8, 21, 570, 644, 1104, 6004, 1503, 896, 177, 665, 380, 151, 179, 4, 116, 0, -57, 894, 478, 10, 30, 578, 2360, -47, -805, 782, 1282, 766, 348, 296, 908, 0, 5276, 658, 1089, 1042, 544, 134, 73, 13156, 30, -66, 1373, 8564, 81, 5426, 16, 297, 734, 1514, 153, 3503, -849, 1441, 2487, 927, 598, 345, 197, 220, 9, 5678, 0, 169, 388, 61, 7100, 219, -76, 1432, 879, 0, 1498, 543, 1797, 2994, 1690, 1573, 0, 173, 989, 335, 17, -74, 8319, 15341, 52, -56, 1670, 370, 466, 2583, 1177, 311, 842, 2744, 503, 62, 4841, 253, 842, 378, 637, -372, 135, 7, 446, 1328, 252, 125, 1324, -244, 657, 2140, 5571, 103, 2133, 8876, 22, 3854, 32, 337, 228, 1125, 1612, 1310, -210, 0, 975, 306, 170, 154, 1972, 994, 380, 54, 512, 1027, 1770, 13, 438, 162, -183, 698, 165, 3499, 378, 855, 32, 0, 0, 2994, 1238, 1717, 224, 1048, 592, 396, 268, 0, 5223, 1611, 780, 399, 1649, 974, 22, 507, 10787, -259, 414, 67, 328, 4, 471, 3038, 2276, -848, 2662, 2597, 550, 357, 1153, 4912, 194, 0, 565, 649, -45, 1310, 2308, 4, 1297, 1808, 2080, 5034, 322, 679, 1742, 154, 140, 377, 3644, -50, 453, 45, 46, 2060, 1316, -361, 349, -68, -157, 320, 86, 681, 0, 4567, 40, 706, 420, 0, 65, -27, 114, 678, 3105, 281, 1932, 481, 156, 0, 557, 1467, 428, 994, 769, 108, 19, 199, 448, 473, 726, 0, 536, 2194, 1406, 44, -6, 1631, 4054, 75, 172, 356, 1508, 49, 544, -383, 606, -410, 469, 2, 241, 0, 322, 2193, 144, 0, 3428, 10, 1841, 84, 622, 309, 154, 1949, 2605, 5163, 557, 119, 0, 356, 1395, 20, 84, 609, 1856, 608, 0, 1859, 282, 154, 222, 897, 131, 0, 37, 1704, 314, 399, 300, 382, 1301, 0, 930, 21, 111, 1199, 1085, 230, 558, 127, 4844, 826, 30, 2708, 108, 577, 2764, 2834, -324, 4, 559, 271, 5774, 816, 3, 274, 11265, 70, 405, 22755, 557, 1554, 89, 444, 4951, 30, 1, 0, 0, 1093, 1561, 1861, 333, 342, 388, 1515, 654, -467, 398, 0, 12848, 5205, -181, 250, 110, 445, 175, 458, 5127, 928, 105, 18558, 375, 133, 2278, 443, 23592, 61, 192, 628, 372, 222, 2469, 459, 254, 563, -29, 494, 781, 2793, 1134, 4568, 0, 1189, 692, 716, 520, 2133, 0, 70, 872, 2458, -30, 409, 60, 667, 620, -60, 237, 373, 1221, 0, 999, 67, -463, 7934, 816, 2557, 546, 78, 24450, 765, 91, 703, 589, 0, 1755, 269, 432, 4382, 109, 52, 922, 1779, 8, -271, 0, 14, 215, 307, 265, 323, 800, 392, 406, 104, 1453, 1168, 766, 1784, 3, 1412, -312, 167, 1239, 776, -13, 4519, 5769, 414, 958, 922, 1585, 910, 3738, 1387, -3, 144, 103, 0, -547, 9326, 1281, 267, 0, 512, 3531, 4358, 222, 22, 37, 591, 12, 131, 228, 1794, -34, 416, 1170, 0, 466, 1463, 6835, -397, 5222, 0, 526, 2422, 430, 20, -211, 1337, 4798, 1077, 9713, 5547, 2433, 105, 275, 366, -315, -55, 0, 2993, 3715, -8, 1577, 588, 0, 944, 9, 144, 1222, 0, 668, 568, 0, -861, 877, 1107, 757, 30, 287, 3079, 3238, 1012, 203, 2880, 486, 1708, 10635, 1728, 67, 387, 56, 18, 0, 174, 162, -16, 113, 2940, 1954, 182, 247, -58, 1685, 410, -367, 6835, 2398, -312, 2079, 573, 399, 57, 641, 1575, -475, 128, 305, -80, 198, 1170, 683, 2312, 28, -15, 476, 0, 217, 157, 83, 1871, 11016, 4023, -57, 91, 506, 161, 633, 41, -375, -284, 0, 0, 2413, 134, 127, -253, 659, 314, 6362, 33, 392, 454, 23, 2034, 185, 0, 738, 0, 862, 1256, 371, 2503, 393, 557, 61, 2991, 557, 173, 80, 3611, -482, 2982, 458, 11, 0, 328, 111, 2223, 37, -15, 9051, 284, 183, 109, 934, 93, 811, 753, 538, 1098, 3086, 2054, 2501, 209, 972, 3, 326, 3323, 4121, 13849, 630, 624, 7503, 0, 2850, 672, 492, 778, 106, 4929, 0, 545, 1728, 1401, 1943, 11, -170, 239, 272, 113, -120, -388, 51, 1689, 108, 25290, 1, -22, 167, 1796, 677, 593, 316, 855, 3096, 325, 1023, 991, 876, 1271, 165, 578, 2595, 8226, 657, 0, 1875, 254, 1252, 790, 656, 329, 83, 138, 0, 39, 426, 693, 2854, 787, 102, 855, 1675, 1996, 473, 1, 1064, 215, 138, 221, 720, 51, 0, 606, 6551, -294, 64, 169, 230, 2485, 209, 71, 137, 2547, 1800, 6512, 1476, 534, 1696, 219, 557, 1277, -30, 175, 1595, 723, 297, 310, 1012, -103, 621, 548, 182, 484, 1342, 0, 66, 21, 608, 5345, 461, 5801, 0, 78, -1415, 30, 100, 1371, 62, 1005, 517, 0, 382, 468, 178, 211, 57, 114, 1222, 1660, -9, 0, 30, 0, 369, 1571, -187, 0, -257, 2459, 1219, 5784, 162, 1909, -195, 4, 350, 87, 777, 3140, 1510, 756, 1086, 339, 205, 2717, 138, 1943, 744, 79, 2400, 419, 414, 27, 2823, 4692, 1261, 0, 0, 483, 7369, 804, 2644, 175, 0, 272, 252, 4053, 2158, 299, 2731, 50, 36, 220, 5746, 0, 238, 76, 679, 1390, 418, 2891, 497, 2933, 948, 0, 0, 687, -230, 2575, 7649, 59, 23495, 365, 97, 5, 739, -177, -454, 1267, 1639, 900, -481, 528, 34, 223, 218, 130, 1156, 314, 1374, 22, 1129, 130, -202, 293, 275, 9224, 515, 760, 2302, -389, 133, 1580, 1020, 298, 13901, 563, -27, 2113, 980, 352, 96, 3842, 3870, 589, 7426, 1, 6101, 231, 876, 86, 363, 8298, 0, 93, 596, 168, 29, 6, -934, 297, 533, 175, -292, 2, 39, 396, -338, 2050, 510, 477, 4176, 0, 0, 297, 167, -353, 1525, 211, 416, 0, 1001, 75, 7602, 0, 163, 673, -154, 3723, 559, 13774, 403, 1696, 722, 296, 257, 418, 488, 900, 2945, -327, 115, 218, 286, 293, 201, 4331, 388, 483, 502, 306, -337, 950, 2410, -633, 0, 265, 698, 459, 0, 1307, 8785, 302, 484, 7, 1539, 3098, 745, 2050, 2766, 4641, 1167, 322, 750, 1559, 0, 0, 46, 0, 1862, 7831, 21614, 2137, 3247, 14, 0, 637, 10655, 94, 997, 2084, 1734, 805, 317, 369, 100, -44, 3296, -50, -411, 922, 1128, 5423, 1537, 0, 519, 186, 83, 1365, 1734, 51, 1567, 96, 8982, -209, -6, 355, 0, 0, 1310, 2520, 1629, 573, 1655, 26, 186, 13658, 23, 87, 49, 3810, 56, 384, 8, 2671, 3240, 147, 63, 7863, 946, 237, 0, 245, 315, 1139, 0, -102, 3984, 1988, 125, 505, 834, 494, 947, 262, 141, -236, 2555, 298, 1179, 31, 357, 1129, 861, 16, 895, 504, 2955, 122, 2204, 111, 1566, 661, 2485, 3756, 43, 1529, 84, 311, 408, 1866, 42, 260, 2994, 0, 1133, 2305, 143, 4721, 5366, 71, 1437, 1093, 817, 196, 205, 0, 1002, 2916, 4513, 7, 0, 253, 28, -65, 376, 1018, 260, 5539, 758, 262, 236, 396, 1857, 4722, 935, 4567, 220, 8, 109, 0, 456, 5553, 137, 299, 162, 1533, 301, 1697, 441, 898, 945, 0, 1830, 6138, 442, 0, 1556, 83, 430, -336, 528, 111, 1556, 408, 1067, 277, 257, 0, 2549, 4612, 433, 680, 3851, 442, 642, 487, 171, 13711, 187, 101, 343, 315, 205, 106, 477, 2875, 1000, -825, 534, 452, 623, 310, 454, 2306, -126, 399, 0, 2109, 0, -470, 81, 1104, 457, 0, 941, -466, 2600, 3, 90, 2570, 181, 66653, 556, 110, 9, 0, 5249, 2499, 276, 3343, 9994, 613, 925, 393, 339, 1216, 1818, 2109, 46, 980, 3060, 46, 274, 805, 3151, 160, 0, 694, 509, 237, 0, 16, 504, -274, 0, 0, 2565, 91, 558, 1032, 351, 231, 3706, 5, 342, 588, 3165, 0, 464, 837, 34230, 5435, 102, 641, 334, 149, 179, 61, 375, 2088, 29, 3839, 2910, 3770, 141, 0, 65, -306, 3740, 0, 131, 23189, -1, 6979, 187, 1203, 6298, -243, 181, 661, 270, 1919, 492, 91, 775, 2669, 2605, 1277, 568, 2344, 4321, 1064, 241, 183, 9214, 513, 14850, 279, 4, -298, 522, 0, 8, 432, -378, 338, 2878, 62, 1303, 255, 0, 1125, 1238, 370, 2417, 4, 444, 28, 2139, 194, 195, 0, 189, 220, 189, 746, 13054, 0, 23421, 41, 7408, 101, 384, 2556, 808, 0, 1158, 334, 296, 1653, 1283, 2295, -42, 55, 365, 772, 334, 1060, 38, 146, 439, 124, 190, 211, 871, 1513, 871, 1298, 360, 597, 457, 2457, 2069, 47, 13, -71, 27, -114, 2645, 1025, 2044, 10, 0, 644, 4293, -238, -613, 1573, 1728, 15, -160, 312, 33, 80, 445, 4565, 3262, 559, 4840, -302, 73, 4790, 1078, 284, 493, 100, 1042, 3665, -9, 576, 945, 108, 93, -97, 3768, 8, 66, 0, -637, 0, 26, 4726, 1782, 3430, 332, 554, 0, 542, 393, 134, 178, 0, 23, 0, 202, 647, 143, 1928, 1630, 102, 0, 1790, 0, 1675, -745, -56, 0, 2396, 223, 5956, 122, 4888, 183, 0, -262, 169, 3364, 15, 126, 1953, 1385, 261, 178, 1819, 0, 6337, 214, 0, 1153, 415, 101, 1490, 779, 2036, 333, 963, 4709, 1074, 90, 243, 414, 0, 2196, 1416, 281, 518, 3, 36, 9301, 883, 21, 3091, 2791, 217, 1579, 0, 314, 655, 212, 523, 111, 1166, 582, 513, 75, 1992, 2910, 66, 599, 89, 796, 571, 215, 1088, 271, 11494, 2741, 1694, 383, 298, 155, 694, 179, 356, 0, 179, 244, 46, 246, 7, 1746, 992, 4312, 103, 506, 461, 268, -454, 365, 2, 1667, 2265, 4622, 0, 188, 0, 592, -276, 1254, 652, 192, 148, 1812, 3478, 264, 71, 1376, 668, 703, -25, 48, 2453, 0, 133, 277, 78, 517, 1, 7944, 3, 623, 290, 8918, 41, 199, 100, 313, 419, 0, 0, 437, 1357, 1610, 0, 709, 10086, 0, -311, 9324, 7832, 191, 6637, 0, 172, 1131, 797, 120, 215, 425, 476, 29, 527, 0, 1093, 70, 39, 296, 1464, 2226, 1286, -738, 3643, 5260, 3096, -312, 286, 89, -72, 4305, 1487, 0, -78, 3495, 3317, 1906, 2996, 446, 1590, 5214, 134, 8, 3120, 171, 39, 171, -140, 728, 1637, -143, 802, 795, 265, 2567, 3402, 1336, 306, 10984, 526, 563, 0, 0, 3107, 0, 1146, 5252, 1195, 488, 1887, 49, 280, -461, 633, 3955, -233, 1626, 1103, 223, -175, -3, 0, 3918, 284, 4577, 876, 3754, 128, 0, 547, 243, 3571, 130, -529, 1636, 12180, 297, 413, 285, 19, 44, 62, 79, 226, 2388, 98, 48, 255, 2, 643, 13410, 0, 3443, 2946, 21, 1212, 2183, 135, 5903, 271, 303, 7264, 0, 204, 444, -27, 287, 800, 4645, 1481, 1575, 5691, 294, 154, 3950, 624, -504, 111, 1131, 322, 0, -89, 3, 1313, 462, 11, 3163, 2, 3157, 4348, -391, 7816, 5763, 240, 508, 2, 1654, 860, 154, 281, 427, 467, 91, 1319, 259, -488, 60, 405, 177, -412, 282, 6181, 373, -92, 49, 0, -283, 1313, 2452, 517, 350, 1623, -435, 333, 3770, 778, 483, 2170, 541, 205, 2269, 0, 973, 894, -12, 0, 5154, -32, 999, 24, 0, 771, 1350, 19, 709, 2040, 303, -311, 690, 2551, -142, -448, 99, 18, 37, 3868, 5041, 260, 67, 157, 1513, 1044, 113, 245, 505, 92, 3685, 1636, 0, 349, 3067, 338, 282, 1190, 73, 384, -388, 354, 192, 70, 22, 335, 2775, 592, 1271, 48, 0, 4869, 737, 83, 1104, 5, 23, 1137, 181, 8267, 871, 1824, 4, 421, 561, 1498, 657, 0, 9192, 7876, 0, 192, 803, 1694, 5559, 737, 174]}],                        {\"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"balance distribution in customer attrition \"}, \"xaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"balance\"}, \"zerolinewidth\": 1}, \"yaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"percent\"}, \"zerolinewidth\": 1}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('da9664bd-1336-4e2c-b2a6-79e4446f017b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Active",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0
         ]
        },
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Churn",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "housing distribution in customer attrition "
        },
        "xaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "housing"
         },
         "zerolinewidth": 1
        },
        "yaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "percent"
         },
         "zerolinewidth": 1
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"0b476aad-01d8-42fc-9e0b-9283cce0d085\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0b476aad-01d8-42fc-9e0b-9283cce0d085\")) {                    Plotly.newPlot(                        \"0b476aad-01d8-42fc-9e0b-9283cce0d085\",                        [{\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Active\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0]}, {\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Churn\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0]}],                        {\"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"housing distribution in customer attrition \"}, \"xaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"housing\"}, \"zerolinewidth\": 1}, \"yaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"percent\"}, \"zerolinewidth\": 1}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0b476aad-01d8-42fc-9e0b-9283cce0d085');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Active",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0
         ]
        },
        {
         "histnorm": "percent",
         "marker": {
          "line": {
           "color": "black",
           "width": 0.4
          }
         },
         "name": "Churn",
         "opacity": 0.8,
         "type": "histogram",
         "x": [
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          1,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          0
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgb(243,243,243)",
        "plot_bgcolor": "rgb(243,243,243)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "loan distribution in customer attrition "
        },
        "xaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "loan"
         },
         "zerolinewidth": 1
        },
        "yaxis": {
         "gridcolor": "rgb(255, 255, 255)",
         "gridwidth": 2,
         "ticklen": 5,
         "title": {
          "text": "percent"
         },
         "zerolinewidth": 1
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"d4550b41-8b6c-44e4-881d-7f3bcca7edf5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d4550b41-8b6c-44e4-881d-7f3bcca7edf5\")) {                    Plotly.newPlot(                        \"d4550b41-8b6c-44e4-881d-7f3bcca7edf5\",                        [{\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Active\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]}, {\"histnorm\": \"percent\", \"marker\": {\"line\": {\"color\": \"black\", \"width\": 0.4}}, \"name\": \"Churn\", \"opacity\": 0.8, \"type\": \"histogram\", \"x\": [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]}],                        {\"paper_bgcolor\": \"rgb(243,243,243)\", \"plot_bgcolor\": \"rgb(243,243,243)\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"loan distribution in customer attrition \"}, \"xaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"loan\"}, \"zerolinewidth\": 1}, \"yaxis\": {\"gridcolor\": \"rgb(255, 255, 255)\", \"gridwidth\": 2, \"ticklen\": 5, \"title\": {\"text\": \"percent\"}, \"zerolinewidth\": 1}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d4550b41-8b6c-44e4-881d-7f3bcca7edf5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#assigning values to churn and active customers\n",
    "mpl.style.use('ggplot')\n",
    "churn     = df[df[\"churn\"] == 1]\n",
    "active = df[df[\"churn\"] == 0]\n",
    " \n",
    "def plot_pie(column) :\n",
    "    \n",
    "    chart1 = go.Pie(values  = churn[column].value_counts().values.tolist(),\n",
    "                    labels  = churn[column].value_counts().keys().tolist(),\n",
    "                    domain  = dict(x = [0,.48]),\n",
    "                    name    = \"Churn\",\n",
    "                    marker  = dict(line = dict(width = 1,\n",
    "                                               color = \"rgb(243,243,243)\")\n",
    "                                  ),\n",
    "                    hole    = .4\n",
    "                   )\n",
    "    chart2 = go.Pie(values  = active[column].value_counts().values.tolist(),\n",
    "                    labels  = active[column].value_counts().keys().tolist(),\n",
    "                    marker  = dict(line = dict(width = 1,\n",
    "                                               color = \"rgb(243,243,243)\")\n",
    "                                  ),\n",
    "                    domain  = dict(x = [.55,1]),\n",
    "                    hole    = .4,\n",
    "                    name    = \"Active\" \n",
    "                   )\n",
    " \n",
    " \n",
    "    layout = go.Layout(dict(title = column + \" distribution in customer attrition \",\n",
    "                            plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "                            paper_bgcolor = \"rgb(243,243,243)\",\n",
    "                            annotations = [dict(text = \"Churn\",\n",
    "                                                font = dict(size = 14),\n",
    "                                                showarrow = False,\n",
    "                                                x = .18, y = .5),\n",
    "                                           dict(text = \"Active\",\n",
    "                                                font = dict(size = 14),\n",
    "                                                showarrow = False,\n",
    "                                                x = .80,y = .5\n",
    "                                               )\n",
    "                                          ]\n",
    "                           )\n",
    "                      )\n",
    "    data = [chart2,chart1]\n",
    "    fig  = go.Figure(data = data,layout = layout)\n",
    "    py.iplot(fig)\n",
    " \n",
    " \n",
    "#function  for histogram for customer churn types\n",
    "def histogram(column) :\n",
    "    chart1 = go.Histogram(x  = churn[column],\n",
    "                          histnorm= \"percent\",\n",
    "                          name = \"Churn\",\n",
    "                          marker = dict(line = dict(width = .4,\n",
    "                                                    color = \"black\"\n",
    "                                                    )\n",
    "                                        ),\n",
    "                         opacity = .8 \n",
    "                         ) \n",
    "    \n",
    "    chart2 = go.Histogram(x  = active[column],\n",
    "                          histnorm = \"percent\",\n",
    "                          name = \"Active\",\n",
    "                          marker = dict(line = dict(width = .4,\n",
    "                                              color = \"black\"\n",
    "                                             )\n",
    "                                 ),\n",
    "                          opacity = .8\n",
    "                         )\n",
    "    \n",
    "    data = [chart2,chart1]\n",
    "    layout = go.Layout(dict(title =column + \" distribution in customer attrition \",\n",
    "                            plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "                            paper_bgcolor = \"rgb(243,243,243)\",\n",
    "                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                             title = column,\n",
    "                                             zerolinewidth=1,\n",
    "                                             ticklen=5,\n",
    "                                             gridwidth=2\n",
    "                                            ),\n",
    "                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                             title = \"percent\",\n",
    "                                             zerolinewidth=1,\n",
    "                                             ticklen=5,\n",
    "                                             gridwidth=2\n",
    "                                            ),\n",
    "                           )\n",
    "                      )\n",
    "    fig  = go.Figure(data=data,layout=layout)\n",
    "    \n",
    "    py.iplot(fig)\n",
    "    \n",
    "#function  for scatter plot matrix  for numerical columns in data\n",
    "def scatter_matrix(df)  :\n",
    "    \n",
    "    df  = df.sort_values(by = \"churn\" ,ascending = False)\n",
    "    classes = df[\"churn\"].unique().tolist()\n",
    "    classes\n",
    "    \n",
    "    class_code  = {classes[k] : k for k in range(2)}\n",
    "    class_code\n",
    " \n",
    "    color_vals = [class_code[cl] for cl in df[\"churn\"]]\n",
    "    color_vals\n",
    " \n",
    "    pl_colorscale = \"Portland\"\n",
    " \n",
    "    pl_colorscale\n",
    " \n",
    "    text = [df.loc[k,\"churn\"] for k in range(len(df))]\n",
    "    text\n",
    " \n",
    "    trace = go.Splom(dimensions = [dict(label  = \"age\",\n",
    "                                       values = df[\"balance\"]),\n",
    "                                  dict(label  = 'balance',\n",
    "                                       values = df['balance']),\n",
    "                                  dict(label  = 'duration',\n",
    "                                       values = df['duration'])],\n",
    "                     text = text,\n",
    "                     marker = dict(color = color_vals,\n",
    "                                   colorscale = pl_colorscale,\n",
    "                                   size = 3,\n",
    "                                   showscale = False,\n",
    "                                   line = dict(width = .1,\n",
    "                                               color='rgb(230,230,230)'\n",
    "                                              )\n",
    "                                  )\n",
    "                    )\n",
    "    axis = dict(showline  = True,\n",
    "                zeroline  = False,\n",
    "                gridcolor = \"#fff\",\n",
    "                ticklen   = 4\n",
    "               )\n",
    "    \n",
    "    layout = go.Layout(dict(title  = \n",
    "                            \"Scatter plot matrix for Numerical columns for customer attrition\",\n",
    "                            autosize = False,\n",
    "                            height = 700,\n",
    "                            width  = 700,\n",
    "                            plot_bgcolor  = 'rgba(240,240,240, 0.95)',\n",
    "                            xaxis1 = dict(axis),\n",
    "                            yaxis1 = dict(axis),\n",
    "                            xaxis2 = dict(axis),\n",
    "                            yaxis2 = dict(axis),\n",
    "                            xaxis3 = dict(axis),\n",
    "                            yaxis3 = dict(axis),\n",
    "                           )\n",
    "                      )\n",
    "    data   = [trace]\n",
    "    fig = go.Figure(data = data,layout = layout )\n",
    "    py.iplot(fig)\n",
    " \n",
    "    \n",
    "cat_cols = [\"marital\", \"job\", \"education\" ]\n",
    "num_cols = [\"age\", \"default\", \"balance\", \"housing\", \"loan\"]\n",
    "#for all categorical columns plot pie\n",
    "for i in cat_cols :\n",
    "    plot_pie(i)\n",
    " \n",
    "#for all categorical columns plot histogram    \n",
    "for i in num_cols :\n",
    "    histogram(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "rgb(196, 230, 195)"
          ],
          [
           0.16666666666666666,
           "rgb(150, 210, 164)"
          ],
          [
           0.3333333333333333,
           "rgb(109, 188, 144)"
          ],
          [
           0.5,
           "rgb(77, 162, 132)"
          ],
          [
           0.6666666666666666,
           "rgb(54, 135, 122)"
          ],
          [
           0.8333333333333334,
           "rgb(38, 107, 110)"
          ],
          [
           1,
           "rgb(29, 79, 96)"
          ]
         ],
         "type": "heatmap",
         "x": [
          "products",
          "duration",
          "loan",
          "housing",
          "balance",
          "default",
          "age"
         ],
         "y": [
          "products",
          "duration",
          "loan",
          "housing",
          "balance",
          "default",
          "age"
         ],
         "z": [
          [
           1,
           -0.04155745875962227,
           0.03472174760057545,
           0.006659940490104468,
           -0.013893822542985323,
           0.030975265612536287,
           -0.00527793615604048
          ],
          [
           -0.04155745875962227,
           1,
           -0.0019136296136021734,
           0.03505086155420101,
           0.022436131268962777,
           -0.009759520939361763,
           0.00018922807371430606
          ],
          [
           0.03472174760057545,
           -0.0019136296136021734,
           1,
           0.07676050743339044,
           -0.08458874010545157,
           0.07643391247916834,
           -0.03141802793725277
          ],
          [
           0.006659940490104468,
           0.03505086155420101,
           0.07676050743339044,
           1,
           -0.0770920466345199,
           0.011075754108813023,
           -0.16870033095220485
          ],
          [
           -0.013893822542985323,
           0.022436131268962777,
           -0.08458874010545157,
           -0.0770920466345199,
           1,
           -0.060953887359227785,
           0.11229988859873112
          ],
          [
           0.030975265612536287,
           -0.009759520939361763,
           0.07643391247916834,
           0.011075754108813023,
           -0.060953887359227785,
           1,
           -0.011424760205286794
          ],
          [
           -0.00527793615604048,
           0.00018922807371430606,
           -0.03141802793725277,
           -0.16870033095220485,
           0.11229988859873112,
           -0.011424760205286794,
           1
          ]
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"6777627a-1e9e-46a7-b735-12e3b059c857\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6777627a-1e9e-46a7-b735-12e3b059c857\")) {                    Plotly.newPlot(                        \"6777627a-1e9e-46a7-b735-12e3b059c857\",                        [{\"colorscale\": [[0.0, \"rgb(196, 230, 195)\"], [0.16666666666666666, \"rgb(150, 210, 164)\"], [0.3333333333333333, \"rgb(109, 188, 144)\"], [0.5, \"rgb(77, 162, 132)\"], [0.6666666666666666, \"rgb(54, 135, 122)\"], [0.8333333333333334, \"rgb(38, 107, 110)\"], [1.0, \"rgb(29, 79, 96)\"]], \"type\": \"heatmap\", \"x\": [\"products\", \"duration\", \"loan\", \"housing\", \"balance\", \"default\", \"age\"], \"y\": [\"products\", \"duration\", \"loan\", \"housing\", \"balance\", \"default\", \"age\"], \"z\": [[1.0, -0.04155745875962227, 0.03472174760057545, 0.006659940490104468, -0.013893822542985323, 0.030975265612536287, -0.00527793615604048], [-0.04155745875962227, 1.0, -0.0019136296136021734, 0.03505086155420101, 0.022436131268962777, -0.009759520939361763, 0.00018922807371430606], [0.03472174760057545, -0.0019136296136021734, 1.0, 0.07676050743339044, -0.08458874010545157, 0.07643391247916834, -0.03141802793725277], [0.006659940490104468, 0.03505086155420101, 0.07676050743339044, 1.0, -0.0770920466345199, 0.011075754108813023, -0.16870033095220485], [-0.013893822542985323, 0.022436131268962777, -0.08458874010545157, -0.0770920466345199, 1.0, -0.060953887359227785, 0.11229988859873112], [0.030975265612536287, -0.009759520939361763, 0.07643391247916834, 0.011075754108813023, -0.060953887359227785, 1.0, -0.011424760205286794], [-0.00527793615604048, 0.00018922807371430606, -0.03141802793725277, -0.16870033095220485, 0.11229988859873112, -0.011424760205286794, 1.0]]}],                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"autotypenumbers\": \"strict\", \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('6777627a-1e9e-46a7-b735-12e3b059c857');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#correlation\n",
    "corr_data = df[['products', 'duration', 'loan', 'housing', 'balance', 'default', 'age']]\n",
    "correlation = corr_data.corr()\n",
    "scatter_cols = correlation.columns.tolist()\n",
    "\n",
    "corr_array = np.array(correlation)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "        z = corr_array,\n",
    "        x = scatter_cols,\n",
    "        y = scatter_cols,\n",
    "        colorscale = 'blugrn'))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 8929 samples.\n",
      "Testing set has 2233 samples.\n"
     ]
    }
   ],
   "source": [
    "#training dataset\n",
    "X = df[['age', 'default', 'balance', 'housing', 'loan']]\n",
    "y = df[\"churn\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.131214%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating model accuracy\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy of XGB classifier on training set: 0.80\n",
      "Accuracy of XGB classifier on test set: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train and test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)\n",
    "\n",
    "#model building\n",
    "xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.08, objective= 'binary:logistic',n_jobs=-1).fit(X_train, y_train)\n",
    "print('Accuracy of XGB classifier on training set: {:.2f}'\n",
    "       .format(xgb_model.score(X_train, y_train)))\n",
    "print('Accuracy of XGB classifier on test set: {:.2f}'\n",
    "       .format(xgb_model.score(X_test[X_train.columns], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ecba6b3ac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting important features\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plot_importance(xgb_model, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Precision = 0.3962432915921288\n",
      "Recall = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning:\n",
      "\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating precision and recall\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {'max_depth':3, 'eta':1, 'objective':'multi:softprob', 'num_class':5 }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "pred = bst.predict(dtest)\n",
    "improv_pred = np.asarray([np.argmax(line) for line in pred])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, improv_pred, average = 'macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, improv_pred, average = 'macro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating the churn probability\n",
    "df['probability'] = xgb_model.predict_proba(df[X_train.columns])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>0.781680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>0.756692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>0.753211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>0.748117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>0.780885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006</td>\n",
       "      <td>0.710455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007</td>\n",
       "      <td>0.636237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>0.862454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009</td>\n",
       "      <td>0.735264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>0.826722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  probability\n",
       "0         1001     0.781680\n",
       "1         1002     0.756692\n",
       "2         1003     0.753211\n",
       "3         1004     0.748117\n",
       "4         1005     0.780885\n",
       "5         1006     0.710455\n",
       "6         1007     0.636237\n",
       "7         1008     0.862454\n",
       "8         1009     0.735264\n",
       "9         1010     0.826722"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing out probability column\n",
    "probability = df[['customer_id', 'probability']]\n",
    "probability.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to excel\n",
    "probability.to_excel(\"churn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.62193\tvalidation_0-error:0.20428\tvalidation_1-auc:0.59356\tvalidation_1-error:0.21467\n",
      "[1]\tvalidation_0-auc:0.63093\tvalidation_0-error:0.20447\tvalidation_1-auc:0.59759\tvalidation_1-error:0.21109\n",
      "[2]\tvalidation_0-auc:0.64356\tvalidation_0-error:0.20532\tvalidation_1-auc:0.59443\tvalidation_1-error:0.20930\n",
      "[3]\tvalidation_0-auc:0.65086\tvalidation_0-error:0.20522\tvalidation_1-auc:0.59801\tvalidation_1-error:0.20930\n",
      "[4]\tvalidation_0-auc:0.65739\tvalidation_0-error:0.20589\tvalidation_1-auc:0.59432\tvalidation_1-error:0.20751\n",
      "[5]\tvalidation_0-auc:0.65958\tvalidation_0-error:0.20579\tvalidation_1-auc:0.59305\tvalidation_1-error:0.20751\n",
      "[6]\tvalidation_0-auc:0.66559\tvalidation_0-error:0.20589\tvalidation_1-auc:0.59270\tvalidation_1-error:0.20751\n",
      "[7]\tvalidation_0-auc:0.67125\tvalidation_0-error:0.20560\tvalidation_1-auc:0.59486\tvalidation_1-error:0.20751\n",
      "[8]\tvalidation_0-auc:0.67515\tvalidation_0-error:0.20541\tvalidation_1-auc:0.60020\tvalidation_1-error:0.20751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\tvalidation_0-auc:0.68022\tvalidation_0-error:0.20551\tvalidation_1-auc:0.60373\tvalidation_1-error:0.20751\n",
      "[10]\tvalidation_0-auc:0.68238\tvalidation_0-error:0.20532\tvalidation_1-auc:0.60778\tvalidation_1-error:0.20751\n",
      "[11]\tvalidation_0-auc:0.69178\tvalidation_0-error:0.20494\tvalidation_1-auc:0.60495\tvalidation_1-error:0.20751\n",
      "[12]\tvalidation_0-auc:0.69533\tvalidation_0-error:0.20475\tvalidation_1-auc:0.60852\tvalidation_1-error:0.20751\n",
      "[13]\tvalidation_0-auc:0.70403\tvalidation_0-error:0.20447\tvalidation_1-auc:0.61291\tvalidation_1-error:0.20751\n",
      "[14]\tvalidation_0-auc:0.70552\tvalidation_0-error:0.20343\tvalidation_1-auc:0.61471\tvalidation_1-error:0.20751\n",
      "[15]\tvalidation_0-auc:0.70909\tvalidation_0-error:0.20343\tvalidation_1-auc:0.61090\tvalidation_1-error:0.20751\n",
      "[16]\tvalidation_0-auc:0.72045\tvalidation_0-error:0.20306\tvalidation_1-auc:0.60611\tvalidation_1-error:0.20751\n",
      "[17]\tvalidation_0-auc:0.72225\tvalidation_0-error:0.20240\tvalidation_1-auc:0.60556\tvalidation_1-error:0.20930\n",
      "[18]\tvalidation_0-auc:0.72391\tvalidation_0-error:0.20230\tvalidation_1-auc:0.60760\tvalidation_1-error:0.20930\n",
      "[19]\tvalidation_0-auc:0.72430\tvalidation_0-error:0.20221\tvalidation_1-auc:0.60770\tvalidation_1-error:0.20930\n",
      "[20]\tvalidation_0-auc:0.72527\tvalidation_0-error:0.20192\tvalidation_1-auc:0.60768\tvalidation_1-error:0.20930\n",
      "[21]\tvalidation_0-auc:0.72928\tvalidation_0-error:0.20155\tvalidation_1-auc:0.60426\tvalidation_1-error:0.20930\n",
      "[22]\tvalidation_0-auc:0.73817\tvalidation_0-error:0.20164\tvalidation_1-auc:0.60982\tvalidation_1-error:0.20930\n",
      "[23]\tvalidation_0-auc:0.73848\tvalidation_0-error:0.20145\tvalidation_1-auc:0.60799\tvalidation_1-error:0.20930\n",
      "[24]\tvalidation_0-auc:0.74094\tvalidation_0-error:0.20060\tvalidation_1-auc:0.60849\tvalidation_1-error:0.21109\n",
      "[25]\tvalidation_0-auc:0.74547\tvalidation_0-error:0.20023\tvalidation_1-auc:0.60828\tvalidation_1-error:0.21288\n",
      "[26]\tvalidation_0-auc:0.74881\tvalidation_0-error:0.19966\tvalidation_1-auc:0.61107\tvalidation_1-error:0.21467\n",
      "[27]\tvalidation_0-auc:0.74917\tvalidation_0-error:0.19966\tvalidation_1-auc:0.61052\tvalidation_1-error:0.21467\n",
      "[28]\tvalidation_0-auc:0.75051\tvalidation_0-error:0.19909\tvalidation_1-auc:0.61239\tvalidation_1-error:0.21646\n",
      "[29]\tvalidation_0-auc:0.75446\tvalidation_0-error:0.19900\tvalidation_1-auc:0.61330\tvalidation_1-error:0.21646\n",
      "[30]\tvalidation_0-auc:0.75921\tvalidation_0-error:0.19862\tvalidation_1-auc:0.60459\tvalidation_1-error:0.21467\n",
      "[31]\tvalidation_0-auc:0.76309\tvalidation_0-error:0.19834\tvalidation_1-auc:0.59892\tvalidation_1-error:0.21467\n",
      "[32]\tvalidation_0-auc:0.76884\tvalidation_0-error:0.19834\tvalidation_1-auc:0.60126\tvalidation_1-error:0.21467\n",
      "[33]\tvalidation_0-auc:0.77075\tvalidation_0-error:0.19815\tvalidation_1-auc:0.59838\tvalidation_1-error:0.21467\n",
      "[34]\tvalidation_0-auc:0.77293\tvalidation_0-error:0.19815\tvalidation_1-auc:0.59806\tvalidation_1-error:0.21467\n",
      "[35]\tvalidation_0-auc:0.77459\tvalidation_0-error:0.19834\tvalidation_1-auc:0.59644\tvalidation_1-error:0.21467\n",
      "[36]\tvalidation_0-auc:0.77469\tvalidation_0-error:0.19815\tvalidation_1-auc:0.59527\tvalidation_1-error:0.21467\n",
      "[37]\tvalidation_0-auc:0.77705\tvalidation_0-error:0.19815\tvalidation_1-auc:0.59356\tvalidation_1-error:0.21467\n",
      "[38]\tvalidation_0-auc:0.77808\tvalidation_0-error:0.19796\tvalidation_1-auc:0.59687\tvalidation_1-error:0.21288\n",
      "[39]\tvalidation_0-auc:0.77855\tvalidation_0-error:0.19815\tvalidation_1-auc:0.59611\tvalidation_1-error:0.21288\n",
      "[40]\tvalidation_0-auc:0.77862\tvalidation_0-error:0.19806\tvalidation_1-auc:0.59418\tvalidation_1-error:0.21288\n",
      "[41]\tvalidation_0-auc:0.78131\tvalidation_0-error:0.19806\tvalidation_1-auc:0.59967\tvalidation_1-error:0.21288\n",
      "[42]\tvalidation_0-auc:0.78320\tvalidation_0-error:0.19796\tvalidation_1-auc:0.60117\tvalidation_1-error:0.21467\n",
      "[43]\tvalidation_0-auc:0.78406\tvalidation_0-error:0.19777\tvalidation_1-auc:0.59932\tvalidation_1-error:0.21467\n",
      "[44]\tvalidation_0-auc:0.78413\tvalidation_0-error:0.19768\tvalidation_1-auc:0.59897\tvalidation_1-error:0.21467\n",
      "[45]\tvalidation_0-auc:0.78429\tvalidation_0-error:0.19740\tvalidation_1-auc:0.59849\tvalidation_1-error:0.21467\n",
      "[46]\tvalidation_0-auc:0.78489\tvalidation_0-error:0.19740\tvalidation_1-auc:0.59751\tvalidation_1-error:0.21467\n",
      "[47]\tvalidation_0-auc:0.78501\tvalidation_0-error:0.19730\tvalidation_1-auc:0.59790\tvalidation_1-error:0.21467\n",
      "[48]\tvalidation_0-auc:0.78523\tvalidation_0-error:0.19711\tvalidation_1-auc:0.59749\tvalidation_1-error:0.21467\n",
      "[49]\tvalidation_0-auc:0.78531\tvalidation_0-error:0.19692\tvalidation_1-auc:0.59755\tvalidation_1-error:0.21467\n",
      "[50]\tvalidation_0-auc:0.78926\tvalidation_0-error:0.19608\tvalidation_1-auc:0.59808\tvalidation_1-error:0.21825\n",
      "[51]\tvalidation_0-auc:0.79133\tvalidation_0-error:0.19589\tvalidation_1-auc:0.59890\tvalidation_1-error:0.21825\n",
      "[52]\tvalidation_0-auc:0.79419\tvalidation_0-error:0.19542\tvalidation_1-auc:0.59587\tvalidation_1-error:0.21825\n",
      "[53]\tvalidation_0-auc:0.79760\tvalidation_0-error:0.19513\tvalidation_1-auc:0.59908\tvalidation_1-error:0.21825\n",
      "[54]\tvalidation_0-auc:0.80049\tvalidation_0-error:0.19495\tvalidation_1-auc:0.59994\tvalidation_1-error:0.21825\n",
      "[55]\tvalidation_0-auc:0.80284\tvalidation_0-error:0.19419\tvalidation_1-auc:0.60075\tvalidation_1-error:0.22182\n",
      "[56]\tvalidation_0-auc:0.80408\tvalidation_0-error:0.19400\tvalidation_1-auc:0.60184\tvalidation_1-error:0.22182\n",
      "[57]\tvalidation_0-auc:0.80510\tvalidation_0-error:0.19334\tvalidation_1-auc:0.60521\tvalidation_1-error:0.22182\n",
      "[58]\tvalidation_0-auc:0.80710\tvalidation_0-error:0.19278\tvalidation_1-auc:0.60499\tvalidation_1-error:0.22182\n",
      "[59]\tvalidation_0-auc:0.80821\tvalidation_0-error:0.19202\tvalidation_1-auc:0.60637\tvalidation_1-error:0.22182\n",
      "[60]\tvalidation_0-auc:0.80875\tvalidation_0-error:0.19164\tvalidation_1-auc:0.60665\tvalidation_1-error:0.22182\n",
      "[61]\tvalidation_0-auc:0.81002\tvalidation_0-error:0.19108\tvalidation_1-auc:0.60733\tvalidation_1-error:0.22182\n",
      "[62]\tvalidation_0-auc:0.81006\tvalidation_0-error:0.19070\tvalidation_1-auc:0.60760\tvalidation_1-error:0.22182\n",
      "[63]\tvalidation_0-auc:0.81075\tvalidation_0-error:0.19042\tvalidation_1-auc:0.60538\tvalidation_1-error:0.22361\n",
      "[64]\tvalidation_0-auc:0.81077\tvalidation_0-error:0.19013\tvalidation_1-auc:0.60424\tvalidation_1-error:0.22361\n",
      "[65]\tvalidation_0-auc:0.81211\tvalidation_0-error:0.18976\tvalidation_1-auc:0.60286\tvalidation_1-error:0.22361\n",
      "[66]\tvalidation_0-auc:0.81218\tvalidation_0-error:0.18976\tvalidation_1-auc:0.60295\tvalidation_1-error:0.22361\n",
      "[67]\tvalidation_0-auc:0.81303\tvalidation_0-error:0.18985\tvalidation_1-auc:0.60021\tvalidation_1-error:0.22540\n",
      "[68]\tvalidation_0-auc:0.81355\tvalidation_0-error:0.18976\tvalidation_1-auc:0.59988\tvalidation_1-error:0.22540\n",
      "[69]\tvalidation_0-auc:0.81501\tvalidation_0-error:0.18938\tvalidation_1-auc:0.59834\tvalidation_1-error:0.22719\n",
      "[70]\tvalidation_0-auc:0.81770\tvalidation_0-error:0.18863\tvalidation_1-auc:0.59824\tvalidation_1-error:0.22719\n",
      "[71]\tvalidation_0-auc:0.81770\tvalidation_0-error:0.18853\tvalidation_1-auc:0.59815\tvalidation_1-error:0.22719\n",
      "[72]\tvalidation_0-auc:0.82013\tvalidation_0-error:0.18844\tvalidation_1-auc:0.59871\tvalidation_1-error:0.22719\n",
      "[73]\tvalidation_0-auc:0.82226\tvalidation_0-error:0.18863\tvalidation_1-auc:0.59920\tvalidation_1-error:0.22898\n",
      "[74]\tvalidation_0-auc:0.82228\tvalidation_0-error:0.18844\tvalidation_1-auc:0.59918\tvalidation_1-error:0.22898\n",
      "[75]\tvalidation_0-auc:0.82351\tvalidation_0-error:0.18806\tvalidation_1-auc:0.59828\tvalidation_1-error:0.22898\n",
      "[76]\tvalidation_0-auc:0.82441\tvalidation_0-error:0.18806\tvalidation_1-auc:0.59657\tvalidation_1-error:0.22898\n",
      "[77]\tvalidation_0-auc:0.82712\tvalidation_0-error:0.18797\tvalidation_1-auc:0.59423\tvalidation_1-error:0.22898\n",
      "[78]\tvalidation_0-auc:0.82760\tvalidation_0-error:0.18787\tvalidation_1-auc:0.59301\tvalidation_1-error:0.22898\n",
      "[79]\tvalidation_0-auc:0.82808\tvalidation_0-error:0.18778\tvalidation_1-auc:0.59239\tvalidation_1-error:0.22898\n",
      "[80]\tvalidation_0-auc:0.83054\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59315\tvalidation_1-error:0.22898\n",
      "[81]\tvalidation_0-auc:0.83120\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59331\tvalidation_1-error:0.22898\n",
      "[82]\tvalidation_0-auc:0.83251\tvalidation_0-error:0.18674\tvalidation_1-auc:0.59372\tvalidation_1-error:0.22898\n",
      "[83]\tvalidation_0-auc:0.83488\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59159\tvalidation_1-error:0.22540\n",
      "[84]\tvalidation_0-auc:0.83632\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59249\tvalidation_1-error:0.22540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85]\tvalidation_0-auc:0.83635\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59296\tvalidation_1-error:0.22540\n",
      "[86]\tvalidation_0-auc:0.83664\tvalidation_0-error:0.18712\tvalidation_1-auc:0.59216\tvalidation_1-error:0.22540\n",
      "[87]\tvalidation_0-auc:0.83813\tvalidation_0-error:0.18721\tvalidation_1-auc:0.59422\tvalidation_1-error:0.22540\n",
      "[88]\tvalidation_0-auc:0.83820\tvalidation_0-error:0.18730\tvalidation_1-auc:0.59386\tvalidation_1-error:0.22540\n",
      "[89]\tvalidation_0-auc:0.83825\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59397\tvalidation_1-error:0.22540\n",
      "[90]\tvalidation_0-auc:0.83838\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59395\tvalidation_1-error:0.22540\n",
      "[91]\tvalidation_0-auc:0.83855\tvalidation_0-error:0.18712\tvalidation_1-auc:0.59547\tvalidation_1-error:0.22540\n",
      "[92]\tvalidation_0-auc:0.83954\tvalidation_0-error:0.18702\tvalidation_1-auc:0.59633\tvalidation_1-error:0.22540\n",
      "[93]\tvalidation_0-auc:0.84223\tvalidation_0-error:0.18693\tvalidation_1-auc:0.59307\tvalidation_1-error:0.22540\n",
      "[94]\tvalidation_0-auc:0.84349\tvalidation_0-error:0.18636\tvalidation_1-auc:0.59272\tvalidation_1-error:0.22540\n",
      "[95]\tvalidation_0-auc:0.84354\tvalidation_0-error:0.18636\tvalidation_1-auc:0.59242\tvalidation_1-error:0.22540\n",
      "[96]\tvalidation_0-auc:0.84508\tvalidation_0-error:0.18523\tvalidation_1-auc:0.59349\tvalidation_1-error:0.22540\n",
      "[97]\tvalidation_0-auc:0.84551\tvalidation_0-error:0.18542\tvalidation_1-auc:0.59126\tvalidation_1-error:0.22540\n",
      "[98]\tvalidation_0-auc:0.84548\tvalidation_0-error:0.18523\tvalidation_1-auc:0.59141\tvalidation_1-error:0.22540\n",
      "[99]\tvalidation_0-auc:0.84917\tvalidation_0-error:0.18485\tvalidation_1-auc:0.59501\tvalidation_1-error:0.22540\n",
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n",
       "       use_label_encoder=True, validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "eval_metric = [\"auc\",\"error\"]\n",
    "%time model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(silent=False, \n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=1000, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=4, \n",
    "                      gamma=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7951979931911843\n"
     ]
    }
   ],
   "source": [
    "#Evaluation dataset\n",
    "eval_set = [(X_train, y_train),(X,y)]\n",
    "\n",
    "#defining parameters\n",
    "model = xgb.XGBClassifier(subsample=1,\n",
    "colsample_bytree=1,\n",
    "min_child_weight=1,\n",
    "max_depth=6,\n",
    "learning_rate=0.3,\n",
    "n_estimators=100)\n",
    "\n",
    "#fit model\n",
    "model.fit(X_train,y_train,early_stopping_rounds=10, eval_metric=\"error\",eval_set=eval_set,verbose=0)\n",
    "\n",
    "#making predictions\n",
    "predictions = model.predict(X)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy:',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining parameters\n",
    "parameters = {\"subsample\":[0.5, 0.75, 1],\n",
    "\"colsample_bytree\":[0.5, 0.75, 1],\n",
    "\"max_depth\":[2, 6, 12],\n",
    "\"min_child_weight\":[1,5,15],\n",
    "\"learning_rate\":[0.3, 0.1, 0.03],\n",
    "\"n_estimators\":[100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n",
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning:\n",
      "\n",
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "       gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "       min_child_w...e,\n",
       "       tree_method=None, use_label_encoder=True, validate_parameters=None,\n",
       "       verbosity=None),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'subsample': [0.5, 0.75, 1], 'colsample_bytree': [0.5, 0.75, 1], 'max_depth': [2, 6, 12], 'min_child_weight': [1, 5, 15], 'learning_rate': [0.3, 0.1, 0.03], 'n_estimators': [100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XBG model\n",
    "model = xgb.XGBClassifier(n_estimators=100, n_jobs=-1)\n",
    "\"\"\"Initialise Grid Search Model to inherit from the XGBoost Model,\n",
    "set the of cross validations to 3 per combination and use accuracy\n",
    "to score the models.\"\"\"\n",
    "model_gs = GridSearchCV(model,param_grid=parameters,cv=3,scoring=\"accuracy\", use_label_encoder=False, [num_class - 1])\n",
    "\n",
    "#Fit model\n",
    "model_gs.fit(X_train,y_train,early_stopping_rounds=10, eval_metric=\"error\",eval_set=eval_set,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7957355312667981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kwame.adu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\data.py:114: UserWarning:\n",
      "\n",
      "Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model_gs.predict(X)\n",
    "print('Accuracy:',accuracy_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
